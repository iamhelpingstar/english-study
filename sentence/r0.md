We propose that fitting the value of a small area around the target action

$$y = r + \mathbb{E}\_{\epsilon} \left[ Q\_{\theta'}(s', \pi_{\phi'}(s') + \epsilon)\right], \tag{13}$$

would have the benefit of smoothing the value estimate by bootstrapping off of similar state-action value estimates.

우리는 목표 행동

$$y = r + \mathbb{E}\_{\epsilon} \left[ Q\_{\theta'}(s', \pi_{\phi'}(s') + \epsilon)\right], \tag{13}$$

주변의 작은 영역의 가치를 적합시키는 것이 유사한 상태-행동 가치 추정치를 이용하여 가치 추정치를 부드럽게 만드는 데 도움이 될 것이라고 제안합니다.

In practice, we can approximate this expectation over actions by adding a small amount of random noise to the target policy and averaging over mini-batches.

실제로, 우리는 목표 정책에 작은 양의 랜덤 노이즈를 추가하고 미니배치에 대해 평균을 내어 이러한 행동에 대한 기대를 근사할 수 있습니다.

Intuitively, it is known that policies derived from SARSA value estimates tend to be safer, as they provide higher value to actions resistant to perturbations.

직관적으로, SARSA 가치 추정치에서 유도된 정책은 변동에 저항하는 행동에 더 높은 가치를 제공하기 때문에 더 안전하다고 알려져 있습니다.

Thus, this style of update can additionally lead to improvement in stochastic domains with failure cases.

따라서, 이러한 업데이트 방식은 실패 사례가 있는 확률적 영역에서의 개선으로 이어질 수 있습니다.

A similar idea was introduced concurrently by Nachum et al. (2018), smoothing over $Q_θ$, rather than $Q_{\theta'}$.

비슷한 아이디어가 Nachum et al. (2018)에 의해 동시에 소개되었는데, 이는 $Q_{\theta'}$가 아닌 $Q_{\theta}$에 대해 평활화하는 것입니다.

Every $d$ iterations, the policy is updated with respect to $Q_{\theta_{1}}$ following the deterministic policy gradient algorithm.

매 $d$번의 반복마다, 정책은 결정론적 정책 그래디언트 알고리즘을 따라 $Q_{\theta_{1}}$에 대해 업데이트됩니다.

