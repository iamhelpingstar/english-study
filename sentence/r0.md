We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic.

이 문제가 액터-크리틱 설정에서도 지속되는 것을 보여주고, 액터와 크리틱 양쪽 모두에 미치는 영향을 최소화하기 위한 새로운 메커니즘을 제안합니다.

In reinforcement learning problems with discrete action spaces, the issue of value overestimation as a result of function approximation errors is well-studied.

이산 동작 공간을 갖는 강화학습 문제에서, 함수 근사 오류로 인한 가치 과대평가 문제는 잘 연구되어 있습니다.

Overestimation bias is a property of Q-learning in which the maximization of a noisy value estimate induces a consistent overestimation.

과대평가 편향은 Q-러닝의 특성으로, 잡음이 섞인 가치 추정치의 최대화가 일관된 과대평가를 유발한다는 것입니다.

This inaccuracy is further exaggerated by the nature of temporal difference learning (Sutton, 1988), in which an estimate of the value function is updated using the estimate of a subsequent state.

이러한 부정확함은 시간차 학습(Sutton, 1988)의 특성에 의해 더욱 과장됩니다. 여기서 가치 함수의 추정치는 후속 상태의 추정치를 사용하여 업데이트됩니다.

During training, Double DQN estimates the value of the current policy with a separate target value function, allowing actions to be evaluated without maximization bias.

트레이닝 중에, 더블 DQN은 별도의 목표 가치 함수를 사용하여 현재 정책의 가치를 추정하며, 이를 통해 행동을 최대화 편향 없이 평가할 수 있습니다.

This favors underestimations, which do not tend to be propagated during learning, as actions with low value estimates are avoided by the policy.

이는 학습 동안 전파되지 않는 경향이 있는 저평가를 선호합니다. 왜냐하면 낮은 가치 추정치를 가진 행동들은 정책에 의해 피해지기 때문입니다.

Given the connection of noise to overestimation bias, this paper contains a number of components that address variance reduction.

잡음과 과대평가 편향 사이의 연관성을 고려할 때, 이 논문은 분산 감소를 다루는 여러 구성 요소를 포함하고 있습니다.

Second, to address the coupling of value and policy, we propose delaying policy updates until the value estimate has converged. Finally, we introduce a novel regularization strategy, where a SARSA-style update bootstraps similar action estimates to further reduce variance.

둘째, 가치와 정책의 결합 문제를 해결하기 위해, 가치 추정치가 수렴할 때까지 정책 업데이트를 지연시키는 것을 제안합니다. 마지막으로, SARSA 스타일 업데이트가 유사한 행동 추정치를 부트스트랩하여 분산을 더욱 줄이는 새로운 정규화 전략을 소개합니다.

an actor-critic algorithm which considers the interplay between function approximation error in both policy and value updates.

정책과 가치 업데이트에서 함수 근사 오류 간의 상호작용을 고려하는 액터-크리틱 알고리즘입니다.

Our work focuses on two outcomes that occur as the result of estimation error, namely overestimation bias and a high variance build-up.

우리의 연구는 추정 오류의 결과로 발생하는 두 가지 결과, 즉 과대평가 편향과 높은 분산 축적에 초점을 맞춥니다.

Other approaches have focused directly on reducing the variance (Anschel et al., 2017), minimizing over-fitting to early high variance estimates (Fox et al., 2016), or through corrective terms (Lee et al., 2013).

다른 접근법들은 분산을 직접 줄이는 것(Anschel et al., 2017), 초기 높은 분산 추정치에 대한 과적합 최소화(Fox et al., 2016), 또는 corrective terms를 통한 방법(Lee et al., 2013)에 중점을 두었습니다.

Further, the variance of the value estimate has been considered directly for risk-aversion (Mannor & Tsitsiklis, 2011) and exploration (O’Donoghue et al., 2017), but without connection to overestimation bias.

더 나아가, 가치 추정의 분산은 위험 회피(Mannor & Tsitsiklis, 2011) 및 탐험(O’Donoghue et al., 2017)을 위해 직접 고려되었지만, 과대평가 편향과는 연관이 없었습니다.

However, rather than provide a direct solution to the accumulation of error, these methods circumvent the problem by considering a longer horizon.

그러나 이러한 방법들은 오류의 축적에 대한 직접적인 해결책을 제공하기보다는, 더 긴 시계열을 고려함으로써 문제를 회피합니다.

however, if the target is susceptible to error ε, then the maximum over the value along with its error will generally be greater than the true maximum.

그러나, 만약 목표가 오류 ε에 취약하다면, 그 오류를 포함한 값의 최대치는 일반적으로 실제 최대치보다 클 것입니다.

As a result, even initially zero-mean errors can cause value updates to result in a consistent overestimation bias, which is then propagated through the Bellman equation.

결과적으로, 초기에는 평균이 제로인 오류조차도 값 업데이트로 인해 일관된 과대평가 편향을 유발할 수 있으며, 이 편향은 그 후 벨만 방정식을 통해 전파됩니다.

Without normalized gradients, overestimation bias is still guaranteed to occur with slightly stricter conditions.

정규화된 그래디언트 없이도, 약간 더 엄격한 조건으로 과대평가 편향이 여전히 발생하는 것은 보장됩니다.

If in expectation the value estimate is at least as large as the true value with respect to φ_true, then Equations (5) and (6) imply that if α < min(ϵ_1, ϵ_2), then the value estimate will be overestimated:

기대치에서 가치 추정치가 φ_true에 대한 실제 가치보다 적어도 그만큼 크다면, 방정식 (5)와 (6)은 α < min(ϵ_1, ϵ_2)일 경우 가치 추정치가 과대평가될 것임을 시사합니다.

In Double Q-learning, the greedy update is disentangled from the value function by maintaining two separate value estimates, each of which is used to update the other.

Double Q-learning에서, Greedy 업데이트는 가치 함수로부터 분리되며, 이는 두 개의 별도 가치 추정치를 유지함으로써 이루어집니다. 각 추정치는 서로를 업데이트하는 데 사용됩니다.

