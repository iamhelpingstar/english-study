We will use $ρ_π(s_t)$ and $ρ_π(s_t, a_t)$ to denote the state and state-action marginals of the trajectory distribution induced by a policy $π(a_t|s_t)$.

우리는 policy $π(a_t|s_t)$에 의해 유도된 trajectory 분포의 state와 state-action 마진을 나타내기 위해 $ρ_π(s_t)$와 $ρ_π(s_t, a_t)$를 사용할 것입니다.

We will consider a more general maximum entropy objective (see e.g. Ziebart (2010)), which favors stochastic policies by augmenting the objective with the expected entropy of the policy over $ρ_π(s_t)$:

$$J(\pi) = \sum_{t=0}^{T} \mathbb{E}_{(s_t,a_t) \sim \rho_\pi} \left[ r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot \mid s_t)) \right] \tag{1}$$

우리는 더 일반적인 최대 엔트로피 objective를 고려할 것입니다 (예: Ziebart (2010) 참조), 이는 $ρ_π(s_t)$에 대한 policy의 기대 엔트로피를 objective에 포함시켜 stochastic policies에 유리하게 작용합니다.

The maximum entropy objective differs from the standard maximum expected reward objective used in conventional reinforcement learning, though the conventional objective can be recovered in the limit as $α → 0$.

최대 엔트로피 objective는 전통적인 강화 학습에서 사용되는 표준 최대 기대 보상 objective와 다르지만, $α → 0$일 때 극한에서는 전통적인 objective를 되찾을 수 있습니다.

For the rest of this paper, we will omit writing the temperature explicitly, as it can always be subsumed into the reward by scaling it by $α^{−1}$.

이 논문의 나머지 부분에서는 temperature를 명시적으로 쓰지 않을 것입니다, 왜냐하면 $α^{−1}$로 스케일링하여 보상에 항상 포함될 수 있기 때문입니다.

Writing down the maximum entropy objective for the infinite horizon discounted case is more involved (Thomas, 2014) and is deferred to Appendix A.

Infinite horizon discounted 케이스에 대한 최대 엔트로피 objective를 기술하는 것은 더 복잡합니다 (Thomas, 2014) 그리고 이는 부록 A로 연기되었습니다.

We will begin by deriving soft policy iteration, a general algorithm for learning optimal maximum entropy policies that alternates between policy evaluation and policy improvement in the maximum entropy framework.

우리는 최대 엔트로피 프레임워크에서 policy evaluation과 policy improvement를 번갈아 가며 최적의 최대 엔트로피 policies를 학습하는 일반적인 알고리즘인 soft policy iteration을 유도하기 시작할 것입니다.

Therefore, we will next approximate the algorithm for continuous domains, where we need to rely on a function approximator to represent the Q-values, and running the two steps until convergence would be computationally too expensive. The approximation gives rise to a new practical algorithm, called soft actor-critic.

따라서, 우리는 continuous 도메인에 대한 알고리즘을 근사할 것입니다. 여기서는 Q-values를 나타내기 위해 function approximator에 의존해야 하며, 두 단계를 수렴할 때까지 실행하는 것은 계산적으로 너무 비싸게 됩니다. 이 근사는 soft actor-critic이라고 불리는 새로운 실용적인 알고리즘을 가져오게 됩니다.

Exhaustive sweeps implicitly devote equal time to all parts of the state space rather than focusing where it is needed.

철저한(exhaustive) 일괄처리는 상태 공간의 필요한 부분에 초점을 맞추기보다는 암암리에 상태 공간의 모든 부분에 동일한 시간을 할애한다.

More appealing is to distribute updates according to the on-policy distribution, that is, according to the distribution observed when following the current policy.

좀 더 매력적인 방법은 활성 정책 분포에 따라, 다시 말해 현재 정책을 따르는 동안 관측된 분포에 따라 갱신을 분산시키는 것이다.

In either case, sample state transitions and rewards are given by the model, and sample actions are given by the current policy.

어떤 경우든지, 표본 상태 전이와 보상은 모델에 의해 주어지고 표본 행동은 현재 정책에 의해 주어진다.

At any point in the planning process one can stop and exhaustively compute $v_{\tilde{\pi}}(s_0)$, the true value of the start state under the greedy policy, $\tilde{\pi}$, given the current action-value function $Q$, as an indication of how well the agent would do on a new episode on which it acted greedily (all the while assuming the model is correct).

계획 과정이 어떤 지점에서든 진행을 중단하고 $v_{\tilde{\pi}}(s_0)$를 철저하게 계산할 수 있다(이 모든 과정에서 모델이 정확하다고 가정한다). 이때 $v_{\tilde{\pi}}(s_0)$는 학습자가 탐욕적으로 행동할 새로운 에피소드에서 학습자가 얼마나 잘하는지를 나타내는 현재의 행동 가치 함수 $Q$가 주어졌을 때 탐욕적 정책 $\tilde{\pi}$ 하에서 시작 상태가 갖는 실제 가치를 나타낸다.

The quality of the policies found is plotted as a function of the number of expected updates completed. 

발견된 정책의 품질이 완성된 기댓값 갱신의 개수에 대한 함수로 그려져 있다.

This presumably is why the exhaustive, unfocused approach does better in the long run, at least for small problems.

짐작컨데, 이것이 아마도 철저하고 초점을 두지 않는 접근법이 적어도 작은 문제에서는 장기적으로 더 좋은 성능을 보여주는 이유일 것이다.

If trajectories can start only from a designated set of start states, and if you are interested in the prediction problem for a given policy, then on-policy trajectory sampling allows the algorithm to completely skip states that cannot be reached by the given policy from any of the start states: such states are irrelevant to the prediction problem.

만약 궤적이 일련이 지정된 시작 상태로부터만 시작할 수 있다면, 그리고 정책이 주어진 상황에서 예측 문제에 관심이 있다면, 활성 정책 궤적 표본추출은 알고리즘으로 하여금 주어진 정책으로는 어떤 시작상태로부터도 도달할 수 없는 상태를 완전히 건너뛰게 해 준다. 이러한 상태들은 예측 문제와는 무관하다(irrelevant).

all rewards for transitions from non-goal states are strictly negative

목표 상태가 아닌 상태로부터의 전이에 대한 보상은 철저하게 음의 값이다.

More generally, planning used in this way can look much deeper than one-step-ahead and evaluate action choices leading to many different predicted state and reward trajectories. Unlike the first use of planning, here planning focuses on a particular state. We call this *decision-time planning*.

좀 더 일반적으로는, 이러한 방식으로 사용된 계획은 한 단계 앞보다 더 멀리 내다볼 수 있고, 이를 통해 상태와 보상의 다양한 예측 궤적을 초래하는 행동 선택을 평가할 수 있다. 계획을 처음 사용할 때와는 다르게, 이 경우에는 계획이 특정 상태에 초점을 맞춘다. 이 책에서는 이것을 결정 시각 계획(*decision-time planning*)이라고 부른다.

Even when planning is only done at decision time, we can still view it, as we did in Section 8.1, as proceeding from simulated experience to updates and values, and ultimately to a policy.

계획이 오직 결정 시각에서만 행해질 때조차, (8.1절에서 그랬던 것처럼) 여전히 계획을 시뮬레이션된 경험으로부터 갱신과 가치, 그리고 궁극적으로는 정책으로 진행하는 것으로 볼 수 있다.

On the other hand, if low latency action selection is the priority,
then one is generally better off doing planning in the background to compute a policy that can then be rapidly applied to each newly encountered state.

반면에 지체 없는 행동 선택이 우선순위라면, 새롭게 마주친 상태에 재빠르게 적용할 수 있는 정책을 계산하기 위해 백그라운드에서 계획을 수행하는 편이 일반적으로 더 좋다.

The approximate value function is applied to the leaf nodes and then backed up toward the current state at the root.

근사적 가치 함수가 리프 노드에 적용되고 이후상태들의 근원 상태가 되는 현재 상태로 보강된다.

In fact, the value function is generally designed by people and never changed as a result of search.

사실, 가치함수는 일반적으로 사람들이 설계하는 것이고 결코 탐색의 결과로 가치 함수가 변하지는 않는다.

However, it is natural to consider allowing the value function to be improved over time, using either the backed-up values computed during heuristic search or any of the other methods presented throughout this book. In a sense we have taken this approach all along. 

하지만 경험적 탐색의 과정에서 계산되는 보강된 가치를 이용하거나 이 책에서 다루는 다른 다른 방법 중 어떤 것이라도 이용하여 가치 함수가 시간에 따라 향상되도록 허용하는 것을 고려하는 것은 자연스럽다.

It was only feasible to search ahead selectively a few steps, but even so the search resulted in significantly better action selections.

탐색을 몇 단계에 대해 선택적으로 할 수밖에 없었지만, 그렇다 하더라도 탐색은 상당히 더 좋은 행동을 선택하는 결과를 가져왔다.

We should not overlook the most obvious way in which heuristic search focuses updates: on the current state.

경험적 탐색이 갱신에 초점을 맞추는 가장 명백한 방법을 간과해서는 안 된다. 바로 현재 상태에 초점을 맞추는 것이다.

Much of the effectiveness of heuristic search is due to its search tree being tightly focused on the states and actions that might immediately follow the current state.

경험적 탐색의 효과는 대부분 경험적 탐색의 탐색트리(search tree)가 현재 상태에 바로 이어서 나올 상태와 행동에 집중해서 초점을 맞춘다는 사실로부터 나온다.

The distribution of updates can be altered in similar ways to focus on the current state and its likely successors.

갱신의 분포도 비슷한 방식으로 현재 상태와 그 이후의 가능한 상태들에 집중하도록 변경될 수 있다.

But the performance of the improved policy depends on properties of the rollout policy and the ranking of actions produced by the Monte Carlo value estimates. 

하지만 향상된 정책의 성능은 주사위 던지기 정책의 특성과 몬테카를로 가치 추정에 의해 만들어진 행동의 순위에 따라 달라진다.

Intuition suggests that the better the rollout policy and the more accurate the value estimates, the better the policy produced by a rollout algorithm is likely be (but see Gelly and Silver, 2007).

직관에 따르면 주사위 던지기 정책이 더 좋을수록 주사위 던지기 알고리즘의 더 정확한 가치 추정과 더 좋은 정책을 만들어 내는 것 같다.(하지만 겔리와 실버(Gelly and Silver 2007)를 참고하라)

The computation time needed by a rollout algorithm depends on the number of actions that have to be evaluated for each decision, the number of time steps in the simulated trajectories needed to obtain useful sample returns, the time it takes the rollout policy to make decisions, and the number of simulated trajectories needed to obtain good Monte Carlo action-value estimates.

각 결정에 대해 평가되어야만 하는 행동의 개수와 유용한 표본 보상을 얻는 데 필요한 시뮬레이션된 궤적에 포함된 시간 단계의 개수, 주사위 던지기 정책이 결정을 내리는 데 필요한 시간, 좋은 몬테카를로 행동 가치 추정값을 얻는 데 필요한 시뮬레이션된 궤적의 개수에 따라 주사위 던지기 알고리즘이 필요로 하는 계산 시간은 달라진다.


In this way they are like reinforcement learning algorithms in avoiding the exhaustive sweeps of dynamic programming by trajectory sampling, and in avoiding the need for distribution models by relying on sample, instead of expected, updates. Finally, rollout algorithms take advantage of the policy improvement property by acting greedily with respect to the estimated action values.

이러한 방식으로, 궤적 표본추출을 통해 동적 프로그래밍의 철저한 일괄처리를 회피하고 기댓값 갱신이 아닌 표본 갱신에 의존함으로써 분포 모델에 대한 필요성을 없앤다는 측면에서 주사위 던지기 알고리즘은 강화학습 알고리즘과 유사하다.

이 방법으로, 그들은 trajectory sampling을 통해 dynamic programming의 exhaustive sweeps를 피하고, expected update 대신 sample update에 의존함으로써 분포 모델의 필요성을 피하는 강화 학습 알고리즘과 유사합니다. 마지막으로, rollout 알고리즘은 추정된 행동 값에 대해 Greedy하게 행동함으로써 policy improvement 속성을 활용합니다.

At its base, MCTS is a rollout algorithm as described above, but enhanced by the addition of a means for accumulating value estimates obtained from the Monte Carlo simulations in order to successively direct simulations toward more highly-rewarding trajectories.

앞에서 말했듯이, 근본적으로 MCTS는 주사위 던지기 알고리즘이지만, 시뮬레이션이 더욱 큰 보상을 주는 궤적을 향하게 하는 연속적인 방향 설정을 위해 몬테카를로 시뮬레이션으로부터 얻은 가치 추정값을 축적하는 수단을 추가하면서 주사위 던지기 알고리즘보다 향상되었다.

Then, finally, an action from the root node (which still represents the current state of the environment) is selected according to some mechanism that depends on the accumulated statistics in the tree;

그런 다음, 마침내 트리 구조에 축적된 통계치에 영향을 받는 어떤 메커니즘에 따라 (여전히 환경의 현재 상태를 나타내는) 루트 노드로부터 발생한 하나의 행동이 선택된다.

At its base, MCTS is a decision-time planning algorithm based on Monte Carlo control applied to simulations that start from the root state; that is, it is a kind of rollout algorithm as described in the previous section.

근본적으로, MCTS는 근원 상태에서 시작하는 시뮬레이션에 적용된 몬테카를로 제어를 기반으로 하는 결정 시각 계획 알고리즘이다. 다시 말해, MCTS는 이전 절에서 설명한 것과 같은 일종의 주사위 던지기 알고리즘이다.

Another important dimension is the distribution of updates, that is, of the focus of search.

또 다른 중요한 차원은 갱신의 분포, 즉 탐색 초점의 분포다.