A diagonal matrix just scales the coordinates by the diagonal entries, so we can take our eigenvectors to be the unit coordinate vectors e1, e2, e3.

대각 행렬은 대각선 항목에 의해 좌표를 스케일링하기 때문에, 우리는 고유벡터를 단위 좌표 벡터 e1, e2, e3로 취급할 수 있습니다.

A has only one linearly independent eigenvector, so by the "only if" part of the diagonalization theorem, A is not diagonalizable.

A는 오직 하나의 선형 독립적인 고유벡터만을 가지므로, 대각화 정리의 "only if" 부분에 따라 A는 대각화될 수 없습니다.

Suppose you measure a data point x which you know for theoretical reasons must lie on a plane spanned by two vectors u and v.

이론적인 이유로 두 벡터 u와 v에 의해 span된 평면 위에 있어야 하는 데이터 포인트 x를 측정한다고 가정해 보세요.

The null space of the m x n matrix you get by "turning them sideways and smooshing them together."

"옆으로 돌리고 함께 눌러붙이는" 방식으로 얻은 m x n 행렬의 null space입니다.

Assigning a Tensor doesn’t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as Parameter, these temporaries would get registered too.

텐서를 할당하는 것은 이러한 효과가 없습니다. 이는 모델에 RNN의 마지막 hidden state와 같은 일시적인 상태를 캐시하고 싶을 수 있기 때문입니다. 만약 Parameter와 같은 클래스가 없다면, 이러한 임시 상태들도 등록될 것입니다.

It is a pain to verify using the row-column rule! Much easier: use associativity of linear transformations:

행-열 규칙을 사용하여 검증하는 것은 고통스럽습니다! 훨씬 쉬운 방법은 선형 변환의 결합성을 사용하는 것입니다:

You have to learn (and should understand, since it's well within what we've covered in class so far.)

배워야 합니다 (그리고 이해해야 합니다, 왜냐하면 그것은 우리가 지금까지 수업에서 다룬 범위 안에 잘 들어맞기 때문입니다.)

Information Retrieval investigates: Find relevant docs in a small and trust set

정보 검색은 다음을 조사합니다: 작고 신뢰할 수 있는 집합에서 관련 문서를 찾기

Leads to random suffer model:

random suffer 모델로 이어집니다:

PageRank = limiting probability of being at a page at any point in time.

PageRank = 시간상 어느 지점에서든 페이지에 있을 확률의 극한값입니다.

The tendency to anthropomorphize AI systems is one of the big obstacles in the way of actually trying to understand how AI might impact the world in the future.

AI 시스템에 인간화 경향을 부여하는 것은 실제로 AI가 미래에 세계에 어떤 영향을 미칠 수 있는지 이해하려는 노력에 있어 큰 장애물 중 하나입니다.

This allows to make the most of the accelerators and enables the algorithms to work with large learned environment models parameterized by deep neural networks.

이를 통해 가속기를 최대한 활용할 수 있고, 딥 뉴럴 네트워크로 매개변수화된 큰 학습 환경 모델과 함께 알고리즘이 작동할 수 있게 합니다.

One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning.

쓰라린 교훈에서 배워야 할 한 가지는 범용 방법의 강력한 힘입니다. 사용 가능한 계산량이 매우 많아지더라도 계산 증가에 따라 계속 확장되는 방법들이 있습니다. 이런 식으로 임의로 확장되는 것처럼 보이는 두 가지 방법은 검색과 학습입니다.

However, using search algorithms in combination with deep neural networks requires efficient implementations, typically written in fast compiled languages; this can come at the expense of usability and hackability, especially for researchers that are not familiar with C++.

그러나 검색 알고리즘을 딥 뉴럴 네트워크와 결합하여 사용하는 것은 일반적으로 빠른 컴파일 언어로 작성된 효율적인 구현을 필요로 하며, 이는 사용성과 해킹 가능성을 희생시킬 수 있습니다. 특히 C++에 익숙하지 않은 연구자들에게는 더욱 그렇습니다.

In Reinforcement Learning the agent must learn to interact with the environment in order to maximize a scalar reward signal.

강화 학습에서 에이전트는 스칼라 보상 신호를 최대화하기 위해 환경과 상호작용하는 방법을 배워야 합니다.

On each step the agent must select an action and receives in exchange an observation and a reward.

각 단계에서 에이전트는 행동을 선택해야 하며, 그 대가로 관측값과 보상을 받습니다.

Alternatively, search allows to select actions by constructing on the fly, in each state, a policy or a value function local to the current state, by searching using a learned model of the environment.

대안적으로, search는 학습된 환경 모델을 사용하여 검색함으로써, 각 상태에서 현재 상태에 로컬인 정책이나 가치 함수를 즉석에서 구성함으로써 행동을 선택할 수 있게 해줍니다.

Typically priors are needed to guide which nodes in the search tree to expand (to reduce the breadth of the tree that we construct), and value functions are used to estimate the value of incomplete paths in the tree that don't reach an episode termination (to reduce the depth of the search tree).

일반적으로 사전 지식은 검색 트리에서 어떤 노드를 확장할지 안내하는 데 필요합니다(우리가 구성하는 트리의 폭을 줄이기 위해), 그리고 가치 함수는 에피소드 종료에 도달하지 않는 트리의 불완전한 경로의 가치를 추정하는 데 사용됩니다(검색 트리의 깊이를 줄이기 위해).

The `RootFnOutput` contains the `prior_logits` from a policy network, the estimated `value` of the root state, and any `embedding` suitable to represent the root state for the environment model.

`RootFnOutput`은 정책 네트워크에서의 `prior_logits`, 루트 상태의 추정된 `value`, 그리고 환경 모델에 대한 루트 상태를 나타내기에 적합한 모든 `embedding`을 포함합니다.

A few fix-ups needed.

몇 가지 수정이 필요합니다.