Even when planning is only done at decision time, we can still view it, as we did in Section 8.1, as proceeding from simulated experience to updates and values, and ultimately to a policy.

계획이 오직 결정 시각에서만 행해질 때조차, (8.1절에서 그랬던 것처럼) 여전히 계획을 시뮬레이션된 경험으로부터 갱신과 가치, 그리고 궁극적으로는 정책으로 진행하는 것으로 볼 수 있다.

On the other hand, if low latency action selection is the priority,

then one is generally better off doing planning in the background to compute a policy that can then be rapidly applied to each newly encountered state.

반면에 지체 없는 행동 선택이 우선순위라면, 새롭게 마주친 상태에 재빠르게 적용할 수 있는 정책을 계산하기 위해 백그라운드에서 계획을 수행하는 편이 일반적으로 더 좋다.

The approximate value function is applied to the leaf nodes and then backed up toward the current state at the root.

근사적 가치 함수가 리프 노드에 적용되고 이후상태들의 근원 상태가 되는 현재 상태로 보강된다.

In fact, the value function is generally designed by people and never changed as a result of search.

사실, 가치함수는 일반적으로 사람들이 설계하는 것이고 결코 탐색의 결과로 가치 함수가 변하지는 않는다.

However, it is natural to consider allowing the value function to be improved over time, using either the backed-up values computed during heuristic search or any of the other methods presented throughout this book. In a sense we have taken this approach all along.

하지만 경험적 탐색의 과정에서 계산되는 보강된 가치를 이용하거나 이 책에서 다루는 다른 다른 방법 중 어떤 것이라도 이용하여 가치 함수가 시간에 따라 향상되도록 허용하는 것을 고려하는 것은 자연스럽다.

It was only feasible to search ahead selectively a few steps, but even so the search resulted in significantly better action selections.

탐색을 몇 단계에 대해 선택적으로 할 수밖에 없었지만, 그렇다 하더라도 탐색은 상당히 더 좋은 행동을 선택하는 결과를 가져왔다.

We should not overlook the most obvious way in which heuristic search focuses updates: on the current state.

경험적 탐색이 갱신에 초점을 맞추는 가장 명백한 방법을 간과해서는 안 된다. 바로 현재 상태에 초점을 맞추는 것이다.

Much of the effectiveness of heuristic search is due to its search tree being tightly focused on the states and actions that might immediately follow the current state.

경험적 탐색의 효과는 대부분 경험적 탐색의 탐색트리(search tree)가 현재 상태에 바로 이어서 나올 상태와 행동에 집중해서 초점을 맞춘다는 사실로부터 나온다.

The distribution of updates can be altered in similar ways to focus on the current state and its likely successors.

갱신의 분포도 비슷한 방식으로 현재 상태와 그 이후의 가능한 상태들에 집중하도록 변경될 수 있다.

But the performance of the improved policy depends on properties of the rollout policy and the ranking of actions produced by the Monte Carlo value estimates.

하지만 향상된 정책의 성능은 주사위 던지기 정책의 특성과 몬테카를로 가치 추정에 의해 만들어진 행동의 순위에 따라 달라진다.

Intuition suggests that the better the rollout policy and the more accurate the value estimates, the better the policy produced by a rollout algorithm is likely be (but see Gelly and Silver, 2007).

직관에 따르면 주사위 던지기 정책이 더 좋을수록 주사위 던지기 알고리즘의 더 정확한 가치 추정과 더 좋은 정책을 만들어 내는 것 같다.(하지만 겔리와 실버(Gelly and Silver 2007)를 참고하라)

The computation time needed by a rollout algorithm depends on the number of actions that have to be evaluated for each decision, the number of time steps in the simulated trajectories needed to obtain useful sample returns, the time it takes the rollout policy to make decisions, and the number of simulated trajectories needed to obtain good Monte Carlo action-value estimates.

각 결정에 대해 평가되어야만 하는 행동의 개수와 유용한 표본 보상을 얻는 데 필요한 시뮬레이션된 궤적에 포함된 시간 단계의 개수, 주사위 던지기 정책이 결정을 내리는 데 필요한 시간, 좋은 몬테카를로 행동 가치 추정값을 얻는 데 필요한 시뮬레이션된 궤적의 개수에 따라 주사위 던지기 알고리즘이 필요로 하는 계산 시간은 달라진다.

In this way they are like reinforcement learning algorithms in avoiding the exhaustive sweeps of dynamic programming by trajectory sampling, and in avoiding the need for distribution models by relying on sample, instead of expected, updates. Finally, rollout algorithms take advantage of the policy improvement property by acting greedily with respect to the estimated action values.

이러한 방식으로, 궤적 표본추출을 통해 동적 프로그래밍의 철저한 일괄처리를 회피하고 기댓값 갱신이 아닌 표본 갱신에 의존함으로써 분포 모델에 대한 필요성을 없앤다는 측면에서 주사위 던지기 알고리즘은 강화학습 알고리즘과 유사하다.

이 방법으로, 그들은 trajectory sampling을 통해 dynamic programming의 exhaustive sweeps를 피하고, expected update 대신 sample update에 의존함으로써 분포 모델의 필요성을 피하는 강화 학습 알고리즘과 유사합니다. 마지막으로, rollout 알고리즘은 추정된 행동 값에 대해 Greedy하게 행동함으로써 policy improvement 속성을 활용합니다.

At its base, MCTS is a rollout algorithm as described above, but enhanced by the addition of a means for accumulating value estimates obtained from the Monte Carlo simulations in order to successively direct simulations toward more highly-rewarding trajectories.

앞에서 말했듯이, 근본적으로 MCTS는 주사위 던지기 알고리즘이지만, 시뮬레이션이 더욱 큰 보상을 주는 궤적을 향하게 하는 연속적인 방향 설정을 위해 몬테카를로 시뮬레이션으로부터 얻은 가치 추정값을 축적하는 수단을 추가하면서 주사위 던지기 알고리즘보다 향상되었다.

Then, finally, an action from the root node (which still represents the current state of the environment) is selected according to some mechanism that depends on the accumulated statistics in the tree;

그런 다음, 마침내 트리 구조에 축적된 통계치에 영향을 받는 어떤 메커니즘에 따라 (여전히 환경의 현재 상태를 나타내는) 루트 노드로부터 발생한 하나의 행동이 선택된다.

At its base, MCTS is a decision-time planning algorithm based on Monte Carlo control applied to simulations that start from the root state; that is, it is a kind of rollout algorithm as described in the previous section.

근본적으로, MCTS는 근원 상태에서 시작하는 시뮬레이션에 적용된 몬테카를로 제어를 기반으로 하는 결정 시각 계획 알고리즘이다. 다시 말해, MCTS는 이전 절에서 설명한 것과 같은 일종의 주사위 던지기 알고리즘이다.

Another important dimension is the distribution of updates, that is, of the focus of search.

또 다른 중요한 차원은 갱신의 분포, 즉 탐색 초점의 분포다.

There is no need in principle to include a separate function approximator for the state value, since it is related to the Q-function and policy according to Equation 3.

원칙적으로 상태 가치에 대한 별도의 function approximator를 포함할 필요가 없으며, 이는 Equation 3에 따라 Q-function과 policy와 관련이 있기 때문입니다.

This quantity can be estimated from a single action sample from the current policy without introducing a bias, but in practice, including a separate function approximator for the soft value can stabilize training and is convenient to train simultaneously with the other networks.

이 양은 현재 policy에서 단일 행동 샘플로부터 bias 없이 추정할 수 있지만, 실제로는 soft value에 대한 별도의 function approximator를 포함하는 것이 훈련을 안정화시키고 다른 네트워크와 동시에 훈련하기에 편리합니다.

Alternatively, we can update the target weights to match the current value function weights periodically (see Appendix E.)

또는, 우리는 target weights를 주기적으로 현재 value function weights와 일치시키도록 업데이트할 수 있습니다 (Appendix E 참조).

However, in our case, the target density is the Q-function, which is represented by a neural network an can be differentiated, and it is thus convenient to apply the reparameterization trick instead, resulting in a lower variance estimator.

그러나 우리의 경우, target 밀도는 신경망에 의해 표현되고 미분될 수 있는 Q-function이며, 따라서 reparameterization trick을 적용하는 것이 편리하며, 이는 더 낮은 분산 추정값을 가져오게 됩니다.

Our algorithm also makes use of two Q-functions to mitigate positive bias in the policy improvement step that is known to degrade performance of value based methods (Hasselt, 2010; Fujimoto et al., 2018).

우리 알고리즘은 또한 정책 개선 단계에서의 positive bias를 완화하고 value based methods의 성능 저하를 알려진 두 개의 Q-functions를 사용합니다 (Hasselt, 2010; Fujimoto et al., 2018).

Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks.

Model-free 딥 강화학습(RL) 알고리즘은 다양한 도전적인 의사결정 및 제어 작업에 대해 입증되었습니다.

PettingZoo models games as Agent Environment Cycle (AEC) games, and thus can support any game multi-agent RL can consider, allowing for fantastically weird cases.

PettingZoo는 게임을 Agent Environment Cycle (AEC) 게임으로 모델링하며, 따라서 multi-agent RL이 고려할 수 있는 모든 게임을 지원할 수 있어, 환상적으로 이상한 경우들을 허용합니다.

Because of this, our API includes lower level functions and attributes that you probably won’t need but are very important when you do. Their functionality is used to implement the high-level functions above though, so including them is just a matter of code factoring.

이 때문에, 우리의 API는 여러분이 아마 필요로 하지 않을 낮은 수준의 함수와 속성들을 포함하고 있지만, 필요할 때 매우 중요합니다. 그들의 기능은 위의 고수준 함수들을 구현하는 데 사용되므로, 그들을 포함하는 것은 단지 코드 요소화의 문제입니다.

While not required by the base API, most downstream wrappers and utilities depend on the following attributes and methods, and they should be added to new environments except in special circumstances where adding one or more is not possible.

기본 API에 의해 필수적이지는 않지만, 대부분의 다운스트림 래퍼와 유틸리티들은 다음의 속성과 메소드에 의존하며, 하나 이상을 추가하는 것이 불가능한 특별한 상황을 제외하고는 새로운 환경에 추가되어야 합니다.

After this vacuous step is taken, the agent will be removed from agents and other changeable attributes.

이 무의미한 단계를 거친 후에, 에이전트는 agents 및 기타 변경 가능한 속성들에서 제거될 것입니다.

Agent generation can just be done with appending it to agents and the other changeable attributes (with it already being in the possible agents and action/observation spaces), and transitioning to it at some point with agent_iter.

에이전트 생성은 단순히 에이전트들과 다른 변경 가능한 속성들에 추가함으로써 이루어질 수 있습니다(이미 가능한 에이전트들과 action/observation 공간 안에 존재함), 그리고 어느 시점에서 agent_iter를 사용하여 그것으로 전환됩니다.

We systematically evaluate SAC on a range of benchmark tasks, as well as challenging real-world tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand.

우리는 SAC를 다양한 벤치마크 작업뿐만 아니라 사지 로봇의 이동이나 민첩한 손을 이용한 로봇 조작과 같은 도전적인 실제 작업에 대해서도 체계적으로 평가합니다.

This quickly becomes extravagantly expensive, as the number of gradient steps and samples per step needed to learn an effective policy increases with task complexity.

이것은 과제의 복잡성이 증가함에 따라 효과적인 정책을 학습하기 위해 필요한 그라디언트 단계 수와 단계당 샘플 수가 증가함에 따라 빠르게 과도하게 비용이 많이 드는 일이 됩니다.

Unlike in conventional reinforcement learning, where the optimal policy is independent of scaling of the reward function, in maximum entropy reinforcement learning the scaling factor has to be compensated by the choice of a suitable temperature, and a sub-optimal temperature can drastically degrade performance (Haarnoja et al., 2018c).

전통적인 강화학습에서 optimal policy가 reward function의 스케일링과 독립적인 것과 달리, 최대 엔트로피 강화학습에서는 스케일링 인자를 적절한 temperature의 선택으로 보상해야 하며, 최적이 아닌 temperature는 성능을 급격히 저하시킬 수 있습니다 (Haarnoja et al., 2018c).

Maximum entropy reinforcement learning generalizes the expected return RL objective, although the original objective can be recovered in the zero temperature limit (Haarnoja et al., 2017).

최대 엔트로피 강화학습은 예상 수익 RL 목표를 일반화하지만, 원래의 목표는 영 온도 limit에서 회복될 수 있습니다 (Haarnoja et al., 2017).

There have been efforts to increase the sample efficiency while retaining robustness by incorporating off-policy samples and by using higher order variance reduction techniques (O’Donoghue et al., 2016; Gu et al., 2016).

샘플 효율성을 증가시키면서 견고성을 유지하기 위해 off-policy 샘플을 통합하고 고차 분산 감소 기술을 사용하는 노력이 있었습니다 (O’Donoghue et al., 2016; Gu et al., 2016).

Crucially, the convergence of this method hinges on how well this sampler approximates the true posterior.

중요하게도, 이 방법의 수렴은 이 샘플러가 얼마나 잘 실제 사후확률을 근사하는지에 달려 있습니다.

Furthermore, these prior maximum entropy methods generally do not exceed the performance of state-of-the-art off-policy algorithms, such as TD3 (Fujimoto et al., 2018) or MPO (Abdolmaleki et al., 2018), when learning from scratch, though they may have other benefits, such as improved exploration and ease of fine-tuning.

또한, 이러한 이전의 최대 엔트로피 방법들은 일반적으로 처음부터 학습할 때 최신 off-policy 알고리즘들, 예를 들어 TD3 (Fujimoto et al., 2018)이나 MPO (Abdolmaleki et al., 2018)의 성능을 초과하지 않지만, 개선된 탐색과 미세 조정의 용이성과 같은 다른 이점들을 가질 수 있습니다.

We will also use ρπ(st) and ρπ(st, at) to denote the state and state-action marginals of the trajectory distribution induced by a policy π(at|st).

또한, 정책 π(at|st)에 의해 유도된 궤적 분포의 상태와 상태-행동 마진을 나타내기 위해 ρπ(st)와 ρπ(st, at)를 사용할 것입니다.

In the context of policy search algorithms, the use of a discount factor is actually a somewhat nuanced choice, and writing down the precise objective that is optimized when using the discount factor is non-trivial (Thomas, 2014).

policy 탐색 알고리즘의 맥락에서, discount factor의 사용은 실제로 다소 미묘한 선택이며, discount factor를 사용할 때 최적화되는 정확한 목표를 기술하는 것은 사소한 일이 아닙니다 (Thomas, 2014).

These prior methods have proposed directly solving for the optimal Q-function, from which the optimal policy can be recovered (Ziebart et al., 2008; Fox et al., 2016; Haarnoja et al., 2017).

이러한 이전 방법들은 최적의 Q-function을 직접 해결하는 것을 제안했으며, 이로부터 최적의 policy를 회복할 수 있습니다 (Ziebart et al., 2008; Fox et al., 2016; Haarnoja et al., 2017).

In the following section, we discuss how we can devise a soft actor-critic algorithm through a policy iteration formulation, where we instead evaluate the Q-function of the current policy and update the policy through an off-policy gradient update.

다음 섹션에서, 우리는 policy 반복 공식을 통해 소프트 actor-critic 알고리즘을 어떻게 고안할 수 있는지에 대해 논의할 것이며, 여기서 우리는 현재 policy의 Q-function을 평가하고 off-policy 그라디언트 업데이트를 통해 policy를 업데이트합니다.

We will first present this derivation, verify that the corresponding algorithm converges to the optimal policy from its density class, and then present a practical deep reinforcement learning algorithm based on this theory.

먼저 이러한 유도를 제시하고, 해당 알고리즘이 밀도 class에서 최적의 policy로 수렴하는지 확인한 다음, 이 이론을 기반으로 한 실용적인 deep 강화 학습 알고리즘을 제시할 것입니다.

We will first present this derivation, verify that the corresponding algorithm converges to the optimal policy from its density class, and then present a practical deep RL algorithm based on this theory.

먼저 이 유도를 제시하고, 해당 알고리즘이 밀도 class에서 최적의 policy로 수렴하는지 확인한 후, 이 이론을 기반으로 한 실용적인 deep RL 알고리즘을 제시할 것입니다.

We will show that soft policy iteration converges to the optimal policy within a set of policies which might correspond, for instance, to a set of parameterized densities.

우리는 soft policy 반복이 예를 들어 파라미터화된 밀도들의 집합에 해당할 수 있는 일련의 policy 내에서 최적의 policy로 수렴함을 보여줄 것입니다.

This particular choice of update can be guaranteed to result in an improved policy in terms of its soft value.

이 특정한 업데이트 선택은 soft value 측면에서 개선된 policy를 가져오는 것을 보장할 수 있습니다.

Finally, the policy parameters can be learned by directly minimizing the expected KL-divergence in Equation 4 (multiplied by α and ignoring the constant log-partition function and by

마지막으로, policy 파라미터들은 방정식 4의 예상 KL-발산을 직접 최소화함으로써 학습될 수 있습니다 (α로 곱하고 상수 로그-파티션 함수는 무시하며).

Furthermore, we find the ubiquitous solution in the discrete action setting, Double DQN (Van Hasselt et al., 2016), to be ineffective in an actor-critic setting.

또한, 우리는 이산 행동 설정에서 널리 사용되는 솔루션인 Double DQN (Van Hasselt et al., 2016)이 actor-critic 설정에서는 비효과적임을 발견했습니다.

While this allows for a less biased value estimation, even an unbiased estimate with high variance can still lead to future overestimations in local regions of state space, which in turn can negatively affect the global policy.

이것은 덜 편향된 가치 추정을 허용하지만, 높은 분산을 가진 비편향 추정조차도 상태 공간의 지역적 영역에서 미래의 과대추정으로 이어질 수 있으며, 이는 차례로 전역 policy에 부정적인 영향을 미칠 수 있습니다.

To address this concern, we propose a clipped Double Q-learning variant which leverages the notion that a value estimate suffering from overestimation bias can be used as an approximate upper-bound to the true value estimate.

이러한 우려를 해결하기 위해, 과대추정 편향으로 인해 고통받는 가치 추정치를 진정한 가치 추정치에 대한 대략적인 상한선으로 사용할 수 있다는 개념을 활용하는 clipped Double Q-learning 변형을 제안합니다.

Given the recent concerns in reproducibility (Henderson et al., 2017), we run our experiments across a large number of seeds with fair evaluation metrics, perform ablation studies across each contribution, and open source both our code and learning curves.

최근 재현성에 대한 우려(Henderson et al., 2017)를 감안하여, 우리는 공정한 평가 지표를 가진 많은 수의 시드에 걸쳐 실험을 수행하고, 각 기여에 대해 소거 연구를 수행하며, 우리의 코드와 학습 곡선을 모두 오픈소스로 제공합니다.

Given current policy parameters φ, let φ_approx define the parameters from the actor update induced by the maximization of the approximate critic Q_θ(s, a) and φ_true the parameters from the hypothetical actor update with respect to the true underlying value function Qπ (s, a) (which is not known during learning):

현재 policy 파라미터 φ가 주어졌을 때, φ_approx는 근사 critic Q_θ(s, a)의 최대화에 의해 유도된 actor 업데이트의 파라미터를 정의하고, φ_true는 학습 중에는 알려지지 않은 진정한 기저 가치 함수 Qπ (s, a)에 대한 가상의 actor 업데이트의 파라미터를 정의합니다.

Firstly, the overestimation may develop into a more significant bias over many updates if left unchecked.

첫째, 과대추정은 검사하지 않고 방치할 경우 많은 업데이트에 걸쳐 더 중요한 편향으로 발전할 수 있습니다.

A very clear overestimation bias occurs from the learning procedure.

학습 절차로부터 매우 명확한 과대추정 편향이 발생합니다.

We further show this reduction is not sufficient experimentally in Section 6.1.

우리는 추가적으로 이 감소가 실험적으로 충분하지 않음을 섹션 6.1에서 보여줍니다.

However, the critics are not entirely independent due to the use of the opposite critic in the learning targets and the same replay buffer.

그러나, learning targets에서 opposite critic의 사용과 같은 replay buffer 때문에, critics는 완전히 독립적이지 않습니다.

It can then be shown that rather than learning an estimate of the expected return, the value estimate approximates the expected return minus the expected discounted sum of future TD-errors:

따라서 expected return의 추정을 학습하는 것보다는, 가치 추정이 expected return에서 미래 TD-errors의 expected 할인된 합을 뺀 값에 근사한다는 것을 보여줄 수 있습니다.

While a larger d would result in a larger benefit with respect to accumulating errors, for fair comparison, the critics are only trained once per time step, and training the actor for too few iterations would cripple learning. Both target networks are updated with τ = 0.005.

더 큰 d는 누적 오류에 대한 더 큰 이점을 가져올 것이지만, 공정한 비교를 위해 critics는 타임 스텝마다 한 번만 훈련되며, actor를 너무 적은 반복으로 훈련하는 것은 학습을 저해할 것입니다. 두 target 네트워크는 τ = 0.005로 업데이트됩니다.

To remove the dependency on the initial parameters of the policy, we use a purely exploratory policy for the first 10,000 time steps of stable length environments (HalfCheetah-v1 and Ant-v1) and the first 1,000 time steps for the remaining environments.

policy의 초기 파라미터에 대한 의존성을 제거하기 위해, 우리는 안정적인 길이 환경(HalfCheetah-v1 및 Ant-v1)의 처음 10,000 타임 스텝과 나머지 환경들에 대한 처음 1,000 타임 스텝 동안 순수하게 탐색적인 policy를 사용합니다.

Unlike the original implementation of DDPG, we used uncorrelated noise for exploration as we found noise drawn from the Ornstein-Uhlenbeck (Uhlenbeck & Ornstein, 1930) process offered no performance benefits.

원래 DDPG 구현과 달리, 우리는 Ornstein-Uhlenbeck (Uhlenbeck & Ornstein, 1930) 과정에서 추출된 noise가 성능 이점을 제공하지 않는다는 것을 발견하여 탐색을 위해 상관관계가 없는 noise를 사용했습니다.

Additionally, we compare our method with our re-tuned version of DDPG, which includes all architecture and hyper-parameter modifications to DDPG without any of our proposed adjustments.

또한, 우리는 제안된 조정 없이 DDPG에 대한 모든 아키텍처 및 하이퍼-파라미터 수정을 포함하는 DDPG의 재조정된 버전과 우리의 방법을 비교합니다.

This is reflected empirically, as both methods result in insignificant improvements over TD3 - CDQ, with an exception in the Ant-v1 environment, which appears to benefit greatly from any overestimation reduction.

이것은 경험적으로 반영되어 있으며, Ant-v1 환경에서의 예외를 제외하고, 두 방법 모두 TD3 - CDQ에 비해 미미한 개선을 가져옵니다. Ant-v1 환경은 어떤 과대추정 감소에서도 큰 이익을 얻는 것으로 보입니다.

As the inclusion of Clipped Double Q-learning into our full method outperforms both prior methods, this suggests that subduing the overestimations from the unbiased estimator is an effective measure to improve performance.

Clipped Double Q-learning을 우리의 전체 방법에 포함시키는 것이 이전 두 방법보다 더 우수한 성능을 보이는 것으로 나타나, 비편향 추정기에서의 과대추정을 억제하는 것이 성능 향상을 위한 효과적인 조치임을 시사합니다.

Our work investigates the importance of a standard technique in deep RL, target networks, and examines their role in limiting errors from imprecise function approximation and stochastic optimization.

우리의 연구는 deep RL에서의 표준 기술인 target 네트워크의 중요성을 조사하고, 부정확한 함수 근사와 확률적 최적화로부터 오류를 제한하는 그들의 역할을 검토합니다.

Finally, we introduce a SARSA-style regularization technique which modifies the temporal difference target to bootstrap off similar state-action pairs.

마지막으로, 우리는 비슷한 상태-행동 쌍으로부터 부트스트랩하기 위해 temporal difference target을 수정하는 SARSA 스타일의 정규화 기법을 도입합니다.

Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play.

어떠한 선행 탐색 없이도, 신경망은 수천 개의 무작위 self-play 게임을 시뮬레이션하는 최신 몬테카를로 트리 탐색 프로그램 수준에서 바둑을 둡니다.

Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the European Go champion by 5 games to 0.

이 탐색 알고리즘을 사용하여, 우리의 프로그램 AlphaGo는 다른 바둑 프로그램들에 대해 99.8%의 승률을 달성했으며, 유럽 바둑 챔피언을 5대 0으로 이겼습니다.

This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.

이것은 컴퓨터 프로그램이 전체 크기의 바둑 게임에서 인간 프로 선수를 처음으로 이긴 사건으로, 이전에는 적어도 10년은 더 걸릴 것으로 생각되었던 업적입니다.

These policies are used to narrow the search to a beam of high probability actions, and to sample actions during rollouts.

이러한 정책들은 탐색을 높은 확률의 행동들로 좁히는 빔으로 제한하고, 롤아웃하는 동안 행동들을 샘플링하기 위해 사용됩니다.

We use these neural networks to reduce the effective depth and breadth of the search tree: evaluating positions using a value network, and sampling actions using a policy network.

우리는 이러한 신경망을 사용하여 탐색 트리의 효과적인 깊이와 너비를 줄입니다: 가치 네트워크를 사용하여 위치를 평가하고, 정책 네트워크를 사용하여 행동을 샘플링합니다.

Next, we train a reinforcement learning (RL) policy network, p_ρ, that improves the SL policy network by optimizing the final outcome of games of self-play. This adjusts the policy towards the correct goal of winning games, rather than maximizing predictive accuracy.

다음으로, 자체 플레이 게임의 최종 결과를 최적화함으로써 SL 정책 네트워크를 개선하는 강화 학습(RL) 정책 네트워크, p_ρ을 훈련합니다. 이는 예측 정확성을 최대화하기보다는 게임을 이기는 것이 올바른 목표로 정책을 조정합니다.

We play games between the current policy network p_ρ and a randomly selected previous iteration of the policy network.

우리는 현재 정책 네트워크 p_ρ와 무작위로 선택된 이전 반복의 정책 네트워크 사이에서 게임을 진행합니다.

The mean squared error between the predicted value and the actual game outcome is plotted against the stage of the game (how many moves had been played in the given position).

예측된 값과 실제 게임 결과 사이의 평균 제곱 오차는 게임의 단계(주어진 위치에서 얼마나 많은 수가 놓여졌는지)에 대해 플롯됩니다.

Figure 2,b shows the position evaluation accuracy of the value network, compared to Monte-Carlo rollouts using the fast rollout policy pπ; the value function was consistently more accurate. A single evaluation of vθ(s) also approached the accuracy of Monte-Carlo rollouts using the RL policy network pρ, but using 15,000 times less computation.

그림 2,b는 빠른 롤아웃 정책 pπ를 사용하는 몬테카를로 롤아웃과 비교하여 가치 네트워크의 위치 평가 정확도를 보여줍니다; 가치 함수는 일관되게 더 정확했습니다. 단일 vθ(s) 평가는 RL 정책 네트워크 pρ를 사용하는 몬테카를로 롤아웃의 정확도에도 접근했지만, 15,000배 적은 계산을 사용했습니다.

The tree is traversed by simulation (i.e., descending the tree in complete games without backup), starting from the root state.

트리는 루트 상태에서 시작하여 시뮬레이션(즉, 백업 없이 완전한 게임에서 트리를 내려가며)을 통해 탐색됩니다.

Evaluating policy and value networks requires several orders of magnitude more computation than traditional search heuristics.

정책 및 가치 네트워크를 평가하는 데는 전통적인 탐색 휴리스틱보다 몇 차수 더 많은 계산이 필요합니다.

In addition, we included the open source program GnuGo, a Go program using state-of-the-art search methods that preceded MCTS.

또한, MCTS보다 앞선 최첨단 검색 방법을 사용하는 바둑 프로그램인 오픈 소스 프로그램 GnuGo를 포함했습니다.

In addition, we included the open source program GnuGo, a Go program using state-of-the-art search methods that preceded MCTS.

또한, MCTS보다 앞선 최첨단 검색 방법을 사용하는 바둑 프로그램인 오픈 소스 프로그램 GnuGo를 포함했습니다.

Programs were evaluated on an Elo scale: a 230 point gap corresponds to a 79% probability of winning, which roughly corresponds to one amateur dan rank advantage on KGS;

프로그램들은 Elo 척도로 평가되었습니다: 230점의 차이는 승리 확률 79%에 해당하며, 이는 대략 KGS에서 한 아마추어 단급의 우위와 대응됩니다;

Games against the human European champion Fan Hui were also included; these games used longer time controls. 95% confidence intervals are shown.

인간 유럽 챔피언 판 후이와의 게임도 포함되었습니다; 이 게임들은 더 긴 시간 제어를 사용했습니다. 95% 신뢰 구간이 표시되어 있습니다.

Our program AlphaGo integrates these components together, at scale, in a high-performance tree search engine.

우리의 프로그램 AlphaGo는 이러한 구성 요소들을 대규모로 고성능 트리 검색 엔진에 통합합니다.

It is natural to wonder whether the same approach can be followed as for stochastic policies: adjusting the policy parameters in the direction of the policy gradient.

확률적 정책에 대해 취해진 것과 같은 접근 방식을 따를 수 있는지 궁금해하는 것은 자연스러운 일입니다: 정책 그라디언트 방향으로 정책 파라미터를 조정하는 것입니다.

We apply our deterministic actor-critic algorithms to several benchmark problems: a high-dimensional bandit; several standard benchmark reinforcement learning tasks with low dimensional action spaces; and a high-dimensional task for controlling an octopus arm.

우리는 결정론적 actor-critic 알고리즘을 여러 벤치마크 문제에 적용합니다: 고차원 밴딧; 낮은 차원의 행동 공간을 가진 여러 표준 벤치마크 강화 학습 작업; 그리고 문어 팔을 제어하는 고차원 작업입니다.

Finally, there are many applications (for example in robotics) where a differentiable control policy is provided, but where there is no functionality to inject noise into the controller.

마지막으로, (예를 들어 로봇 공학에서) 미분 가능한 제어 정책이 제공되지만 컨트롤러에 잡음을 주입하는 기능이 없는 많은 응용 분야가 있습니다.

