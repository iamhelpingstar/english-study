A diagonal matrix just scales the coordinates by the diagonal entries, so we can take our eigenvectors to be the unit coordinate vectors e1, e2, e3.

대각 행렬은 대각선 항목에 의해 좌표를 스케일링하기 때문에, 우리는 고유벡터를 단위 좌표 벡터 e1, e2, e3로 취급할 수 있습니다.

A has only one linearly independent eigenvector, so by the "only if" part of the diagonalization theorem, A is not diagonalizable.

A는 오직 하나의 선형 독립적인 고유벡터만을 가지므로, 대각화 정리의 "only if" 부분에 따라 A는 대각화될 수 없습니다.

Suppose you measure a data point x which you know for theoretical reasons must lie on a plane spanned by two vectors u and v.

이론적인 이유로 두 벡터 u와 v에 의해 span된 평면 위에 있어야 하는 데이터 포인트 x를 측정한다고 가정해 보세요.

The null space of the m x n matrix you get by "turning them sideways and smooshing them together."

"옆으로 돌리고 함께 눌러붙이는" 방식으로 얻은 m x n 행렬의 null space입니다.

Assigning a Tensor doesn’t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as Parameter, these temporaries would get registered too.

텐서를 할당하는 것은 이러한 효과가 없습니다. 이는 모델에 RNN의 마지막 hidden state와 같은 일시적인 상태를 캐시하고 싶을 수 있기 때문입니다. 만약 Parameter와 같은 클래스가 없다면, 이러한 임시 상태들도 등록될 것입니다.

It is a pain to verify using the row-column rule! Much easier: use associativity of linear transformations:

행-열 규칙을 사용하여 검증하는 것은 고통스럽습니다! 훨씬 쉬운 방법은 선형 변환의 결합성을 사용하는 것입니다:

You have to learn (and should understand, since it's well within what we've covered in class so far.)

배워야 합니다 (그리고 이해해야 합니다, 왜냐하면 그것은 우리가 지금까지 수업에서 다룬 범위 안에 잘 들어맞기 때문입니다.)