Are we supposed to initialize them all manually, essentially repeating what we already write in the forward pass definition?

우리는 기본적으로 순방향 패스 정의에서 이미 작성한 것을 반복하면서 모두 수동으로 초기화해야 하는 것인가요?

Greater difficulties arise when the environment changes to become better than it was before, and yet the formerly correct policy does not reveal the improvement. In these cases the modeling error may not be detected for a long time, if ever.

환경이 이전보다 개선되기 위해 바뀌었지만 이전에 올바른 정책이 개선된 점을 드러내지 않을 때 더 큰 어려움이 발생합니다. 이러한 경우 모델링 오류가 오랫동안 감지되지 않을 수 있습니다.

In a stochastic environment, variations in estimated transition probabilities also contribute to variations in the sizes of changes and in the urgency with which pairs need to be updated.

확률적 환경에서 추정된 전이 확률의 변화는 변화의 크기와 쌍을 업데이트해야 하는 긴급도의 변화에도 기여합니다.

The important feature of functional programming to grok when working with JAX is very simple: don’t write code with side-effects.

JAX를 사용할 때 이해해야 할 함수형 프로그래밍의 중요한 특징은 매우 간단합니다: side-effects가 있는 코드를 작성하지 마세요.

In this example, this wouldn’t actually help speed anyway, for many reasons, but treat this as a toy model of wanting to JIT-compile the update of model parameters, where `jax.jit` makes an enormous difference.

이 예시에서는 여러 이유로 실제로 속도를 향상시키지는 못하지만, 이것을 모델 매개변수의 업데이트를 JIT 컴파일하고자 하는 장난감 모델로 취급하세요, 여기서 `jax.jit`는 엄청난 차이를 만듭니다.

Implicit in that point of view is that expected updates, if possible, are preferable to sample updates.

이러한 관점에 내재된 생각은, 가능하기만 하다면 기댓값 갱신이 표본 갱신보다 더 선호할 만하다는 것이다.

The difference between these expected and sample updates is significant to the extent that the environment is stochastic, specifically, to the extent that, given a state and action, many possible next states may occur with various probabilities.

환경이 확률론적인 경우에 한해, 좀 더 분명히 말하면 상태와 행동이 주어졌을 때 가능성 있는 많은 다음 상태들이 다양한 확률로 발생할 수 있는 경우에 한해, 이러한 기댓값 갱신과 표본 갱신의 차이점은 중요한 의미를 갖는다.

The sample update is in addition affected by sampling error. On the other hand, the sample update is cheaper computationally because it considers only one next state, not all possible next states.

표본 갱신은 추가로 표본 오차에 의한 영향을 받는다. 반면에, 표본 오차는 가능한 모든 다음 상태가 아니라 오직 하나의 다음 상태만을 고려하기 때문에 많은 계산을 필요로 하지 않는다.

Given a unit of computational effort, is it better devoted to a few expected updates or to b times as many sample updates?

한 단위의 계산량이 몇 개의 기댓값 갱신에 사용되는 것이 좋은가? 아니면 개수가 b배인 표본 갱신에 사용되는 것이 좋은가?

For these cases, many state–action pairs could have their values improved dramatically, to within a few percent of the effect of an expected update, in the same time that a single state–action pair could undergo an expected update.

이 경우에 많은 수의 상태-행동 쌍들은 기댓값 갱신이 가치를 향상시키는 효과의 몇 퍼센트 이내로 그들의 가치를 극적으로 향상시킬 수 있었을 것이다. 하지만 이때 소요되는 시간은 고작 하나의 상태-행동 쌍이 기댓값 갱신을 수행하는 시간과 같은 시간이다.

We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic.

$$y=r + \gamma Q_{\theta'}(s', \pi_{\phi}(s')). \tag{8}$$

이 문제가 액터-크리틱 설정에서도 지속되는 것을 보여주고, 액터와 크리틱 양쪽 모두에 미치는 영향을 최소화하기 위한 새로운 메커니즘을 제안합니다.

$$y=r + \gamma Q_{\theta'}(s', \pi_{\phi}(s')). \tag{8}$$

In reinforcement learning problems with discrete action spaces, the issue of value overestimation as a result of function approximation errors is well-studied.

이산 동작 공간을 갖는 강화학습 문제에서, 함수 근사 오류로 인한 가치 과대평가 문제는 잘 연구되어 있습니다.

Overestimation bias is a property of Q-learning in which the maximization of a noisy value estimate induces a consistent overestimation.

과대평가 편향은 Q-러닝의 특성으로, 잡음이 섞인 가치 추정치의 최대화가 일관된 과대평가를 유발한다는 것입니다.

This inaccuracy is further exaggerated by the nature of temporal difference learning (Sutton, 1988), in which an estimate of the value function is updated using the estimate of a subsequent state.

이러한 부정확함은 시간차 학습(Sutton, 1988)의 특성에 의해 더욱 과장됩니다. 여기서 가치 함수의 추정치는 후속 상태의 추정치를 사용하여 업데이트됩니다.

During training, Double DQN estimates the value of the current policy with a separate target value function, allowing actions to be evaluated without maximization bias.

트레이닝 중에, 더블 DQN은 별도의 목표 가치 함수를 사용하여 현재 정책의 가치를 추정하며, 이를 통해 행동을 최대화 편향 없이 평가할 수 있습니다.

This favors underestimations, which do not tend to be propagated during learning, as actions with low value estimates are avoided by the policy.

이는 학습 동안 전파되지 않는 경향이 있는 저평가를 선호합니다. 왜냐하면 낮은 가치 추정치를 가진 행동들은 정책에 의해 피해지기 때문입니다.

Given the connection of noise to overestimation bias, this paper contains a number of components that address variance reduction.

잡음과 과대평가 편향 사이의 연관성을 고려할 때, 이 논문은 분산 감소를 다루는 여러 구성 요소를 포함하고 있습니다.

Second, to address the coupling of value and policy, we propose delaying policy updates until the value estimate has converged. Finally, we introduce a novel regularization strategy, where a SARSA-style update bootstraps similar action estimates to further reduce variance.

둘째, 가치와 정책의 결합 문제를 해결하기 위해, 가치 추정치가 수렴할 때까지 정책 업데이트를 지연시키는 것을 제안합니다. 마지막으로, SARSA 스타일 업데이트가 유사한 행동 추정치를 부트스트랩하여 분산을 더욱 줄이는 새로운 정규화 전략을 소개합니다.

an actor-critic algorithm which considers the interplay between function approximation error in both policy and value updates.

정책과 가치 업데이트에서 함수 근사 오류 간의 상호작용을 고려하는 액터-크리틱 알고리즘입니다.

Our work focuses on two outcomes that occur as the result of estimation error, namely overestimation bias and a high variance build-up.

우리의 연구는 추정 오류의 결과로 발생하는 두 가지 결과, 즉 과대평가 편향과 높은 분산 축적에 초점을 맞춥니다.

Other approaches have focused directly on reducing the variance (Anschel et al., 2017), minimizing over-fitting to early high variance estimates (Fox et al., 2016), or through corrective terms (Lee et al., 2013).

다른 접근법들은 분산을 직접 줄이는 것(Anschel et al., 2017), 초기 높은 분산 추정치에 대한 과적합 최소화(Fox et al., 2016), 또는 corrective terms를 통한 방법(Lee et al., 2013)에 중점을 두었습니다.

Further, the variance of the value estimate has been considered directly for risk-aversion (Mannor & Tsitsiklis, 2011) and exploration (O’Donoghue et al., 2017), but without connection to overestimation bias.

더 나아가, 가치 추정의 분산은 위험 회피(Mannor & Tsitsiklis, 2011) 및 탐험(O’Donoghue et al., 2017)을 위해 직접 고려되었지만, 과대평가 편향과는 연관이 없었습니다.

However, rather than provide a direct solution to the accumulation of error, these methods circumvent the problem by considering a longer horizon.

그러나 이러한 방법들은 오류의 축적에 대한 직접적인 해결책을 제공하기보다는, 더 긴 시계열을 고려함으로써 문제를 회피합니다.

however, if the target is susceptible to error ε, then the maximum over the value along with its error will generally be greater than the true maximum.

그러나, 만약 목표가 오류 ε에 취약하다면, 그 오류를 포함한 값의 최대치는 일반적으로 실제 최대치보다 클 것입니다.

As a result, even initially zero-mean errors can cause value updates to result in a consistent overestimation bias, which is then propagated through the Bellman equation.

결과적으로, 초기에는 평균이 제로인 오류조차도 값 업데이트로 인해 일관된 과대평가 편향을 유발할 수 있으며, 이 편향은 그 후 벨만 방정식을 통해 전파됩니다.

Without normalized gradients, overestimation bias is still guaranteed to occur with slightly stricter conditions.

정규화된 그래디언트 없이도, 약간 더 엄격한 조건으로 과대평가 편향이 여전히 발생하는 것은 보장됩니다.

If in expectation the value estimate is at least as large as the true value with respect to $\phi_{\text{true}}, \mathbb{E}\left[ Q_{\theta}(s, \pi_{\text{true}}(s)) \right] \geq \mathbb{E} \left[ Q^{\pi}(s, \pi_{\text{true}}(s))\right]$, then Equations (5) and (6) imply that if $α < \min(ϵ_1, ϵ_2)$, then the value estimate will be overestimated:

$$\mathbb{E}\left[ Q_{\theta}(s, \pi_{\text{approx}}(s)) \right] \geq \mathbb{E} \left[ Q^{\pi}(s, \pi_{\text{approx}}(s))\right] \tag{7}$$

기대치에서 가치 추정치가 $\phi_{\text{true}}, \mathbb{E}\left[ Q_{\theta}(s, \pi_{\text{true}}(s)) \right] \geq \mathbb{E} \left[ Q^{\pi}(s, \pi_{\text{true}}(s))\right]$에 대한 실제 가치보다 적어도 그만큼 크다면, 방정식 (5)와 (6)은 $α < \min(ϵ_1, ϵ_2)$일 경우 가치 추정치가 과대평가될 것임을 시사합니다.

In Double Q-learning, the greedy update is disentangled from the value function by maintaining two separate value estimates, each of which is used to update the other.

Double Q-learning에서, Greedy 업데이트는 가치 함수로부터 분리되며, 이는 두 개의 별도 가치 추정치를 유지함으로써 이루어집니다. 각 추정치는 서로를 업데이트하는 데 사용됩니다.

In an actor-critic setting, an analogous update uses the current policy rather than the target policy in the learning target.

Actor-critic 설정에서, 유사한 업데이트는 학습 대상에서 target policy 대신 current policy를 사용합니다.

To address this problem, we propose to simply upper-bound the less biased value estimate $Q\_{\theta_{2}}$ by the biased estimate $Q\_{\theta_{1}}$.

이 문제를 해결하기 위해, 우리는 편향된 추정치 $Q\_{\theta_{1}}$에 의해 덜 편향된 가치 추정치 $Q\_{\theta_{2}}$를 단순히 상한으로 설정하는 것을 제안합니다.

This results in taking the minimum between the two estimates, to give the target update of our Clipped Double Q-learning algorithm:

$$y\_1 = r + \gamma \underset{i=1, 2}{\min}Q\_{\theta'\_{i}}(s', \pi\_{\phi\_{1}}(s')). \tag{10}$$

이는 두 추정치 사이의 최소값을 취함으로써 우리의 Clipped Double Q-learning 알고리즘의 타겟 업데이트를 결정하는 결과를 가져옵니다.

$$y\_1 = r + \gamma \underset{i=1, 2}{\min}Q\_{\theta'\_{i}}(s', \pi\_{\phi\_{1}}(s')). \tag{10}$$

With Clipped Double Q-learning, the value target cannot introduce any additional overestimation over using the standard Q-learning target.

Clipped Double Q-learning에서는 값 타겟이 표준 Q-learning 타겟을 사용할 때보다 추가적인 과대평가를 도입할 수 없습니다.

While this update rule may induce an underestimation bias, this is far preferable to overestimation bias, as unlike overestimated actions, the value of underestimated actions will not be explicitly propagated through the policy update.

이 업데이트 규칙은 과소평가 편향을 유발할 수 있지만, 이는 과대평가 편향보다 훨씬 바람직합니다. 과대평가된 행동과 달리, 과소평가된 행동의 가치는 정책 업데이트를 통해 명시적으로 전파되지 않기 때문입니다.

A secondary benefit is that by treating the function approximation error as a random variable we can see that the minimum operator should provide higher value to states with lower variance estimation error, as the expected minimum of a set of random variables decreases as the variance of the random variables increases.

부수적인 이점으로, 함수 근사 오류를 랜덤 변수로 취급함으로써, 최소 연산자가 분산 추정 오류가 낮은 상태에 더 높은 가치를 제공해야 함을 알 수 있습니다. 이는 랜덤 변수 집합의 예상 최소값이 랜덤 변수의 분산이 증가함에 따라 감소하기 때문입니다.

This is exacerbated in a function approximation setting where the Bellman equation is never exactly satisfied, and each update leaves some amount of residual TD-error $δ(s, a)$:

$$Q_{\theta}(s, a)=r + \gamma\mathbb{E}\left[Q_{\theta}(s', a')\right]-\delta(s, a). \tag{11}$$

이 문제는 함수 근사 설정에서 더욱 심화되는데, 여기서는 벨만 방정식이 정확히 만족되지 않으며, 각 업데이트가 일정량의 잔여 TD-error $δ(s, a)$를 남깁니다.

$$Q_{\theta}(s, a)=r + \gamma\mathbb{E}\left[Q_{\theta}(s', a')\right]-\delta(s, a). \tag{11}$$

Given a large discount factor γ, the variance can grow rapidly with each update if the error from each update is not tamed.

큰 할인 요인 γ가 주어진 경우, 각 업데이트에서 오류를 제어하지 않으면 분산이 각 업데이트와 함께 빠르게 증가할 수 있습니다.

Furthermore, each gradient update only reduces error with respect to a small mini-batch, which gives no guarantees about the size of errors in value estimates outside the mini-batch.

또한, 각 그래디언트 업데이트는 오직 작은 미니배치에 대한 오류만을 줄이므로, 미니배치 외부의 가치 추정치에서 오류의 크기에 대한 보장은 없습니다.

This insight allows us to consider the interplay between high variance estimates and policy performance, when designing reinforcement learning algorithms.

이 통찰은 우리가 강화 학습 알고리즘을 설계할 때, 높은 분산 추정치와 정책 성능 사이의 상호 작용을 고려할 수 있게 해줍니다.

While the accumulation of error can be detrimental in itself, when paired with a policy maximizing over the value estimate, it can result in wildly divergent values.

오류의 축적 자체가 해로울 수 있지만, 가치 추정치를 최대화하는 정책과 결합될 때, 매우 다양한 값으로 이어질 수 있습니다.

To provide some intuition, we examine the learning behavior with and without target networks on both the critic and actor in Figure 3, where we graph the value, in a similar manner to Figure 1, in the Hopper-v1 environment.

직관을 제공하기 위해, 우리는 Figure 3에서 비평가와 액터 모두에 대한 타겟 네트워크의 유무에 따른 학습 행동을 검토합니다. 여기서는 Hopper-v1 환경에서 Figure 1과 유사한 방식으로 값을 그래프로 나타냅니다.

In (a) we compare the behavior with a fixed policy and in (b) we examine the value estimates with a policy that continues to learn, trained with the current value estimate.

(a)에서는 고정된 정책과의 행동을 비교하고, (b)에서는 계속 학습하는 정책과 함께 현재 가치 추정치로 훈련된 가치 추정치를 검토합니다.

While updating the value estimate without target networks ($τ = 1$) increases the volatility, all update rates result in similar convergent behaviors when considering a fixed policy.

타겟 네트워크 없이 가치 추정치를 업데이트하는 것($τ = 1$)이 변동성을 증가시키지만, 고정된 정책을 고려할 때 모든 업데이트 비율은 유사한 수렴 행동을 보입니다.

These results suggest that the divergence that occurs without target networks is the result of policy updates with a high variance value estimate.

이 결과들은 타겟 네트워크가 없을 때 발생하는 발산이 높은 분산 가치 추정치를 가진 정책 업데이트의 결과임을 시사합니다.

If target networks can be used to reduce the error over multiple updates, and policy updates on high-error states cause divergent behavior, then the policy network should be updated at a lower frequency than the value network, to first minimize error before introducing a policy update.

타겟 네트워크가 여러 업데이트에 걸쳐 오류를 줄이는 데 사용될 수 있고, 높은 오류 상태에서의 정책 업데이트가 발산 행동을 유발한다면, 정책 업데이트를 도입하기 전에 먼저 오류를 최소화하기 위해 정책 네트워크는 가치 네트워크보다 낮은 빈도로 업데이트되어야 합니다.

The modification is to only update the policy and target networks after a fixed number of updates $d$ to the critic.

수정 사항은 critic에 대한 고정된 횟수 $d$ 의 업데이트가 이루어진 후에만 정책 및 타겟 네트워크를 업데이트하는 것입니다.

By sufficiently delaying the policy updates, we limit the likelihood of repeating updates with respect to an unchanged critic.

정책 업데이트를 충분히 지연함으로써, 변하지 않은 critic에 대한 반복 업데이트의 가능성을 제한합니다.

The effectiveness of this strategy is captured by our empirical results presented in Section 6.1, which show an improvement in performance while using fewer policy updates.

이 전략의 효과성은 6.1절에 제시된 우리의 경험적 결과로 포착되며, 이는 적은 수의 정책 업데이트를 사용하면서도 성능이 향상됨을 보여줍니다.

Our approach enforces the notion that similar actions should have similar value.

우리의 접근 방식은 유사한 행동들은 유사한 가치를 가져야 한다는 개념을 강조합니다.

While the function approximation does this implicitly, the relationship between similar actions can be forced explicitly by modifying the training procedure.

함수 근사는 이를 암시적으로 수행하지만, 유사한 행동들 사이의 관계는 훈련 절차를 수정함으로써 명시적으로 강제될 수 있습니다.

We propose that fitting the value of a small area around the target action

$$y = r + \mathbb{E}\_{\epsilon} \left[ Q\_{\theta'}(s', \pi_{\phi'}(s') + \epsilon)\right], \tag{13}$$

would have the benefit of smoothing the value estimate by bootstrapping off of similar state-action value estimates.

우리는 target action

$$y = r + \mathbb{E}\_{\epsilon} \left[ Q\_{\theta'}(s', \pi_{\phi'}(s') + \epsilon)\right], \tag{13}$$

주변의 작은 영역에 대한 가치를 적합시키는 것이 비슷한 상태-행동 가치 추정치에서 부트스트래핑을 통해 가치 추정을 smoothing하는 이점이 있을 것이라고 제안합니다.

In practice, we can approximate this expectation over actions by adding a small amount of random noise to the target policy and averaging over mini-batches.

실제로, 우리는 목표 정책에 작은 양의 랜덤 노이즈를 추가하고 미니배치에 대해 평균을 내어 이러한 행동에 대한 기대를 근사할 수 있습니다.

Intuitively, it is known that policies derived from SARSA value estimates tend to be safer, as they provide higher value to actions resistant to perturbations.

직관적으로, SARSA 가치 추정치에서 유도된 정책은 변동에 저항하는 행동에 더 높은 가치를 제공하기 때문에 더 안전하다고 알려져 있습니다.

Thus, this style of update can additionally lead to improvement in stochastic domains with failure cases.

따라서, 이러한 업데이트 방식은 실패 사례가 있는 확률적 영역에서의 개선으로 이어질 수 있습니다.

A similar idea was introduced concurrently by Nachum et al. (2018), smoothing over $Q_θ$, rather than $Q_{\theta'}$.

비슷한 아이디어가 Nachum et al. (2018)에 의해 동시에 소개되었는데, 이는 $Q_{\theta'}$가 아닌 $Q_{\theta}$에 대해 smoothing하는 것입니다.

Every $d$ iterations, the policy is updated with respect to $Q_{\theta_{1}}$ following the deterministic policy gradient algorithm.

매 $d$번의 반복마다, 정책은 결정론적 정책 그래디언트 알고리즘을 따라 $Q_{\theta_{1}}$에 대해 업데이트됩니다.

