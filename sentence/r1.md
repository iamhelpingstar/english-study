These equations result from a telescoping sum, and we see that $\hat{A}_t(k)$ involves a k-step estimate of the returns, minus a baseline term $V(s_t)$.

이 방정식들은 망원합으로부터 나오며, 우리는 $\hat{A}_t(k)$가 k-단계 return 추정치에서 baseline term - $V(s_t)$를 뺀 것을 포함한다는 것을 알 수 있습니다.

So, in 10-dim to capture 0.1% of the data we need 50% of the range.

그러므로, 10차원에서 0.1%의 데이터를 포착하기 위해 우리는 범위의 50%가 필요합니다.

What fraction of “space” do we need to cover to get 0.1% of data (10 nearest neighbors)

0.1%의 데이터(가장 가까운 이웃 10개)를 얻기 위해 필요한 "공간"의 비율은 얼마인가요?

Similar CDs have similar sets of customers, and vice-versa.

비슷한 CD는 비슷한 고객 세트를 가지며, 그 반대도 마찬가지입니다.

Various cohesion measures of the union of two clusters.

두 클러스터의 합집합에 대한 다양한 결합도 측정 방법입니다.

Smallest maximum distance to other points.

다른 점들까지의 가장 작은 최대 거리입니다.

Define a notion of cohesion, and merge clusters whose union is most cohesive.

응집력의 개념을 정의하고, 응집력이 가장 높은 합집합을 이루는 클러스터들을 병합합니다.

Stop if the density of the cluster that results from the best merger is below some threshold.

최상의 병합으로 인해 생성된 클러스터의 밀도가 일정 임계값 아래로 떨어지면 중지합니다.

The density can be defined in many different ways. Roughly, it should be the number of cluster points per unit volume of the cluster.

밀도는 여러 가지 방식으로 정의될 수 있습니다. 대략적으로, 클러스터의 단위 부피당 클러스터 점의 수로 정의되어야 합니다.

That ratio can be estimated by the number of points divided by some power of the diameter or radius of the cluster.

그 비율은 클러스터의 직경이나 반경의 어떤 거듭제곱으로 나눈 점의 수로 추정할 수 있습니다.

The correct power could be the number of dimensions of the space.

올바른 거듭제곱은 공간의 차원 수일 수 있습니다.

Sometimes, 1 or 2 is chosen as the power, regardless of the number of dimensions.

때때로, 차원 수에 관계없이 1 또는 2가 거듭제곱으로 선택됩니다.

Density of the merged cluster = divide by the number of points in the cluster by diameter or avg. distance.

병합된 클러스터의 밀도 = 클러스터 내 점의 수를 클러스터의 지름이나 평균 거리로 나눕니다.

Other suggested criteria involved measuring the density of a cluster, based on the radius or diameter.

다른 제안된 기준에는 반지름이나 지름을 기반으로 클러스터의 밀도를 측정하는 것이 포함됩니다.

Both these notions make sense in the non-Euclidean environment. The diameter is still the maximum distance between any two points in the cluster.

이 두 가지 개념은 비유클리드 환경에서도 의미가 있습니다. 지름은 여전히 클러스터 내 임의의 두 점 사이의 최대 거리입니다.

The radius can be defined using the clustroid in place of the centroid.

반지름은 중심점 대신 클러스토이드를 사용하여 정의할 수 있습니다.

Moreover, it makes sense to use the same sort of evaluation for the radius as we used to select the clustroid in the first place.

또한, 클러스토이드를 처음 선택할 때 사용한 것과 같은 종류의 평가를 반지름에 사용하는 것이 합리적입니다.

For example, if we take the clustroid to be the point with the smallest sum of squares of distances to the other nodes, then define the radius to be that sum of squares (or its square root).

예를 들어, 클러스토이드를 다른 노드들에 대한 거리의 제곱합이 가장 작은 점으로 취한다면, 반지름을 그 제곱합(또는 그 제곱근)으로 정의합니다.

tree (Any) – a pytree to be mapped over, with each leaf providing the first positional argument to `f`.

tree (Any) - 각 leaf가 `f`에게 첫 번째 positional argument를 제공하는, 매핑될 pytree입니다.

rest (Any) – a tuple of pytrees, each of which has the same structure as `tree` or has `tree` as a prefix.

rest (Any) - 각각이 `tree`와 같은 구조를 가지거나 `tree`를 접두사로 가지는 pytree들의 튜플입니다.

is_leaf (Callable[[Any], bool] | None) – an optionally specified function that will be called at each flattening step. It should return a boolean, which indicates whether the flattening should traverse the current object, or if it should be stopped immediately, with the whole subtree being treated as a leaf.

is_leaf (Callable[[Any], bool] | None) – 선택적으로 지정된 함수로, 각 평탄화 단계에서 호출됩니다. 이 함수는 boolean을 반환해야 하며, 이는 평탄화가 현재 객체를 계속 탐색해야 하는지, 또는 즉시 중단해야 하는지를 나타냅니다. 즉시 중단하는 경우, 전체 하위 트리가 leaf로 취급됩니다.

A new pytree with the same structure as `tree` but with the value at each leaf given by `f(x, *xs)` where `x` is the value at the corresponding leaf in `tree` and `xs` is the tuple of values at corresponding nodes in `rest`.

`tree`와 동일한 구조를 가진 새로운 pytree이지만, 각 leaf의 값은 `f(x, *xs)`로 주어집니다. 여기서 `x`는 `tree`의 해당 leaf에 있는 값이고 `xs`는 `rest`의 해당 노드들에 있는 값들의 튜플입니다.

If multiple inputs are passed, the structure of the tree is taken from the first input; subsequent inputs need only have `tree` as a prefix:

여러 입력이 전달될 경우, tree의 구조는 첫 번째 입력에서 가져옵니다; 이후의 입력들은 `tree`를 접두사로만 가지면 됩니다:

If the positional arguments to `fun` are container (pytree) types, the corresponding element of `in_axes` can itself be a matching container, so that distinct array axes can be mapped for different container elements.

`fun`에 대한 positional arguments가 컨테이너(pytree) 타입인 경우, `in_axes`의 해당 요소 자체도 일치하는 컨테이너가 될 수 있어, 서로 다른 컨테이너 요소에 대해 다른 배열 축이 매핑될 수 있습니다.

Axis integers must be in the range `[-ndim, ndim)` for each output array, where `ndim` is the number of dimensions (axes) of the array returned by the `vmap()`-ed function, which is one more than the number of dimensions (axes) of the corresponding array returned by `fun`.

출력 배열에 대한 축 정수는 각각의 출력 배열에 대해 `[-ndim, ndim)` 범위 내에 있어야 하며, 여기서 `ndim`은 `vmap()`에 의해 변환된 함수가 반환하는 배열의 차원(축) 수로, 이는 `fun`에 의해 반환된 해당 배열의 차원(축) 수보다 하나 더 많습니다.

Batched/vectorized version of `fun` with arguments that correspond to those of `fun`, but with extra array axes at positions indicated by `in_axes`, and a return value that corresponds to that of `fun`, but with extra array axes at positions indicated by `out_axes`.

`in_axes`에 의해 지정된 위치에 추가적인 배열 축을 가진 `fun`의 인자에 해당하는 인자를 가지고 있으며, `out_axes`에 의해 지정된 위치에 추가적인 배열 축을 가진 `fun`의 반환 값에 해당하는 반환 값을 가진 `fun`의 배치된/벡터화된 버전입니다.

The vdot(*a*, *b*) function handles complex numbers differently than dot(*a*, *b*). If the first argument is complex the complex conjugate of the first argument is used for the calculation of the dot product.

vdot(*a*, *b*) 함수는 dot(*a*, *b*)와 다르게 복소수를 다룬다. 첫 번째 인자가 복소수인 경우, 점곱셈 계산을 위해 첫 번째 인자의 복소수 켤레가 사용된다.

Typical implementations create a new instance of the class by invoking the superclass’s `__new__()` method using `super().__new__(cls[, ...])` with appropriate arguments and then modifying the newly created instance as necessary before returning it.

일반적인 구현에서는 적절한 인자를 사용하여 상위 클래스의 `__new__()` 메서드를 `super().__new__(cls[, ...])`를 호출하여 클래스의 새로운 인스턴스를 생성하고, 필요에 따라 새로 생성된 인스턴스를 수정한 후에 반환합니다.

If `__new__()` is invoked during object construction and it returns an instance of cls, then the new instance’s `__init__()` method will be invoked like `__init__(self[, ...])`, where self is the new instance and the remaining arguments are the same as were passed to the object constructor.

만약 객체 생성 중 `__new__()`가 호출되어 cls의 인스턴스를 반환하면, 새 인스턴스의 `__init__()` 메서드가 `__init__(self[, ...])`와 같이 호출됩니다. 여기서 self는 새 인스턴스를 나타내고 나머지 인자들은 객체 생성자에 전달된 것과 동일합니다.

`__new__()` is intended mainly to allow subclasses of immutable types (like int, str, or tuple) to customize instance creation. It is also commonly overridden in custom metaclasses in order to customize class creation.

`__new__()`는 주로 불변 타입(예: int, str, tuple 등)의 서브클래스가 인스턴스 생성을 사용자 정의할 수 있도록 하기 위한 목적으로 사용됩니다. 또한, 클래스 생성을 사용자 정의하기 위해 사용자 정의 메타클래스에서도 흔히 오버라이드됩니다.

We also introduce the notion of a response function to help understand the bias introduced by $γ$ and $λ$.

우리는 또한 $γ$와 $λ$에 의해 도입된 편향을 이해하는 데 도움이 되는 응답 함수 개념을 소개합니다.

The response function lets us quantify the temporal credit assignment problem: long range dependencies between actions and rewards correspond to nonzero values of the response function for $l \gg 0$.

Response function은 우리가 시간적 credit assignment 문제를 정량화할 수 있게 해줍니다: 행동과 보상 간의 장기 의존성은 $l ≫ 0$ 일 때 Response function의 nonzero 값에 해당합니다.

Thus, the error introduced by this approximation will be small if $\mathcal{X}$ rapidly decays as $l$ increases, i.e., if the effect of an action on rewards is “forgotten” after $≈ 1/(1 − γ)$ timesteps.

따라서, 이 근사에 의해 도입된 오류는 $l$이 증가함에 따라 $\mathcal{X}$가 빠르게 감소한다면, 즉, 행동의 영향이 대략 $1/(1 - γ)$ 타임스텝 후에 "잊혀진다면" 작을 것입니다.

This constraint is equivalent to constraining the average KL divergence between the previous value function and the new value function to be smaller than ϵ, where the value function is taken to parameterize a conditional Gaussian distribution with mean $V_\phi(s)$ and variance $σ^2$ .

이 제약은 이전 가치 함수와 새 가치 함수 사이의 평균 KL divergence가 ε보다 작게 유지되는 것과 동등합니다. 여기서 가치 함수는 평균이 $V_\phi(s)$이고 분산이 $σ^2$인 조건부 가우시안 분포를 매개변수화하는 것으로 간주됩니다.

Note that $H$ is the “Gauss-Newton” approximation of the Hessian of the objective, and it is (up to a $σ^2$ factor) the Fisher information matrix when interpreting the value function as a conditional probability distribution.

$H$는 목적의 헤시안 근사치에 대한 "가우스-뉴턴" 근사치이며, 값 함수를 조건부 확률 분포로 해석할 때 ($σ^2$ 계수까지) 피셔 정보 행렬이라는 점에 유의하세요.

While we experimented with this choice, we did not notice a difference in performance from

the λ = 1 estimator in Equation (28).

우리는 이 선택을 실험해 보았지만, 식 (28)의 λ = 1 추정기와 성능에서 차이를 느끼지 못했습니다.

To see this, consider the extreme case where we overfit the value function, and the Bellman residual $r_t + γV (s_{t+1}) − V (s_t)$ becomes zero at all timesteps—the policy gradient estimate would be zero.

이를 이해하려면, Value function을 과적합하여 Bellman residual $r_t + γV (s_{t+1}) − V (s_t)$가 모든 타임스텝에서 0이 되는 극단적인 경우를 고려해보세요—Policy gradient 추정치는 0이 될 것입니다.

dynamically standing up, for the biped, which starts off laying on its back.

등을 대고 누워 시작하는, 이족보행을 위해, 동적으로 일어서는 동작입니다.

The humanoid model has 33 state dimensions and 10 actuated degrees of freedom, while the

quadruped model has 29 state dimensions and 8 actuated degrees of freedom.

휴머노이드 모델은 33개의 상태 차원과 10개의 구동 자유도를 가지고 있으며, 사족보행 로봇 모델은 29개의 상태 차원과 8개의 구동 자유도를 가집니다.

Right: performance after 20 iterations of policy optimization, as γ and λ are varied. White means higher reward. The best results are obtained at intermediate values of both.

오른쪽: γ와 λ가 변할 때 Policy optimization의 20번 반복 후의 성능입니다. 흰색은 더 높은 보상을 의미합니다. 두 값의 중간 값에서 최상의 결과를 얻습니다.

if a sub-environment is not terminated nor truncated, PPO estimates the value of the next state in this sub-environment as the value target.

sub-environment가 terminated되거나 truncated되지 않은 경우, PPO는 이 sub-environment에서 다음 상태의 가치를 value target으로 추정합니다.

