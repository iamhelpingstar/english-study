`__new__()` is intended mainly to allow subclasses of immutable types (like int, str, or tuple) to customize instance creation. It is also commonly overridden in custom metaclasses in order to customize class creation.

`__new__()`는 주로 불변 타입(예: int, str, tuple 등)의 서브클래스가 인스턴스 생성을 사용자 정의할 수 있도록 하기 위한 목적으로 사용됩니다. 또한, 클래스 생성을 사용자 정의하기 위해 사용자 정의 메타클래스에서도 흔히 오버라이드됩니다.

We also introduce the notion of a response function to help understand the bias introduced by $γ$ and $λ$.

우리는 또한 $γ$와 $λ$에 의해 도입된 편향을 이해하는 데 도움이 되는 응답 함수 개념을 소개합니다.

The response function lets us quantify the temporal credit assignment problem: long range dependencies between actions and rewards correspond to nonzero values of the response function for $l \gg 0$.

Response function은 우리가 시간적 credit assignment 문제를 정량화할 수 있게 해줍니다: 행동과 보상 간의 장기 의존성은 $l ≫ 0$ 일 때 Response function의 nonzero 값에 해당합니다.

Thus, the error introduced by this approximation will be small if $\mathcal{X}$ rapidly decays as $l$ increases, i.e., if the effect of an action on rewards is “forgotten” after $≈ 1/(1 − γ)$ timesteps.

따라서, 이 근사에 의해 도입된 오류는 $l$이 증가함에 따라 $\mathcal{X}$가 빠르게 감소한다면, 즉, 행동의 영향이 대략 $1/(1 - γ)$ 타임스텝 후에 "잊혀진다면" 작을 것입니다.

This constraint is equivalent to constraining the average KL divergence between the previous value function and the new value function to be smaller than ϵ, where the value function is taken to parameterize a conditional Gaussian distribution with mean $V_\phi(s)$ and variance $σ^2$ .

이 제약은 이전 가치 함수와 새 가치 함수 사이의 평균 KL divergence가 ε보다 작게 유지되는 것과 동등합니다. 여기서 가치 함수는 평균이 $V_\phi(s)$이고 분산이 $σ^2$인 조건부 가우시안 분포를 매개변수화하는 것으로 간주됩니다.

Note that $H$ is the “Gauss-Newton” approximation of the Hessian of the objective, and it is (up to a $σ^2$ factor) the Fisher information matrix when interpreting the value function as a conditional probability distribution.

$H$는 목적의 헤시안 근사치에 대한 "가우스-뉴턴" 근사치이며, 값 함수를 조건부 확률 분포로 해석할 때 ($σ^2$ 계수까지) 피셔 정보 행렬이라는 점에 유의하세요.

While we experimented with this choice, we did not notice a difference in performance from the λ = 1 estimator in Equation (28).

우리는 이 선택을 실험해 보았지만, 식 (28)의 λ = 1 추정기와 성능에서 차이를 느끼지 못했습니다.

To see this, consider the extreme case where we overfit the value function, and the Bellman residual $r_t + γV (s_{t+1}) − V (s_t)$ becomes zero at all timesteps—the policy gradient estimate would be zero.

이를 이해하려면, Value function을 과적합하여 Bellman residual $r_t + γV (s_{t+1}) − V (s_t)$가 모든 타임스텝에서 0이 되는 극단적인 경우를 고려해보세요—Policy gradient 추정치는 0이 될 것입니다.

dynamically standing up, for the biped, which starts off laying on its back.

등을 대고 누워 시작하는, 이족보행을 위해, 동적으로 일어서는 동작입니다.

The humanoid model has 33 state dimensions and 10 actuated degrees of freedom, while the

quadruped model has 29 state dimensions and 8 actuated degrees of freedom.

휴머노이드 모델은 33개의 상태 차원과 10개의 구동 자유도를 가지고 있으며, 사족보행 로봇 모델은 29개의 상태 차원과 8개의 구동 자유도를 가집니다.

Right: performance after 20 iterations of policy optimization, as γ and λ are varied. White means higher reward. The best results are obtained at intermediate values of both.

오른쪽: γ와 λ가 변할 때 Policy optimization의 20번 반복 후의 성능입니다. 흰색은 더 높은 보상을 의미합니다. 두 값의 중간 값에서 최상의 결과를 얻습니다.

if a sub-environment is not terminated nor truncated, PPO estimates the value of the next state in this sub-environment as the value target.

sub-environment가 terminated되거나 truncated되지 않은 경우, PPO는 이 sub-environment에서 다음 상태의 가치를 value target으로 추정합니다.

A class method can be called either on the class (such as `C.f()`) or on an instance (such as `C().f()`). The instance is ignored except for its class. If a class method is called for a derived class, the derived class object is passed as the implied first argument.

클래스 메서드는 클래스에 대해 (예: `C.f()`) 또는 인스턴스에 대해 (예: `C().f()`) 호출될 수 있습니다. 인스턴스는 그 클래스를 제외하고는 무시됩니다. 파생된 클래스에 대한 클래스 메서드가 호출되면, 파생된 클래스 객체가 암시적인 첫 번째 인자로 전달됩니다.

Various cohesion measures of the union of two clusters.

두 클러스터의 합집합에 대한 다양한 응집력 측정 방법입니다.

Each trial took about 2 hours to run on a 16-core machine, where the simulation rollouts were parallelized, as were the function, gradient, and matrix-vector-product evaluations used when optimizing the policy and value function.

각 시험은 16코어 기계에서 약 2시간이 걸렸으며, 여기서 시뮬레이션 롤아웃은 병렬화되었고, 정책과 가치 함수를 최적화할 때 사용되는 함수, 그래디언트, 행렬-벡터 곱셈 평가도 병렬화되었습니다.

Hence, it is plausible that this algorithm could be run on a real robot, or multiple real robots learning in parallel, if there were a way to reset the state of the robot and ensure that it doesn’t damage itself.

따라서, 로봇의 상태를 재설정하고 로봇이 스스로를 손상시키지 않도록 보장할 수 있는 방법이 있다면, 이 알고리즘이 실제 로봇이나 병렬로 학습하는 여러 실제 로봇에서 실행될 수 있을 것으로 보입니다.

For 3D standing, the value function always helped, but the results are roughly the same for λ = 0.96 and λ = 1.

3D 서기의 경우, Value function은 항상 도움이 되었지만, λ = 0.96과 λ = 1의 결과는 대략 비슷합니다.

Policy gradient methods provide a way to reduce reinforcement learning to stochastic gradient descent, by providing unbiased gradient estimates.

Policy gradient 방법은 편향되지 않은 그래디언트 추정치를 제공함으로써 강화 학습을 확률적 그래디언트 하강으로 줄이는 방법을 제공합니다.

One question that merits future investigation is the relationship between value function estimation error and policy gradient estimation error.

미래의 조사가 필요한 한 가지 질문은 Value function 추정 오류와 Policy gradient 추정 오류 사이의 관계입니다.

If this relationship were known, we could choose an error metric for value function fitting that is well-matched to the quantity of interest, which is typically the accuracy of the policy gradient estimation.

이 관계를 알고 있다면, 일반적으로 Policy gradient 추정의 정확도인 관심 있는(interest) 양에 잘 맞는 Value function 적합을 위한 오류 척도를 선택할 수 있을 것입니다.

While formulating this problem in a way that is suitable for numerical optimization and provides convergence guarantees remains an open question, such an approach could allow the value function and policy representations to share useful features of the input, resulting in even faster learning.

이 문제를 수치 최적화에 적합하고 수렴 보장을 제공하는 방식으로 구성하는 것은 여전히 열린 질문이지만, 이러한 접근은 Value function과 Policy 표현이 입력의 유용한 특징을 공유하게 하여, 더욱 빠른 학습을 가능하게 할 수 있습니다.

However, note that those papers consider control problems with substantially lower-dimensional state and action spaces than the ones considered here.

하지만, 그 논문들은 여기서 고려된 것보다 훨씬 낮은 차원의 상태와 행동 공간을 가진 제어 문제를 고려한다는 점에 유의하세요.

WHAT’S THE RELATIONSHIP WITH COMPATIBLE FEATURES?

COMPATIBLE FEATURES와의 관계는 무엇인가요?

Compatible features are often mentioned in relation to policy gradient algorithms that make use of a value function, and the idea was proposed in the paper On Actor-Critic Methods by Konda & Tsitsiklis (2003).

Compatible features는 종종 Value function을 사용하는 Policy gradient 알고리즘과 관련하여 언급되며, 이 아이디어는 Konda & Tsitsiklis (2003)의 논문 On Actor-Critic Methods에서 제안되었습니다.

These authors pointed out that due to the limited representation power of the policy, the policy gradient only depends on a certain subspace of the space of advantage functions.

이 저자들은 정책의 제한된 표현력 때문에, Policy gradient는 이점 함수 공간의 특정 부분 공간에만 의존한다고 지적했습니다.

This subspace is spanned by the compatible features.

이 부분 공간은 Compatible features에 의해 생성됩니다.

This theory of compatible features provides no guidance on how to exploit the temporal structure of the problem to obtain better estimates of the advantage function, making it mostly orthogonal to the ideas in this paper.

Compatible features 이론은 문제의 Temporal 구조를 활용하여 Advantage function의 더 나은 추정치를 얻는 방법에 대한 지침을 제공하지 않아, 이 논문의 아이디어와 대부분 orthogonal합니다.

