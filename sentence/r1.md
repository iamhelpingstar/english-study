We propose that fitting the value of a small area around the target action

$$y = r + \mathbb{E}\_{\epsilon} \left[ Q\_{\theta'}(s', \pi_{\phi'}(s') + \epsilon)\right], \tag{13}$$

would have the benefit of smoothing the value estimate by bootstrapping off of similar state-action value estimates.

우리는 target action

$$y = r + \mathbb{E}\_{\epsilon} \left[ Q\_{\theta'}(s', \pi_{\phi'}(s') + \epsilon)\right], \tag{13}$$

주변의 작은 영역에 대한 가치를 적합시키는 것이 비슷한 상태-행동 가치 추정치에서 부트스트래핑을 통해 가치 추정을 smoothing하는 이점이 있을 것이라고 제안합니다.

In practice, we can approximate this expectation over actions by adding a small amount of random noise to the target policy and averaging over mini-batches.

실제로, 우리는 목표 정책에 작은 양의 랜덤 노이즈를 추가하고 미니배치에 대해 평균을 내어 이러한 행동에 대한 기대를 근사할 수 있습니다.

Intuitively, it is known that policies derived from SARSA value estimates tend to be safer, as they provide higher value to actions resistant to perturbations.

직관적으로, SARSA 가치 추정치에서 유도된 정책은 변동에 저항하는 행동에 더 높은 가치를 제공하기 때문에 더 안전하다고 알려져 있습니다.

Thus, this style of update can additionally lead to improvement in stochastic domains with failure cases.

따라서, 이러한 업데이트 방식은 실패 사례가 있는 확률적 영역에서의 개선으로 이어질 수 있습니다.

A similar idea was introduced concurrently by Nachum et al. (2018), smoothing over $Q_θ$, rather than $Q_{\theta'}$.

비슷한 아이디어가 Nachum et al. (2018)에 의해 동시에 소개되었는데, 이는 $Q_{\theta'}$가 아닌 $Q_{\theta}$에 대해 smoothing하는 것입니다.

Every $d$ iterations, the policy is updated with respect to $Q_{\theta_{1}}$ following the deterministic policy gradient algorithm.

매 $d$번의 반복마다, 정책은 결정론적 정책 그래디언트 알고리즘을 따라 $Q_{\theta_{1}}$에 대해 업데이트됩니다.

The sample update is in addition affected by sampling error. On the other hand, the sample update is cheaper computationally because it considers only one next state, not all possible next states.

샘플 업데이트는 샘플링 오류의 영향을 추가적으로 받습니다. 반면에, 샘플 업데이트는 가능한 모든 다음 상태가 아닌 오직 하나의 다음 상태만을 고려하기 때문에 계산적으로 더 저렴합니다.

To provide some intuition, we examine the learning behavior with and without target networks on both the critic and actor in Figure 3, where we graph the value, in a similar manner to Figure 1, in the Hopper-v1 environment.

직관을 제공하기 위해, 우리는 그림 3에서 Hopper-v1 환경에서 그림 1과 유사한 방식으로 값(value)을 그래프로 나타내며, critic과 actor에서 target 네트워크가 있는 경우와 없는 경우에 대한 학습 행동을 살펴봅니다.

