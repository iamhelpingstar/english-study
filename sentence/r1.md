Cofactor expansion is recursive, but you don't have to use cofactor expansion to compute the determinants of the minors! Or you can do row operations and then a cofactor expansion.

여인수 전개는 재귀적이지만, 소행렬식의 행렬식을 구하기 위해 꼭 여인수 전개를 사용할 필요는 없습니다! 또는 행 연산을 한 다음에 여인수 전개를 할 수도 있습니다.

If all scientific papers whose results are in doubt because of bad `rand()`s were to disappear from library shelves, there would be a gap on each shelf about as big as your fist.

만약 결과가 좋지 않은 `rand()` 때문에 의심받는 모든 과학 논문들이 도서관의 서가에서 사라진다면, 각 서가마다 주먹만한 크기의 공간이 생길 것이다.

You’re used to *stateful* pseudorandom number generators (PRNGs) from numpy and other libraries, which helpfully hide a lot of details under the hood to give you a ready fountain of pseudorandomness:

numpy와 다른 라이브러리들에서 사용하는 상태 유지형 의사난수 생성기(PRNGs)에 익숙하며, 이들은 많은 세부 사항들을 숨겨서 사용자에게 준비된 의사난수의 원천을 제공한다.

Underneath the hood, numpy uses the Mersenne Twister PRNG to power its pseudorandom functions.

numpy는 내부적으로 메르센 트위스터(Mersenne Twister) PRNG를 사용하여 그것의 의사난수 함수들을 구동한다.

The PRNG has a period of 2^19937−1 and at any point can be described by **624 32bit unsigned ints** and a **position** indicating how much of this “entropy” has been used up.

PRNG는 2^19937−1의 주기를 가지고 있으며, 어느 시점에서든 624개의 32비트 unsigned int와, 이 '엔트로피'가 얼마나 사용되었는지를 나타내는 위치로 설명될 수 있다.

The problem with magic PRNG state is that it’s hard to reason about how it’s being used and updated across different threads, processes, and devices, and it’s very easy to screw up when the details of entropy production and consumption are hidden from the end user.

매직 PRNG 상태의 문제점은 다른 스레드, 프로세스, 장치들 간에 어떻게 사용되고 업데이트되는지 이해하기 어렵고, 엔트로피 생성 및 소비의 세부사항이 최종 사용자로부터 숨겨져 있을 때 실수하기 매우 쉽다는 것이다.

That is, its design allows us to **fork** the PRNG state into new PRNGs for use with parallel stochastic generation.

즉, 그것의 설계는 PRNG 상태를 **fork**하여 새로운 PRNG들로 만들고, 이를 병렬 확률적 생성에 사용할 수 있게 해준다.

Reusing the same state will cause **sadness** and **monotony**, depriving the end user of **lifegiving chaos**:

동일한 상태를 재사용하면 슬픔과 단조로움을 유발하여 최종 사용자에게 생동감 넘치는 혼란을 줄 수 있습니다:

In this section we focus on pseudo random number generation (PRNG); that is, the process of algorithmically generating sequences of numbers whose properties approximate the properties of sequences of random numbers sampled from an appropriate distribution.

이 섹션에서는 의사 난수 생성(PRNG)에 초점을 맞춥니다; 즉, 적절한 분포에서 샘플링된 난수 시퀀스의 속성을 근사하는 속성을 가진 숫자 시퀀스를 알고리즘적으로 생성하는 과정입니다.

Generally, JAX strives to be compatible with NumPy, but pseudo random number generation is a notable exception.

일반적으로 JAX는 NumPy와의 호환성을 추구하지만, 의사 난수 생성은 주목할 만한 예외입니다.

To better understand the difference between the approaches taken by JAX and NumPy when it comes to random number generation we will discuss both approaches in this section.

JAX와 NumPy가 난수 생성에 있어서 취하는 접근 방식의 차이를 더 잘 이해하기 위해, 이 섹션에서는 두 접근 방식에 대해 논의할 것입니다.

It doesn’t matter which part of the output of `split(key)` we call `key`, and which we call `subkey`.

`split(key)`의 출력 중 어느 부분을 `key`로 부르고, 어느 부분을 `subkey`로 부르는지는 중요하지 않다.

We want a PRNG design that is **expressive** in that it is convenient to use and it doesn’t constrain the user’s ability to write numerical programs with exactly the behavior that they want,

우리는 사용하기 편리하며 사용자가 원하는 정확한 동작을 가진 수치 프로그램을 작성하는 능력을 제한하지 않는, 표현력이 있는 PRNG 설계를 원한다,

We want a PRNG design that has semantics that are **invariant to `@jit` compilation boundaries and device backends,**

우리는 @jit 컴파일 경계와 디바이스 백엔드에 불변하는 의미론을 가진 PRNG 설계를 원한다,

We want a PRNG design that is **parallelizable** in that it doesn’t add sequencing constraints between random function calls that otherwise would have no data dependence,

우리는 랜덤 함수 호출 간에 데이터 의존성이 없는 시퀀싱 제약을 추가하지 않는다는 점에서 병렬화 가능한 PRNG 설계를 원합니다,

Even if we didn’t require reproducibility and thus allowed any evaluation order, parallelization across calls (#5) would still be made difficult by the need to update shared state.

재현성을 요구하지 않고 어떤 평가 순서도 허용한다 해도, 공유 상태를 업데이트해야 하는 필요성 때문에 호출 간 병렬화(#5)는 여전히 어려울 것이다.

Moreover, because the same PRNG state would need to be accessed and maintained in both Python and any compiled code, this model would likely lead to engineering challenges to achieve compilation invariance (#3) and scaling to multiple replicas (#6).

또한, 동일한 PRNG 상태가 파이썬과 컴파일된 코드 양쪽에서 접근 및 유지되어야 하기 때문에, 이 모델은 컴파일 불변성(#3)을 달성하고 여러 복제본으로 확장하는 데(#6) 엔지니어링 측면의 도전이 될 가능성이 높다.

To allow for vectorization (#4) within primitive PRNG function calls that generate arrays (e.g. to rand() with a shape argument), we drop this sequential-equivalent guarantee.

배열을 생성하는 기본 PRNG 함수 호출 내에서 벡터화(#4)를 허용하기 위해 (예: shape 인자를 가진 rand() 함수에 적용될 때), 우리는 이 순차적 동등성 보장을 포기한다.

This vectorization can be supported by any of the three programming models discussed in this section, though it motivates the implementation in terms of a counter-based PRNG as described in the next section.

이 vectorization은 이 섹션에서 논의된 세 가지 프로그래밍 모델 중 어느 것에 의해서도 지원될 수 있지만, 다음 섹션에서 설명되는 카운터 기반 PRNG의 구현을 동기 부여한다.

This explicit threading can also make the semantics invariant to compilation boundaries (#3).

이 명시적인 스레딩은 semantic을 컴파일 경계에 대해 불변하게 만들 수도 있다(#3).

But worse, it hasn’t actually improved the expressiveness (#1): there is still no way for foo() to call into bar() or baz() while maintaining its own PRNG state.

하지만 더 나쁜 것은, 이것이 실제로 표현력(#1)을 향상시키지 못했다는 점이다: foo()가 자신의 PRNG 상태를 유지하면서 bar()나 baz()로 호출하는 방법이 여전히 없다.

Without knowledge of their callers or the subroutines they call, functions must defensively pass in and return the rng state everywhere.

호출하는 함수나 호출되는 서브루틴에 대한 지식 없이, 함수들은 방어적으로 어디서나 rng 상태를 전달하고 반환해야 한다. .

Moreover, it also doesn’t improve the prospects for parallelization (#5) or scaling to multiple replicas (#6) because everything is still sequential, even if the sequencing is made explicit in the functional programming sense.

또한, 이것은 병렬화(#5)나 여러 복제본으로의 확장(#6)에 대한 전망도 개선하지 않는다. 왜냐하면 모든 것이 여전히 순차적이기 때문이다, 심지어 그 순서가 함수형 프로그래밍의 의미에서 명시적으로 만들어졌다 하더라도

There is no sequential dependence between the calls to bar() and baz(), and they can be evaluated in either order without affecting the value of the result, which solves the remaining performance goals (#5, #6).

bar()와 baz() 호출 사이에 순차적 의존성이 없으며, 결과값에 영향을 주지 않고 어느 순서로든 평가될 수 있어, 남아 있는 성능 목표(#5, #6)를 해결한다.

Functions do not need to return updated versions of PRNGs, and it is straightforward to call a random subroutine without affecting existing PRNG states, improving the expressiveness (#1) from the other functional model.

함수들은 PRNG의 업데이트된 버전을 반환할 필요가 없으며, 기존의 PRNG 상태에 영향을 주지 않고 임의의 서브루틴을 호출하는 것이 간단하다. 이는 다른 함수형 모델에 비해 표현력(#1)을 향상시킨다.

The example doesn’t show it, but as a consequence of choice (2), the only way to advance the PRNG state is to call split().

예제에서는 보여주지 않지만, 선택 (2)의 결과로, PRNG 상태를 진행시키는 유일한 방법은 split()을 호출하는 것이다.

That is, we have two ways to achieve (1), and they differ in whether they burden the user program with explicit calls to split(), as in the above example, or instead burden the user program with explicit threading.

즉, (1)을 달성하는 두 가지 방법이 있는데, 이는 사용자 프로그램에 split()의 명시적인 호출을 부담시키거나, 위의 예와 같이 또는 명시적인 스레딩으로 사용자 프로그램에 부담을 주는지의 차이가 있다.

We prefer the former, i.e., the version with explicit splitting, because we can easily implement the explicit-threading version in terms of it.

우리는 전자, 즉 명시적 분할이 있는 버전을 선호한다. 왜냐하면 이를 통해 명시적 스레딩 버전을 쉽게 구현할 수 있기 때문이다.

The asymmetry in the hash function arguments, of type Key and Count, is that the latter is trivial and computationally cheap to advance by an arbitrary amount, since we just need to increase the integer value, while the former is only advanced by hashing. That’s why we use the count argument for vectorization.

해시 함수 인자들, 즉 Key와 Count의 비대칭성은 후자가 임의의 양만큼 진행시키기가 쉽고 계산 비용이 적다는 것이다. 왜냐하면 우리는 단순히 정수 값을 증가시키면 되는 반면, 전자는 해싱을 통해서만 진행된다. 그래서 벡터화를 위해 count 인자를 사용한다.

Here’s what a training loop on the host might look like when the step requires a PRNG (maybe for dropout or for VAE training):

host에서의 training loop는 PRNG(아마도 드롭아웃이나 VAE 훈련을 위한)가 필요한 단계일 때 다음과 같은 모습일 것입니다:

Even if we did, it would be backend-dependent and we might have to introduce sequential dependencies between random calls to ensure deterministic ordering and hence reproducibility.

우리가 그렇게 한다 해도, 그것은 백엔드에 의존적일 것이며, 우리는 결정적(deterministic) 순서를 보장하고 따라서 재현성을 확보하기 위해 무작위 호출 사이에 순차적 의존성을 도입해야 할 수도 있습니다.

We could consider providing an additional API that allows access to a hardware PRNG for users who want to give up other desiderata (like strict reproducibility).

우리는 엄격한 재현성과 같은 다른 바람직한 요소들을 포기하고자 하는 사용자들을 위해 하드웨어 PRNG에 접근할 수 있는 추가 API를 제공하는 것을 고려할 수 있습니다.

We don’t know of any users or examples for which this property is important.

이 속성이 중요한 사용자나 예시를 우리는 알지 못합니다.

But the line through v1 is contained in the λ1-eigenspace, and v2 does not have eigenvalue v1.

v1을 통과하는 선은 λ1-고유공간에 포함되어 있고, v2는 고유값 v1을 갖지 않습니다.

The combination of RL and high-capacity function approximators such as neural networks holds the promise of automating a wide range of decision making and control tasks, but widespread adoption of these methods in real-world domains has been hampered by two major challenges.

RL과 신경망과 같은 고용량 function approximators의 결합은 다양한 의사결정 및 control 작업을 자동화하는 데 있어 큰 가능성을 가지고 있지만, 이러한 방법들이 실제 세계 영역에서 널리 채택되는 것은 두 가지 주요한 challenges에 의해 방해받고 있습니다.

First, model-free deep RL methods are notoriously expensive in terms of their sample complexity.

첫째, model-free deep RL methods는 그들의 sample complexity 측면에서 악명 높게 비용이 많이 듭니다.

This quickly becomes extravagantly expensive, as the number of gradient steps and samples per step needed to learn an effective policy increases with task complexity.

이는 과제의 복잡성이 증가함에 따라 효과적인 정책을 학습하는 데 필요한 gradient steps의 수와 각 step당 샘플 수가 빠르게 증가함에 따라 지나치게 비싸게 됩니다.

To that end, we draw on the maximum entropy framework, which augments the standard maximum reward reinforcement learning objective with an entropy maximization term.

이를 위해, 우리는 표준 최대 보상 강화 학습 목표에 엔트로피 최대화 term을 추가하는 최대 엔트로피 framework를 활용합니다.

Maximum entropy reinforcement learning alters the RL objective, though the original objective can be recovered using a temperature parameter.

최대 엔트로피 강화 학습은 RL 목표를 변경하지만, 원래의 목표는 temperature parameter를 사용하여 회복될 수 있습니다.

On-policy training tends to improve stability but results in poor sample complexity.

On-policy 훈련은 안정성을 향상시키는 경향이 있지만, 샘플 복잡성이 낮은 결과를 초래합니다.

A similar method can be derived as a zero-step special case of stochastic value gradients (SVG(0)).

비슷한 방법은 확률적 가치 gradients(SVG(0))의 제로-스텝 특수 사례로 유도될 수 있습니다.

the Q-function is estimating the optimal Q-function, and the actor does not directly affect the Q-function except through the data distribution.

Q-function은 최적의 Q-function을 추정하고, actor는 데이터 분포를 통해서가 아니면 Q-function에 직접적인 영향을 미치지 않습니다.

Crucially, the convergence of this method hinges on how well this sampler approximates the true posterior.

중요하게도, 이 방법의 수렴은 이 샘플러가 실제 사후 확률을 얼마나 잘 근사하는지에 달려 있습니다.

In our experiments, we demonstrate that our soft actor-critic algorithm does in fact exceed the performance of prior state-of-the-art off-policy deep RL methods by a wide margin.

우리의 실험에서, 우리는 soft actor-critic 알고리즘이 실제로 이전의 최신 off-policy deep RL 방법들의 성능을 큰 차이로 능가함을 입증합니다.

We will use $ρ_π(s_t)$ and $ρ_π(s_t, a_t)$ to denote the state and state-action marginals of the trajectory distribution induced by a policy $π(a_t|s_t)$.

우리는 policy $π(a_t|s_t)$에 의해 유도된 trajectory 분포의 state와 state-action 마진을 나타내기 위해 $ρ_π(s_t)$와 $ρ_π(s_t, a_t)$를 사용할 것입니다.

We will consider a more general maximum entropy objective (see e.g. Ziebart (2010)), which favors stochastic policies by augmenting the objective with the expected entropy of the policy over $ρ_π(s_t)$:

$$J(\pi) = \sum_{t=0}^{T} \mathbb{E}_{(s\_t,a\_t) \sim \rho\_\pi} \left[ r(s\_t, a\_t) + \alpha \mathcal{H}(\pi(\cdot \mid s_t)) \right] \tag{1}$$

우리는 더 일반적인 최대 엔트로피 objective를 고려할 것입니다 (예: Ziebart (2010) 참조), 이는 $ρ_π(s_t)$에 대한 policy의 기대 엔트로피를 objective에 포함시켜 stochastic policies에 유리하게 작용합니다.

The maximum entropy objective differs from the standard maximum expected reward objective used in conventional reinforcement learning, though the conventional objective can be recovered in the limit as $α → 0$.

최대 엔트로피 objective는 전통적인 강화 학습에서 사용되는 표준 최대 기대 보상 objective와 다르지만, $α → 0$일 때 극한에서는 전통적인 objective를 되찾을 수 있습니다.

For the rest of this paper, we will omit writing the temperature explicitly, as it can always be subsumed into the reward by scaling it by $α^{−1}$.

이 논문의 나머지 부분에서는 temperature를 명시적으로 쓰지 않을 것입니다, 왜냐하면 $α^{−1}$로 스케일링하여 보상에 항상 포함될 수 있기 때문입니다.

Writing down the maximum entropy objective for the infinite horizon discounted case is more involved (Thomas, 2014) and is deferred to Appendix A.

Infinite horizon discounted 케이스에 대한 최대 엔트로피 objective를 기술하는 것은 더 복잡합니다 (Thomas, 2014) 그리고 이는 부록 A로 연기되었습니다.

We will begin by deriving soft policy iteration, a general algorithm for learning optimal maximum entropy policies that alternates between policy evaluation and policy improvement in the maximum entropy framework.

우리는 최대 엔트로피 프레임워크에서 policy evaluation과 policy improvement를 번갈아 가며 최적의 최대 엔트로피 policies를 학습하는 일반적인 알고리즘인 soft policy iteration을 유도하기 시작할 것입니다.

Therefore, we will next approximate the algorithm for continuous domains, where we need to rely on a function approximator to represent the Q-values, and running the two steps until convergence would be computationally too expensive. The approximation gives rise to a new practical algorithm, called soft actor-critic.

따라서, 우리는 continuous 도메인에 대한 알고리즘을 근사할 것입니다. 여기서는 Q-values를 나타내기 위해 function approximator에 의존해야 하며, 두 단계를 수렴할 때까지 실행하는 것은 계산적으로 너무 비싸게 됩니다. 이 근사는 soft actor-critic이라고 불리는 새로운 실용적인 알고리즘을 가져오게 됩니다.

Exhaustive sweeps implicitly devote equal time to all parts of the state space rather than focusing where it is needed.

철저한(exhaustive) 일괄처리는 상태 공간의 필요한 부분에 초점을 맞추기보다는 암암리에 상태 공간의 모든 부분에 동일한 시간을 할애한다.

More appealing is to distribute updates according to the on-policy distribution, that is, according to the distribution observed when following the current policy.

좀 더 매력적인 방법은 활성 정책 분포에 따라, 다시 말해 현재 정책을 따르는 동안 관측된 분포에 따라 갱신을 분산시키는 것이다.

In either case, sample state transitions and rewards are given by the model, and sample actions are given by the current policy.

어떤 경우든지, 표본 상태 전이와 보상은 모델에 의해 주어지고 표본 행동은 현재 정책에 의해 주어진다.

At any point in the planning process one can stop and exhaustively compute $v_{\tilde{\pi}}(s_0)$, the true value of the start state under the greedy policy, $\tilde{\pi}$, given the current action-value function $Q$, as an indication of how well the agent would do on a new episode on which it acted greedily (all the while assuming the model is correct).

계획 과정이 어떤 지점에서든 진행을 중단하고 $v_{\tilde{\pi}}(s_0)$를 철저하게 계산할 수 있다(이 모든 과정에서 모델이 정확하다고 가정한다). 이때 $v_{\tilde{\pi}}(s_0)$는 학습자가 탐욕적으로 행동할 새로운 에피소드에서 학습자가 얼마나 잘하는지를 나타내는 현재의 행동 가치 함수 $Q$가 주어졌을 때 탐욕적 정책 $\tilde{\pi}$ 하에서 시작 상태가 갖는 실제 가치를 나타낸다.

The quality of the policies found is plotted as a function of the number of expected updates completed.

발견된 정책의 품질이 완성된 기댓값 갱신의 개수에 대한 함수로 그려져 있다.

This presumably is why the exhaustive, unfocused approach does better in the long run, at least for small problems.

짐작컨데, 이것이 아마도 철저하고 초점을 두지 않는 접근법이 적어도 작은 문제에서는 장기적으로 더 좋은 성능을 보여주는 이유일 것이다.

If trajectories can start only from a designated set of start states, and if you are interested in the prediction problem for a given policy, then on-policy trajectory sampling allows the algorithm to completely skip states that cannot be reached by the given policy from any of the start states: such states are irrelevant to the prediction problem.

만약 궤적이 일련이 지정된 시작 상태로부터만 시작할 수 있다면, 그리고 정책이 주어진 상황에서 예측 문제에 관심이 있다면, 활성 정책 궤적 표본추출은 알고리즘으로 하여금 주어진 정책으로는 어떤 시작상태로부터도 도달할 수 없는 상태를 완전히 건너뛰게 해 준다. 이러한 상태들은 예측 문제와는 무관하다(irrelevant).

all rewards for transitions from non-goal states are strictly negative

목표 상태가 아닌 상태로부터의 전이에 대한 보상은 철저하게 음의 값이다.

More generally, planning used in this way can look much deeper than one-step-ahead and evaluate action choices leading to many different predicted state and reward trajectories. Unlike the first use of planning, here planning focuses on a particular state. We call this *decision-time planning*.

좀 더 일반적으로는, 이러한 방식으로 사용된 계획은 한 단계 앞보다 더 멀리 내다볼 수 있고, 이를 통해 상태와 보상의 다양한 예측 궤적을 초래하는 행동 선택을 평가할 수 있다. 계획을 처음 사용할 때와는 다르게, 이 경우에는 계획이 특정 상태에 초점을 맞춘다. 이 책에서는 이것을 결정 시각 계획(*decision-time planning*)이라고 부른다.

