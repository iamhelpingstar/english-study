Note that we are not using a discount as part of the problem specification; it will appear below as an algorithm parameter that adjusts a bias-variance tradeoff.

문제 사양의 일부로 discount을 사용하지 않고 있음을 유의하세요; 이것은 아래에서 편향-분산 트레이드오프를 조정하는 알고리즘 매개변수로 나타납니다.

Before proceeding, we will introduce the notion of a γ-just estimator of the advantage function, which is an estimator that does not introduce bias when we use it in place of $A^{π,γ}$(which is not known and must be estimated) in Equation (6) to estimate $g^{γ}$.

계속 진행하기 전에 식 (6)에서 $A^{π,γ}$(알 수 없으므로 추정해야 함) 대신 사용하여 $g^{γ}$를 추정할 때 편향을 도입하지 않는 추정기인 어드밴티지 함수의 γ-just 추정기라는 개념을 소개하겠습니다.

One sufficient condition for $\hat{A}_t$ to be γ-just is that $\hat{A}_t$ decomposes as the difference between two functions $Q_t$ and $b_t$, where $Q_t$ can depend on any trajectory variables but gives an unbiased estimator of the γ-discounted Q-function, and $b_t$ is an arbitrary function of the states and actions sampled before $a_t$.

$\hat{A}_t$가 γ-just인 충분 조건 중 하나는 $\hat{A}_t$가 두 함수 $Q_t$와 $b_t$의 차이로 분해되며, $Q_t$는 임의의 궤적 변수에 의존할 수 있지만 γ-discounted Q-function의 무편향 추정치를 제공하고, $b_t$는 $a_t$ 이전에 샘플링된 상태와 행동들의 임의 함수라는 것입니다.

This section will be concerned with producing an accurate estimate $\hat{A}_{t}$ of the discounted advantage function $A^{π,γ}(s_t, a_t)$, which will then be used to construct a policy gradient estimator of the following form:

이 섹션은 discounted advantage 함수 $A^{π,γ}(s_t, a_t)$의 정확한 추정치 $\hat{A}_{t}$를 생산하는 데에 중점을 두며, 이것은 다음 형태의 policy gradient 추정기를 구성하는 데 사용될 것입니다:

In fact, if we have the correct value function $V=V^{π,γ}$, then it is a γ-just advantage estimator, and in fact, an unbiased estimator $A^{π,γ}$

실제로, 우리가 올바른 가치 함수 $V=V^{π,γ}$를 가지고 있다면, 이것은 γ-just advantage 추정기이며, 사실상 무편향 추정기 $A^{π,γ}$입니다.

These equations result from a telescoping sum, and we see that $\hat{A}_t(k)$ involves a k-step estimate of the returns, minus a baseline term $V(s_t)$.

이 방정식들은 망원합으로부터 나오며, 우리는 $\hat{A}_t(k)$가 k-단계 return 추정치에서 baseline term - $V(s_t)$를 뺀 것을 포함한다는 것을 알 수 있습니다.

So, in 10-dim to capture 0.1% of the data we need 50% of the range.

그러므로, 10차원에서 0.1%의 데이터를 포착하기 위해 우리는 범위의 50%가 필요합니다.

What fraction of “space” do we need to cover to get 0.1% of data (10 nearest neighbors)

0.1%의 데이터(가장 가까운 이웃 10개)를 얻기 위해 필요한 "공간"의 비율은 얼마인가요?

Similar CDs have similar sets of customers, and vice-versa.

비슷한 CD는 비슷한 고객 세트를 가지며, 그 반대도 마찬가지입니다.

Smallest maximum distance to other points.

다른 점들까지의 가장 작은 최대 거리입니다.

Define a notion of cohesion, and merge clusters whose union is most cohesive.

응집력의 개념을 정의하고, 응집력이 가장 높은 합집합을 이루는 클러스터들을 병합합니다.

Stop if the density of the cluster that results from the best merger is below some threshold.

최상의 병합으로 인해 생성된 클러스터의 밀도가 일정 임계값 아래로 떨어지면 중지합니다.

The density can be defined in many different ways. Roughly, it should be the number of cluster points per unit volume of the cluster.

밀도는 여러 가지 방식으로 정의될 수 있습니다. 대략적으로, 클러스터의 단위 부피당 클러스터 점의 수로 정의되어야 합니다.

That ratio can be estimated by the number of points divided by some power of the diameter or radius of the cluster.

그 비율은 클러스터의 직경이나 반경의 어떤 거듭제곱으로 나눈 점의 수로 추정할 수 있습니다.

The correct power could be the number of dimensions of the space.

올바른 거듭제곱은 공간의 차원 수일 수 있습니다.

Sometimes, 1 or 2 is chosen as the power, regardless of the number of dimensions.

때때로, 차원 수에 관계없이 1 또는 2가 거듭제곱으로 선택됩니다.

Density of the merged cluster = divide by the number of points in the cluster by diameter or avg. distance.

병합된 클러스터의 밀도 = 클러스터 내 점의 수를 클러스터의 지름이나 평균 거리로 나눕니다.

Other suggested criteria involved measuring the density of a cluster, based on the radius or diameter.

다른 제안된 기준에는 반지름이나 지름을 기반으로 클러스터의 밀도를 측정하는 것이 포함됩니다.

Both these notions make sense in the non-Euclidean environment. The diameter is still the maximum distance between any two points in the cluster.

이 두 가지 개념은 비유클리드 환경에서도 의미가 있습니다. 지름은 여전히 클러스터 내 임의의 두 점 사이의 최대 거리입니다.

The radius can be defined using the clustroid in place of the centroid.

반지름은 중심점 대신 클러스토이드를 사용하여 정의할 수 있습니다.

Moreover, it makes sense to use the same sort of evaluation for the radius as we used to select the clustroid in the first place.

또한, 클러스토이드를 처음 선택할 때 사용한 것과 같은 종류의 평가를 반지름에 사용하는 것이 합리적입니다.

For example, if we take the clustroid to be the point with the smallest sum of squares of distances to the other nodes, then define the radius to be that sum of squares (or its square root).

예를 들어, 클러스토이드를 다른 노드들에 대한 거리의 제곱합이 가장 작은 점으로 취한다면, 반지름을 그 제곱합(또는 그 제곱근)으로 정의합니다.

tree (Any) – a pytree to be mapped over, with each leaf providing the first positional argument to `f`.

tree (Any) - 각 leaf가 `f`에게 첫 번째 positional argument를 제공하는, 매핑될 pytree입니다.

rest (Any) – a tuple of pytrees, each of which has the same structure as `tree` or has `tree` as a prefix.

rest (Any) - 각각이 `tree`와 같은 구조를 가지거나 `tree`를 접두사로 가지는 pytree들의 튜플입니다.

is_leaf (Callable[[Any], bool] | None) – an optionally specified function that will be called at each flattening step. It should return a boolean, which indicates whether the flattening should traverse the current object, or if it should be stopped immediately, with the whole subtree being treated as a leaf.

is_leaf (Callable[[Any], bool] | None) – 선택적으로 지정된 함수로, 각 평탄화 단계에서 호출됩니다. 이 함수는 boolean을 반환해야 하며, 이는 평탄화가 현재 객체를 계속 탐색해야 하는지, 또는 즉시 중단해야 하는지를 나타냅니다. 즉시 중단하는 경우, 전체 하위 트리가 leaf로 취급됩니다.

A new pytree with the same structure as `tree` but with the value at each leaf given by `f(x, *xs)` where `x` is the value at the corresponding leaf in `tree` and `xs` is the tuple of values at corresponding nodes in `rest`.

`tree`와 동일한 구조를 가진 새로운 pytree이지만, 각 leaf의 값은 `f(x, *xs)`로 주어집니다. 여기서 `x`는 `tree`의 해당 leaf에 있는 값이고 `xs`는 `rest`의 해당 노드들에 있는 값들의 튜플입니다.

If multiple inputs are passed, the structure of the tree is taken from the first input; subsequent inputs need only have `tree` as a prefix:

여러 입력이 전달될 경우, tree의 구조는 첫 번째 입력에서 가져옵니다; 이후의 입력들은 `tree`를 접두사로만 가지면 됩니다:

If the positional arguments to `fun` are container (pytree) types, the corresponding element of `in_axes` can itself be a matching container, so that distinct array axes can be mapped for different container elements.

`fun`에 대한 positional arguments가 컨테이너(pytree) 타입인 경우, `in_axes`의 해당 요소 자체도 일치하는 컨테이너가 될 수 있어, 서로 다른 컨테이너 요소에 대해 다른 배열 축이 매핑될 수 있습니다.

Axis integers must be in the range `[-ndim, ndim)` for each output array, where `ndim` is the number of dimensions (axes) of the array returned by the `vmap()`-ed function, which is one more than the number of dimensions (axes) of the corresponding array returned by `fun`.

출력 배열에 대한 축 정수는 각각의 출력 배열에 대해 `[-ndim, ndim)` 범위 내에 있어야 하며, 여기서 `ndim`은 `vmap()`에 의해 변환된 함수가 반환하는 배열의 차원(축) 수로, 이는 `fun`에 의해 반환된 해당 배열의 차원(축) 수보다 하나 더 많습니다.

Batched/vectorized version of `fun` with arguments that correspond to those of `fun`, but with extra array axes at positions indicated by `in_axes`, and a return value that corresponds to that of `fun`, but with extra array axes at positions indicated by `out_axes`.

`in_axes`에 의해 지정된 위치에 추가적인 배열 축을 가진 `fun`의 인자에 해당하는 인자를 가지고 있으며, `out_axes`에 의해 지정된 위치에 추가적인 배열 축을 가진 `fun`의 반환 값에 해당하는 반환 값을 가진 `fun`의 배치된/벡터화된 버전입니다.

The vdot(*a*, *b*) function handles complex numbers differently than dot(*a*, *b*). If the first argument is complex the complex conjugate of the first argument is used for the calculation of the dot product.

vdot(*a*, *b*) 함수는 dot(*a*, *b*)와 다르게 복소수를 다룬다. 첫 번째 인자가 복소수인 경우, 점곱셈 계산을 위해 첫 번째 인자의 복소수 켤레가 사용된다.

Typical implementations create a new instance of the class by invoking the superclass’s `__new__()` method using `super().__new__(cls[, ...])` with appropriate arguments and then modifying the newly created instance as necessary before returning it.

일반적인 구현에서는 적절한 인자를 사용하여 상위 클래스의 `__new__()` 메서드를 `super().__new__(cls[, ...])`를 호출하여 클래스의 새로운 인스턴스를 생성하고, 필요에 따라 새로 생성된 인스턴스를 수정한 후에 반환합니다.

If `__new__()` is invoked during object construction and it returns an instance of cls, then the new instance’s `__init__()` method will be invoked like `__init__(self[, ...])`, where self is the new instance and the remaining arguments are the same as were passed to the object constructor.

만약 객체 생성 중 `__new__()`가 호출되어 cls의 인스턴스를 반환하면, 새 인스턴스의 `__init__()` 메서드가 `__init__(self[, ...])`와 같이 호출됩니다. 여기서 self는 새 인스턴스를 나타내고 나머지 인자들은 객체 생성자에 전달된 것과 동일합니다.

`__new__()` is intended mainly to allow subclasses of immutable types (like int, str, or tuple) to customize instance creation. It is also commonly overridden in custom metaclasses in order to customize class creation.

`__new__()`는 주로 불변 타입(예: int, str, tuple 등)의 서브클래스가 인스턴스 생성을 사용자 정의할 수 있도록 하기 위한 목적으로 사용됩니다. 또한, 클래스 생성을 사용자 정의하기 위해 사용자 정의 메타클래스에서도 흔히 오버라이드됩니다.

We also introduce the notion of a response function to help understand the bias introduced by $γ$ and $λ$.

우리는 또한 $γ$와 $λ$에 의해 도입된 편향을 이해하는 데 도움이 되는 응답 함수 개념을 소개합니다.

The response function lets us quantify the temporal credit assignment problem: long range dependencies between actions and rewards correspond to nonzero values of the response function for $l \gg 0$.

Response function은 우리가 시간적 credit assignment 문제를 정량화할 수 있게 해줍니다: 행동과 보상 간의 장기 의존성은 $l ≫ 0$ 일 때 Response function의 nonzero 값에 해당합니다.

Thus, the error introduced by this approximation will be small if $\mathcal{X}$ rapidly decays as $l$ increases, i.e., if the effect of an action on rewards is “forgotten” after $≈ 1/(1 − γ)$ timesteps.

따라서, 이 근사에 의해 도입된 오류는 $l$이 증가함에 따라 $\mathcal{X}$가 빠르게 감소한다면, 즉, 행동의 영향이 대략 $1/(1 - γ)$ 타임스텝 후에 "잊혀진다면" 작을 것입니다.

This constraint is equivalent to constraining the average KL divergence between the previous value function and the new value function to be smaller than ϵ, where the value function is taken to parameterize a conditional Gaussian distribution with mean $V_\phi(s)$ and variance $σ^2$ .

이 제약은 이전 가치 함수와 새 가치 함수 사이의 평균 KL divergence가 ε보다 작게 유지되는 것과 동등합니다. 여기서 가치 함수는 평균이 $V_\phi(s)$이고 분산이 $σ^2$인 조건부 가우시안 분포를 매개변수화하는 것으로 간주됩니다.

Note that $H$ is the “Gauss-Newton” approximation of the Hessian of the objective, and it is (up to a $σ^2$ factor) the Fisher information matrix when interpreting the value function as a conditional probability distribution.

$H$는 목적의 헤시안 근사치에 대한 "가우스-뉴턴" 근사치이며, 값 함수를 조건부 확률 분포로 해석할 때 ($σ^2$ 계수까지) 피셔 정보 행렬이라는 점에 유의하세요.

While we experimented with this choice, we did not notice a difference in performance from the λ = 1 estimator in Equation (28).

우리는 이 선택을 실험해 보았지만, 식 (28)의 λ = 1 추정기와 성능에서 차이를 느끼지 못했습니다.

To see this, consider the extreme case where we overfit the value function, and the Bellman residual $r_t + γV (s_{t+1}) − V (s_t)$ becomes zero at all timesteps—the policy gradient estimate would be zero.

이를 이해하려면, Value function을 과적합하여 Bellman residual $r_t + γV (s_{t+1}) − V (s_t)$가 모든 타임스텝에서 0이 되는 극단적인 경우를 고려해보세요—Policy gradient 추정치는 0이 될 것입니다.

dynamically standing up, for the biped, which starts off laying on its back.

등을 대고 누워 시작하는, 이족보행을 위해, 동적으로 일어서는 동작입니다.

The humanoid model has 33 state dimensions and 10 actuated degrees of freedom, while the quadruped model has 29 state dimensions and 8 actuated degrees of freedom.

휴머노이드 모델은 33개의 상태 차원과 10개의 구동 자유도를 가지고 있으며, 사족보행 로봇 모델은 29개의 상태 차원과 8개의 구동 자유도를 가집니다.

Right: performance after 20 iterations of policy optimization, as γ and λ are varied. White means higher reward. The best results are obtained at intermediate values of both.

오른쪽: γ와 λ가 변할 때 Policy optimization의 20번 반복 후의 성능입니다. 흰색은 더 높은 보상을 의미합니다. 두 값의 중간 값에서 최상의 결과를 얻습니다.

if a sub-environment is not terminated nor truncated, PPO estimates the value of the next state in this sub-environment as the value target.

sub-environment가 terminated되거나 truncated되지 않은 경우, PPO는 이 sub-environment에서 다음 상태의 가치를 value target으로 추정합니다.

A class method can be called either on the class (such as `C.f()`) or on an instance (such as `C().f()`). The instance is ignored except for its class. If a class method is called for a derived class, the derived class object is passed as the implied first argument.

클래스 메서드는 클래스에 대해 (예: `C.f()`) 또는 인스턴스에 대해 (예: `C().f()`) 호출될 수 있습니다. 인스턴스는 그 클래스를 제외하고는 무시됩니다. 파생된 클래스에 대한 클래스 메서드가 호출되면, 파생된 클래스 객체가 암시적인 첫 번째 인자로 전달됩니다.

Various cohesion measures of the union of two clusters.

두 클러스터의 합집합에 대한 다양한 응집력 측정 방법입니다.

Each trial took about 2 hours to run on a 16-core machine, where the simulation rollouts were parallelized, as were the function, gradient, and matrix-vector-product evaluations used when optimizing the policy and value function.

각 시험은 16코어 기계에서 약 2시간이 걸렸으며, 여기서 시뮬레이션 롤아웃은 병렬화되었고, 정책과 가치 함수를 최적화할 때 사용되는 함수, 그래디언트, 행렬-벡터 곱셈 평가도 병렬화되었습니다.

Hence, it is plausible that this algorithm could be run on a real robot, or multiple real robots learning in parallel, if there were a way to reset the state of the robot and ensure that it doesn’t damage itself.

따라서, 로봇의 상태를 재설정하고 로봇이 스스로를 손상시키지 않도록 보장할 수 있는 방법이 있다면, 이 알고리즘이 실제 로봇이나 병렬로 학습하는 여러 실제 로봇에서 실행될 수 있을 것으로 보입니다.

For 3D standing, the value function always helped, but the results are roughly the same for λ = 0.96 and λ = 1.

3D 서기의 경우, Value function은 항상 도움이 되었지만, λ = 0.96과 λ = 1의 결과는 대략 비슷합니다.

Policy gradient methods provide a way to reduce reinforcement learning to stochastic gradient descent, by providing unbiased gradient estimates.

Policy gradient 방법은 편향되지 않은 그래디언트 추정치를 제공함으로써 강화 학습을 확률적 그래디언트 하강으로 줄이는 방법을 제공합니다.

One question that merits future investigation is the relationship between value function estimation error and policy gradient estimation error.

미래의 조사가 필요한 한 가지 질문은 Value function 추정 오류와 Policy gradient 추정 오류 사이의 관계입니다.

If this relationship were known, we could choose an error metric for value function fitting that is well-matched to the quantity of interest, which is typically the accuracy of the policy gradient estimation.

이 관계를 알고 있다면, 일반적으로 Policy gradient 추정의 정확도인 관심 있는(interest) 양에 잘 맞는 Value function 적합을 위한 오류 척도를 선택할 수 있을 것입니다.

While formulating this problem in a way that is suitable for numerical optimization and provides convergence guarantees remains an open question, such an approach could allow the value function and policy representations to share useful features of the input, resulting in even faster learning.

이 문제를 수치 최적화에 적합하고 수렴 보장을 제공하는 방식으로 구성하는 것은 여전히 열린 질문이지만, 이러한 접근은 Value function과 Policy 표현이 입력의 유용한 특징을 공유하게 하여, 더욱 빠른 학습을 가능하게 할 수 있습니다.

However, note that those papers consider control problems with substantially lower-dimensional state and action spaces than the ones considered here.

하지만, 그 논문들은 여기서 고려된 것보다 훨씬 낮은 차원의 상태와 행동 공간을 가진 제어 문제를 고려한다는 점에 유의하세요.

WHAT’S THE RELATIONSHIP WITH COMPATIBLE FEATURES?

COMPATIBLE FEATURES와의 관계는 무엇인가요?

Compatible features are often mentioned in relation to policy gradient algorithms that make use of a value function, and the idea was proposed in the paper On Actor-Critic Methods by Konda & Tsitsiklis (2003).

Compatible features는 종종 Value function을 사용하는 Policy gradient 알고리즘과 관련하여 언급되며, 이 아이디어는 Konda & Tsitsiklis (2003)의 논문 On Actor-Critic Methods에서 제안되었습니다.

These authors pointed out that due to the limited representation power of the policy, the policy gradient only depends on a certain subspace of the space of advantage functions.

이 저자들은 정책의 제한된 표현력 때문에, Policy gradient는 advantage 함수 공간의 특정 부분 공간에만 의존한다고 지적했습니다.

This subspace is spanned by the compatible features.

이 부분 공간은 Compatible features에 의해 생성됩니다.

This theory of compatible features provides no guidance on how to exploit the temporal structure of the problem to obtain better estimates of the advantage function, making it mostly orthogonal to the ideas in this paper.

Compatible features 이론은 문제의 Temporal 구조를 활용하여 Advantage function의 더 나은 추정치를 얻는 방법에 대한 지침을 제공하지 않아, 이 논문의 아이디어와 대부분 orthogonal합니다.

Another enticing possibility is to use a shared function approximation architecture for the policy and the value function, while optimizing the policy using generalized advantage estimation.

또 다른 매력적인 가능성은 Policy와 Value function에 대해 공유 함수 근사 아키텍처를 사용하면서, Policy를 generalized advantage 추정을 사용하여 최적화하는 것입니다.

In particular, to be able to “match up” leaves in these parameter pytrees with values in the argument pytrees, the parameter pytrees are often constrained to be tree prefixes of the argument pytrees.

특히, 이러한 parameter pytrees의 잎(leaf)과 argument pytrees의 값 사이에 "매칭"을 할 수 있도록, parameter pytrees는 종종 argument pytrees의 트리 접두사로 제한됩니다.

a pair of an iterable with the children to be flattened recursively, and some opaque auxiliary data to pass back to the unflattening recipe.

children을 재귀적으로 평탄화할 iterable과, unflattening recipe에 전달할 불투명한 auxiliary 데이터의 쌍입니다.

Using $q$ to denote the sampling distribution, the contribution of a single $s_n$ to the loss function is

$q$를 샘플링 분포를 나타내는 데 사용하면, 단일 $s_n$의 손실 함수에 대한 기여는 다음과 같다.

For each of these states $s_n$, we perform multiple actions ($a_1$ and $a_2$ here) and perform a rollout after each action, using common random numbers (CRN) to reduce the variance.

이러한 상태들 중 각각의 $s_n$에 대해, 우리는 여러 행동($a_1$과 $a_2$ 여기서)을 수행하고 각 행동 후에 롤아웃을 수행하며, 분산을 줄이기 위해 공통 랜덤 숫자(CRN)를 사용한다.

The analytic estimator integrates over the action at each state sn, and does not depend on the action an that was sampled.

분석적 추정치는 각 상태 sn에서의 행동에 대해 통합하고, 샘플링된 행동 an에 의존하지 않는다.

Score all pairs of records that the LSH scheme identified as candidates; report high scores as matches.

LSH 스키마에서 후보로 식별된 모든 레코드 쌍의 점수를 매기고, 높은 점수를 일치 항목으로 보고합니다.

Any field not used in the LSH could have been used to validate, provided corresponding values  were closer for true matches than false.

LSH에 사용되지 않은 필드도 해당 값이 거짓보다 참 일치에 더 가깝다면 유효성 검사에 사용할 수 있었습니다.

Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour.

가치 분포를 연구하는 확립된 문헌이 있지만, 지금까지는 위험 인식 행동과 같은 특정 목적을 위해 항상 사용되어 왔습니다.

We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter.

우리는 정책 평가와 제어 설정 모두에서 이론적 결과로 시작하여, 후자에서 중요한 분포적 불안정성을 드러냅니다.

We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning.

우리는 근사 강화 학습에서 가치 분포의 중요성을 입증하는 최신 결과와 일화적 증거를 모두 얻었습니다.

Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.

마지막으로, 우리는 이론적 및 경험적 증거를 결합하여 근사 설정에서 가치 분포가 학습에 미치는 영향을 강조합니다.

One of the major tenets of reinforcement learning states that, when not otherwise constrained in its behaviour, an agent should aim to maximize its expected utility Q, or Value (Sutton & Barto, 1998).

강화 학습의 주요 원칙 중 하나는, 행동에 다른 제약이 없는 경우, 에이전트는 기대 유틸리티 Q 또는 Value를 최대화하는 것을 목표로 해야 한다고 명시합니다 (Sutton & Barto, 1998).

Bellman’s equation succinctly describes this value in terms of the expected reward and expected outcome of the random transition $(x, a) → (X, A)$.

벨만 방정식은 기대 보상과 무작위 전환 $(x, a) → (X', A')$의 예상 결과 측면에서 이 Value를 간결하게 설명합니다.

In this paper, we aim to go beyond the notion of value and argue in favour of a distributional perspective on reinforcement learning.

이 논문에서는 Value 개념을 넘어서 강화 학습에 대한 분포적 관점을 지지하는 논의를 하고자 합니다.

This random return is also described by a recursive equation, but one of a distributional nature:

이 무작위 반환도 재귀 방정식에 의해 설명되지만, 그것은 분포적 성격을 가진 방정식입니다.

By analogy with the well-known case, we call this quantity the value distribution.

잘 알려진 경우와 비슷하게, 우리는 이 양을 Value distribution이라고 부릅니다.

in reinforcement learning it has thus far been subordinated to specific purposes.

강화 학습에서는 지금까지 특정 목적에 종속되어 왔습니다.

Specifically, although the optimality operator is a contraction in expected value (matching the usual optimality result), it is not a contraction in any metric over distributions.

구체적으로, 최적성 연산자가 예상 가치에서 수축(통상적인 최적성 결과와 일치)하는 것은 사실이지만, 분포에 대한 어떤 척도에서도 수축이 아닙니다.

These results provide evidence in favour of learning algorithms that model the effects of nonstationary policies.

이 결과들은 비정상적(nonstationary) 정책의 영향을 모델링하는 학습 알고리즘을 지지하는 증거를 제공합니다.

As a whole, we argue that this approach makes approximate reinforcement learning significantly better behaved.

전체적으로, 우리는 이 접근 방식이 근사 강화 학습을 훨씬 더 잘 행동하게 만든다고 주장합니다.

By modelling the value distribution within a DQN agent (Mnih et al., 2015), we obtain considerably increased performance across the gamut of benchmark Atari 2600 games, and in fact achieve state-of-the-art performance on a number of games.

DQN 에이전트(Mnih et al., 2015) 내에서 Value distribution을 모델링함으로써, 벤치마크 Atari 2600 게임 전반에 걸쳐 상당히 향상된 성능을 얻었으며, 실제로 여러 게임에서 최신 성능을 달성했습니다.

The main distinction, of course, is that in our setting there are no given targets.

물론, 우리 설정의 주요 차이점은 주어진 목표가 없다는 것입니다.

Our first aim is to gain an understanding of the theoretical behaviour of the distributional analogues of the Bellman operators, in particular in the less well-understood control setting.

우리의 첫 번째 목표는 벨만 연산자의 분포적 유사물, 특히 덜 이해된 제어 설정에서의 이론적 행동에 대한 이해를 얻는 것입니다.

We will find it convenient to conflate the random variables under consideration with their versions under the $\text{inf}$, writing $d_p(U, V)= \underset{U, V}{\text{inf}}\| U - V\|_p$ whenever it is unambiguous:

고려 중인 무작위 변수를 $\text{inf}$ 아래에 해당 버전과 결합하고 모호하지 않을 때마다 $d_p(U, V)= \underset{U, V}{\text{inf}}\| U - V\|_p$을 작성하는 것이 편리할 것입니다:

we believe the greater legibility justifies the technical inaccuracy.

우리는 더 큰 가독성이 기술적 부정확성을 정당화한다고 믿습니다.

We will need the following additional property, which makes no independence assumptions on its variables.

우리는 변수에 대한 독립성 가정을 하지 않는 다음과 같은 추가적인 속성이 필요합니다.

While Tπ bears a surface resemblance to the usual Bellman operator (2), it is fundamentally different.

Tπ는 표면적으로는 일반적인 벨만 연산자(2)와 유사해 보이지만, 근본적으로 다릅니다.

In particular, three sources of randomness define the compound distribution $\mathcal{T}^{π}Z$:

특히, 세 가지 무작위성 소스가 복합 분포 $\mathcal{T}^{π}Z$를 정의합니다:

We now set out to understand the distributional operators of the control setting – where we seek a policy $π$ that maximizes value –and the corresponding notion of an optimal value distribution.

이제 우리는 가치를 극대화하는 정책 $π$를 추구하는 제어 설정의 분포 연산자와 최적 가치 분포에 해당하는 개념을 이해하려고 합니다.

As with the optimal value function, this notion is intimately tied to that of an optimal policy.

최적 가치 함수와 마찬가지로, 이 개념은 최적 정책과 밀접하게 연결되어 있습니다.

In this section we show that the distributional analogue of the Bellman optimality operator converges, in a weak sense, to the set of optimal value distributions.

이 섹션에서는 벨만 최적성 연산자의 분포적 유사체가 약한 의미에서 최적 가치 분포 집합으로 수렴한다는 것을 보여줍니다.

However, this operator is not a contraction in any metric between distributions, and is in general much more temperamental than the policy evaluation operators.

그러나 이 연산자는 분포 간의 어떤 척도에서도 수축이 아니며, 일반적으로 정책 평가 연산자들보다 훨씬 더 변덕스럽습니다.

We believe the convergence issues we outline here are a symptom of the inherent instability of greedy updates, as highlighted by e.g. Tsitsiklis (2002) and most recently Harutyunyan et al. (2016).

여기서 개요를 제시하는 수렴 문제들은 Tsitsiklis (2002)와 가장 최근에 Harutyunyan et al. (2016)이 강조한 바와 같이, 탐욕스러운 업데이트의 내재적 불안정성의 증상이라고 믿습니다.

Although this policy is implicit in (6), we cannot ignore it in the distributional setting.

이 정책이 (6)에서는 암시되어 있지만, 분포적 설정에서는 무시할 수 없습니다.

By inspecting Lemma 4.

Lemma 4를 검토함으로써.

In fact, the best we can hope for is pointwise convergence, not even to the set $\mathcal{Z}^{∗}$ but to the larger set of nonstationary optimal value distributions.

사실, 우리가 바랄 수 있는 최선은 $\mathcal{Z}^{∗}$ 집합이 아니라 더 큰 비정상적(nonstationary) 최적 가치 분포 집합에 대한 점별 수렴입니다.

Statistical learning refers to a set of tools for making sense of complex datasets.

통계적 학습은 복잡한 데이터셋을 이해하기 위한 일련의 도구들을 의미합니다.

In recent years, we have seen a staggering increase in the scale and scope of data collection across virtually all areas of science and industry.

최근 몇 년 동안, 과학과 산업의 거의 모든 분야에서 데이터 수집의 규모와 범위가 엄청나게 증가했다는 것을 목격하였습니다.

Since it was published in 2013, ISLR has become a mainstay of undergraduate and graduate classrooms worldwide, as well as an important reference book for data scientists.

2013년에 출판된 이후, ISLR은 전 세계 대학교의 학부 및 대학원 교실에서 중요한 교재로 자리잡았으며, 데이터 과학자들에게 중요한 참고서로도 사용되고 있습니다.

The intention behind ISLP (and ISLR) is to concentrate more on the applications of the methods and less on the mathematical details, so it is appropriate for advanced undergraduates or master’s students in statistics or related quantitative fields, or for individuals in other disciplines who wish to use statistical learning tools to analyze their data.

ISLP(와 ISLR)의 목적은 방법론의 적용에 더 초점을 맞추고 수학적 세부 사항에는 덜 집중하는 것이므로, 통계학이나 관련 정량적 분야의 고급 학부생이나 석사 과정 학생들, 또는 자신들의 데이터를 분석하기 위해 통계적 학습 도구를 사용하고자 하는 다른 분야의 개인에게 적합합니다.

It has been an honor and a privilege for us to see the considerable impact that ISLR has had on the way in which statistical learning is practiced, both in and out of the academic setting.

ISLR이 학계 안팎에서 통계적 학습이 실행되는 방식에 미친 상당한 영향을 목격하는 것은 우리에게 영광이자 특권이었습니다.

Statistical learning refers to a vast set of tools for understanding data.

통계적 학습은 데이터를 이해하기 위한 광범위한 도구들을 의미합니다.

However, it is also clear from Figure 1.1 that there is a significant amount of variability associated with this average value, and so age alone is unlikely to provide an accurate prediction of a particular man’s wage.

그러나 그림 1.1에서 분명히 보이듯이, 이 평균 값에는 상당한 변동성이 관련되어 있으며, 따라서 나이만으로는 특정 남성의 임금을 정확히 예측하기 어려울 것입니다.

Wages increase by approximately $10,000, in a roughly linear (or straight-line) fashion, between 2003 and 2009, though this rise is very slight relative to the variability in the data.

2003년부터 2009년 사이에 임금은 대략 $10,000 증가하며, 이는 대략적으로 선형적(또는 직선적)인 방식으로 이루어집니다. 그러나 이 증가는 데이터의 변동성에 비해 매우 미미합니다.

Boxplots of the previous day’s percentage change in the S&P index for the days for which the market increased or decreased, obtained from the Smarket data.

Smarket 데이터에서 얻은, 시장이 상승하거나 하락한 날들에 대한 S&P 지수의 전날 퍼센트 변화에 대한 박스플롯입니다.

A model that could accurately predict the direction in which the market will move would be very useful!

시장이 움직일 방향을 정확하게 예측할 수 있는 모델은 매우 유용할 것입니다!

The left-hand panel of Figure 1.2 displays two boxplots of the previous day’s percentage changes in the stock index: one for the 648 days for which the market increased on the subsequent day, and one for the 602 days for which the market decreased.

그림 1.2의 왼쪽 패널은 주식 지수의 전날 퍼센트 변화에 대한 두 개의 박스플롯을 보여줍니다: 하나는 시장이 다음 날 상승한 648일에 대한 것이고, 다른 하나는 시장이 하락한 602일에 대한 것입니다.

Of course, this lack of pattern is to be expected: in the presence of strong correlations between successive days’ returns, one could adopt a simple trading strategy to generate profits from the market.

물론, 이러한 패턴의 부재는 예상되는 바입니다: 연속적인 날들의 수익률 사이에 강한 상관관계가 있을 경우, 시장에서 수익을 창출하기 위해 간단한 거래 전략을 채택할 수 있습니다.

Interestingly, there are hints of some weak trends in the data that suggest that, at least for this 5-year period, it is possible to correctly predict the direction of movement in the market approximately 60% of the time (Figure 1.3).

흥미롭게도, 데이터에는 약한 추세의 단서가 있어, 적어도 이 5년 기간 동안 시장의 움직임 방향을 약 60%의 정확도로 예측할 수 있다는 것을 시사합니다 (그림 1.3).

For example, in a marketing setting, we might have demographic information for a number of current or potential customers.

예를 들어, 마케팅 상황에서, 우리는 현재 또는 잠재 고객들에 대한 인구통계적 정보를 가지고 있을 수 있습니다.

We fit a quadratic discriminant analysis model to the subset of the Smarket data corresponding to the 2001–2004 time period, and predicted the probability of a stock market decrease using the 2005 data.

Quadratic discriminant analysis 모델을 2001년부터 2004년까지의 기간에 해당하는 Smarket 데이터의 부분집합에 적용하고, 2005년 데이터를 사용하여 주식 시장 하락의 확률을 예측했습니다.

On average, the predicted probability of decrease is higher for the days in which the market does decrease.

평균적으로, 시장이 하락하는 날에는 하락할 확률이 더 높게 예측됩니다.

We devote Chapter 12 to a discussion of statistical learning methods for problems in which no natural output variable is available.

우리는 자연스러운 출력 변수가 없는 문제에 대한 통계적 학습 방법에 대한 논의를 12장에 할애합니다.

There is clear evidence that cell lines with the same cancer type tend to be located near each other in this two-dimensional representation.

이러한 이차원 표현에서 같은 암 유형을 가진 세포주들이 서로 가까이 위치하는 경향이 있다는 명확한 증거가 있습니다.

In addition, even though the cancer information was not used to produce the left-hand panel, the clustering obtained does bear some resemblance to some of the actual cancer types observed in the right-hand panel.

또한, 왼쪽 패널을 생성하는 데 암 정보가 사용되지 않았음에도 불구하고, 얻어진 군집은 실제로 오른쪽 패널에서 관찰된 일부 암 유형과 어느 정도 유사성을 보입니다.

This provides some independent verification of the accuracy of our clustering analysis.

이것은 우리의 군집 분석의 정확성에 대한 독립적인 검증을 제공합니다.

