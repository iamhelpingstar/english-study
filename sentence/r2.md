Note that $H$ is the “Gauss-Newton” approximation of the Hessian of the objective, and it is (up to a $σ^2$ factor) the Fisher information matrix when interpreting the value function as a conditional probability distribution.

$H$는 목적의 헤시안 근사치에 대한 "가우스-뉴턴" 근사치이며, 값 함수를 조건부 확률 분포로 해석할 때 ($σ^2$ 계수까지) 피셔 정보 행렬이라는 점에 유의하세요.

While we experimented with this choice, we did not notice a difference in performance from the λ = 1 estimator in Equation (28).

우리는 이 선택을 실험해 보았지만, 식 (28)의 λ = 1 추정기와 성능에서 차이를 느끼지 못했습니다.

To see this, consider the extreme case where we overfit the value function, and the Bellman residual $r_t + γV (s_{t+1}) − V (s_t)$ becomes zero at all timesteps—the policy gradient estimate would be zero.

이를 이해하려면, Value function을 과적합하여 Bellman residual $r_t + γV (s_{t+1}) − V (s_t)$가 모든 타임스텝에서 0이 되는 극단적인 경우를 고려해보세요—Policy gradient 추정치는 0이 될 것입니다.

dynamically standing up, for the biped, which starts off laying on its back.

등을 대고 누워 시작하는, 이족보행을 위해, 동적으로 일어서는 동작입니다.

The humanoid model has 33 state dimensions and 10 actuated degrees of freedom, while the quadruped model has 29 state dimensions and 8 actuated degrees of freedom.

휴머노이드 모델은 33개의 상태 차원과 10개의 구동 자유도를 가지고 있으며, 사족보행 로봇 모델은 29개의 상태 차원과 8개의 구동 자유도를 가집니다.

Right: performance after 20 iterations of policy optimization, as γ and λ are varied. White means higher reward. The best results are obtained at intermediate values of both.

오른쪽: γ와 λ가 변할 때 Policy optimization의 20번 반복 후의 성능입니다. 흰색은 더 높은 보상을 의미합니다. 두 값의 중간 값에서 최상의 결과를 얻습니다.

if a sub-environment is not terminated nor truncated, PPO estimates the value of the next state in this sub-environment as the value target.

sub-environment가 terminated되거나 truncated되지 않은 경우, PPO는 이 sub-environment에서 다음 상태의 가치를 value target으로 추정합니다.

A class method can be called either on the class (such as `C.f()`) or on an instance (such as `C().f()`). The instance is ignored except for its class. If a class method is called for a derived class, the derived class object is passed as the implied first argument.

클래스 메서드는 클래스에 대해 (예: `C.f()`) 또는 인스턴스에 대해 (예: `C().f()`) 호출될 수 있습니다. 인스턴스는 그 클래스를 제외하고는 무시됩니다. 파생된 클래스에 대한 클래스 메서드가 호출되면, 파생된 클래스 객체가 암시적인 첫 번째 인자로 전달됩니다.

Various cohesion measures of the union of two clusters.

두 클러스터의 합집합에 대한 다양한 응집력 측정 방법입니다.

Each trial took about 2 hours to run on a 16-core machine, where the simulation rollouts were parallelized, as were the function, gradient, and matrix-vector-product evaluations used when optimizing the policy and value function.

각 시험은 16코어 기계에서 약 2시간이 걸렸으며, 여기서 시뮬레이션 롤아웃은 병렬화되었고, 정책과 가치 함수를 최적화할 때 사용되는 함수, 그래디언트, 행렬-벡터 곱셈 평가도 병렬화되었습니다.

Hence, it is plausible that this algorithm could be run on a real robot, or multiple real robots learning in parallel, if there were a way to reset the state of the robot and ensure that it doesn’t damage itself.

따라서, 로봇의 상태를 재설정하고 로봇이 스스로를 손상시키지 않도록 보장할 수 있는 방법이 있다면, 이 알고리즘이 실제 로봇이나 병렬로 학습하는 여러 실제 로봇에서 실행될 수 있을 것으로 보입니다.

For 3D standing, the value function always helped, but the results are roughly the same for λ = 0.96 and λ = 1.

3D 서기의 경우, Value function은 항상 도움이 되었지만, λ = 0.96과 λ = 1의 결과는 대략 비슷합니다.

Policy gradient methods provide a way to reduce reinforcement learning to stochastic gradient descent, by providing unbiased gradient estimates.

Policy gradient 방법은 편향되지 않은 그래디언트 추정치를 제공함으로써 강화 학습을 확률적 그래디언트 하강으로 줄이는 방법을 제공합니다.

One question that merits future investigation is the relationship between value function estimation error and policy gradient estimation error.

미래의 조사가 필요한 한 가지 질문은 Value function 추정 오류와 Policy gradient 추정 오류 사이의 관계입니다.

If this relationship were known, we could choose an error metric for value function fitting that is well-matched to the quantity of interest, which is typically the accuracy of the policy gradient estimation.

이 관계를 알고 있다면, 일반적으로 Policy gradient 추정의 정확도인 관심 있는(interest) 양에 잘 맞는 Value function 적합을 위한 오류 척도를 선택할 수 있을 것입니다.

While formulating this problem in a way that is suitable for numerical optimization and provides convergence guarantees remains an open question, such an approach could allow the value function and policy representations to share useful features of the input, resulting in even faster learning.

이 문제를 수치 최적화에 적합하고 수렴 보장을 제공하는 방식으로 구성하는 것은 여전히 열린 질문이지만, 이러한 접근은 Value function과 Policy 표현이 입력의 유용한 특징을 공유하게 하여, 더욱 빠른 학습을 가능하게 할 수 있습니다.

However, note that those papers consider control problems with substantially lower-dimensional state and action spaces than the ones considered here.

하지만, 그 논문들은 여기서 고려된 것보다 훨씬 낮은 차원의 상태와 행동 공간을 가진 제어 문제를 고려한다는 점에 유의하세요.

WHAT’S THE RELATIONSHIP WITH COMPATIBLE FEATURES?

COMPATIBLE FEATURES와의 관계는 무엇인가요?

Compatible features are often mentioned in relation to policy gradient algorithms that make use of a value function, and the idea was proposed in the paper On Actor-Critic Methods by Konda & Tsitsiklis (2003).

Compatible features는 종종 Value function을 사용하는 Policy gradient 알고리즘과 관련하여 언급되며, 이 아이디어는 Konda & Tsitsiklis (2003)의 논문 On Actor-Critic Methods에서 제안되었습니다.

These authors pointed out that due to the limited representation power of the policy, the policy gradient only depends on a certain subspace of the space of advantage functions.

이 저자들은 정책의 제한된 표현력 때문에, Policy gradient는 advantage 함수 공간의 특정 부분 공간에만 의존한다고 지적했습니다.

This subspace is spanned by the compatible features.

이 부분 공간은 Compatible features에 의해 생성됩니다.

This theory of compatible features provides no guidance on how to exploit the temporal structure of the problem to obtain better estimates of the advantage function, making it mostly orthogonal to the ideas in this paper.

Compatible features 이론은 문제의 Temporal 구조를 활용하여 Advantage function의 더 나은 추정치를 얻는 방법에 대한 지침을 제공하지 않아, 이 논문의 아이디어와 대부분 orthogonal합니다.

Another enticing possibility is to use a shared function approximation architecture for the policy and the value function, while optimizing the policy using generalized advantage estimation.

또 다른 매력적인 가능성은 Policy와 Value function에 대해 공유 함수 근사 아키텍처를 사용하면서, Policy를 generalized advantage 추정을 사용하여 최적화하는 것입니다.

In particular, to be able to “match up” leaves in these parameter pytrees with values in the argument pytrees, the parameter pytrees are often constrained to be tree prefixes of the argument pytrees.

특히, 이러한 parameter pytrees의 잎(leaf)과 argument pytrees의 값 사이에 "매칭"을 할 수 있도록, parameter pytrees는 종종 argument pytrees의 트리 접두사로 제한됩니다.

a pair of an iterable with the children to be flattened recursively, and some opaque auxiliary data to pass back to the unflattening recipe.

children을 재귀적으로 평탄화할 iterable과, unflattening recipe에 전달할 불투명한 auxiliary 데이터의 쌍입니다.

Using $q$ to denote the sampling distribution, the contribution of a single $s_n$ to the loss function is

$q$를 샘플링 분포를 나타내는 데 사용하면, 단일 $s_n$의 손실 함수에 대한 기여는 다음과 같다.

For each of these states $s_n$, we perform multiple actions ($a_1$ and $a_2$ here) and perform a rollout after each action, using common random numbers (CRN) to reduce the variance.

이러한 상태들 중 각각의 $s_n$에 대해, 우리는 여러 행동($a_1$과 $a_2$ 여기서)을 수행하고 각 행동 후에 롤아웃을 수행하며, 분산을 줄이기 위해 공통 랜덤 숫자(CRN)를 사용한다.

The analytic estimator integrates over the action at each state sn, and does not depend on the action an that was sampled.

분석적 추정치는 각 상태 sn에서의 행동에 대해 통합하고, 샘플링된 행동 an에 의존하지 않는다.

Score all pairs of records that the LSH scheme identified as candidates; report high scores as matches.

LSH 스키마에서 후보로 식별된 모든 레코드 쌍의 점수를 매기고, 높은 점수를 일치 항목으로 보고합니다.

Any field not used in the LSH could have been used to validate, provided corresponding values  were closer for true matches than false.

LSH에 사용되지 않은 필드도 해당 값이 거짓보다 참 일치에 더 가깝다면 유효성 검사에 사용할 수 있었습니다.

Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour.

가치 분포를 연구하는 확립된 문헌이 있지만, 지금까지는 위험 인식 행동과 같은 특정 목적을 위해 항상 사용되어 왔습니다.

We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter.

우리는 정책 평가와 제어 설정 모두에서 이론적 결과로 시작하여, 후자에서 중요한 분포적 불안정성을 드러냅니다.

We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning.

우리는 근사 강화 학습에서 가치 분포의 중요성을 입증하는 최신 결과와 일화적 증거를 모두 얻었습니다.

Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.

마지막으로, 우리는 이론적 및 경험적 증거를 결합하여 근사 설정에서 가치 분포가 학습에 미치는 영향을 강조합니다.

One of the major tenets of reinforcement learning states that, when not otherwise constrained in its behaviour, an agent should aim to maximize its expected utility Q, or Value (Sutton & Barto, 1998).

강화 학습의 주요 원칙 중 하나는, 행동에 다른 제약이 없는 경우, 에이전트는 기대 유틸리티 Q 또는 Value를 최대화하는 것을 목표로 해야 한다고 명시합니다 (Sutton & Barto, 1998).

Bellman’s equation succinctly describes this value in terms of the expected reward and expected outcome of the random transition $(x, a) → (X, A)$.

벨만 방정식은 기대 보상과 무작위 전환 $(x, a) → (X', A')$의 예상 결과 측면에서 이 Value를 간결하게 설명합니다.

In this paper, we aim to go beyond the notion of value and argue in favour of a distributional perspective on reinforcement learning.

이 논문에서는 Value 개념을 넘어서 강화 학습에 대한 분포적 관점을 지지하는 논의를 하고자 합니다.

This random return is also described by a recursive equation, but one of a distributional nature:

이 무작위 반환도 재귀 방정식에 의해 설명되지만, 그것은 분포적 성격을 가진 방정식입니다.

By analogy with the well-known case, we call this quantity the value distribution.

잘 알려진 경우와 비슷하게, 우리는 이 양을 Value distribution이라고 부릅니다.

in reinforcement learning it has thus far been subordinated to specific purposes.

강화 학습에서는 지금까지 특정 목적에 종속되어 왔습니다.

Specifically, although the optimality operator is a contraction in expected value (matching the usual optimality result), it is not a contraction in any metric over distributions.

구체적으로, 최적성 연산자가 예상 가치에서 수축(통상적인 최적성 결과와 일치)하는 것은 사실이지만, 분포에 대한 어떤 척도에서도 수축이 아닙니다.

These results provide evidence in favour of learning algorithms that model the effects of nonstationary policies.

이 결과들은 비정상적(nonstationary) 정책의 영향을 모델링하는 학습 알고리즘을 지지하는 증거를 제공합니다.

As a whole, we argue that this approach makes approximate reinforcement learning significantly better behaved.

전체적으로, 우리는 이 접근 방식이 근사 강화 학습을 훨씬 더 잘 행동하게 만든다고 주장합니다.

By modelling the value distribution within a DQN agent (Mnih et al., 2015), we obtain considerably increased performance across the gamut of benchmark Atari 2600 games, and in fact achieve state-of-the-art performance on a number of games.

DQN 에이전트(Mnih et al., 2015) 내에서 Value distribution을 모델링함으로써, 벤치마크 Atari 2600 게임 전반에 걸쳐 상당히 향상된 성능을 얻었으며, 실제로 여러 게임에서 최신 성능을 달성했습니다.

The main distinction, of course, is that in our setting there are no given targets.

물론, 우리 설정의 주요 차이점은 주어진 목표가 없다는 것입니다.

Our first aim is to gain an understanding of the theoretical behaviour of the distributional analogues of the Bellman operators, in particular in the less well-understood control setting.

우리의 첫 번째 목표는 벨만 연산자의 분포적 유사물, 특히 덜 이해된 제어 설정에서의 이론적 행동에 대한 이해를 얻는 것입니다.

We will find it convenient to conflate the random variables under consideration with their versions under the $\text{inf}$, writing $d_p(U, V)= \underset{U, V}{\text{inf}}\| U - V\|_p$ whenever it is unambiguous:

고려 중인 무작위 변수를 $\text{inf}$ 아래에 해당 버전과 결합하고 모호하지 않을 때마다 $d_p(U, V)= \underset{U, V}{\text{inf}}\| U - V\|_p$을 작성하는 것이 편리할 것입니다:

we believe the greater legibility justifies the technical inaccuracy.

우리는 더 큰 가독성이 기술적 부정확성을 정당화한다고 믿습니다.

We will need the following additional property, which makes no independence assumptions on its variables.

우리는 변수에 대한 독립성 가정을 하지 않는 다음과 같은 추가적인 속성이 필요합니다.

While Tπ bears a surface resemblance to the usual Bellman operator (2), it is fundamentally different.

Tπ는 표면적으로는 일반적인 벨만 연산자(2)와 유사해 보이지만, 근본적으로 다릅니다.

In particular, three sources of randomness define the compound distribution $\mathcal{T}^{π}Z$:

특히, 세 가지 무작위성 소스가 복합 분포 $\mathcal{T}^{π}Z$를 정의합니다:

We now set out to understand the distributional operators of the control setting – where we seek a policy $π$ that maximizes value –and the corresponding notion of an optimal value distribution.

이제 우리는 가치를 극대화하는 정책 $π$를 추구하는 제어 설정의 분포 연산자와 최적 가치 분포에 해당하는 개념을 이해하려고 합니다.

As with the optimal value function, this notion is intimately tied to that of an optimal policy.

최적 가치 함수와 마찬가지로, 이 개념은 최적 정책과 밀접하게 연결되어 있습니다.

In this section we show that the distributional analogue of the Bellman optimality operator converges, in a weak sense, to the set of optimal value distributions.

이 섹션에서는 벨만 최적성 연산자의 분포적 유사체가 약한 의미에서 최적 가치 분포 집합으로 수렴한다는 것을 보여줍니다.

However, this operator is not a contraction in any metric between distributions, and is in general much more temperamental than the policy evaluation operators.

그러나 이 연산자는 분포 간의 어떤 척도에서도 수축이 아니며, 일반적으로 정책 평가 연산자들보다 훨씬 더 변덕스럽습니다.

We believe the convergence issues we outline here are a symptom of the inherent instability of greedy updates, as highlighted by e.g. Tsitsiklis (2002) and most recently Harutyunyan et al. (2016).

여기서 개요를 제시하는 수렴 문제들은 Tsitsiklis (2002)와 가장 최근에 Harutyunyan et al. (2016)이 강조한 바와 같이, 탐욕스러운 업데이트의 내재적 불안정성의 증상이라고 믿습니다.

Although this policy is implicit in (6), we cannot ignore it in the distributional setting.

이 정책이 (6)에서는 암시되어 있지만, 분포적 설정에서는 무시할 수 없습니다.

By inspecting Lemma 4.

Lemma 4를 검토함으로써.

In fact, the best we can hope for is pointwise convergence, not even to the set $\mathcal{Z}^{∗}$ but to the larger set of nonstationary optimal value distributions.

사실, 우리가 바랄 수 있는 최선은 $\mathcal{Z}^{∗}$ 집합이 아니라 더 큰 비정상적(nonstationary) 최적 가치 분포 집합에 대한 점별 수렴입니다.

Statistical learning refers to a set of tools for making sense of complex datasets.

통계적 학습은 복잡한 데이터셋을 이해하기 위한 일련의 도구들을 의미합니다.

In recent years, we have seen a staggering increase in the scale and scope of data collection across virtually all areas of science and industry.

최근 몇 년 동안, 과학과 산업의 거의 모든 분야에서 데이터 수집의 규모와 범위가 엄청나게 증가했다는 것을 목격하였습니다.

Since it was published in 2013, ISLR has become a mainstay of undergraduate and graduate classrooms worldwide, as well as an important reference book for data scientists.

2013년에 출판된 이후, ISLR은 전 세계 대학교의 학부 및 대학원 교실에서 중요한 교재로 자리잡았으며, 데이터 과학자들에게 중요한 참고서로도 사용되고 있습니다.

The intention behind ISLP (and ISLR) is to concentrate more on the applications of the methods and less on the mathematical details, so it is appropriate for advanced undergraduates or master’s students in statistics or related quantitative fields, or for individuals in other disciplines who wish to use statistical learning tools to analyze their data.

ISLP(와 ISLR)의 목적은 방법론의 적용에 더 초점을 맞추고 수학적 세부 사항에는 덜 집중하는 것이므로, 통계학이나 관련 정량적 분야의 고급 학부생이나 석사 과정 학생들, 또는 자신들의 데이터를 분석하기 위해 통계적 학습 도구를 사용하고자 하는 다른 분야의 개인에게 적합합니다.

It has been an honor and a privilege for us to see the considerable impact that ISLR has had on the way in which statistical learning is practiced, both in and out of the academic setting.

ISLR이 학계 안팎에서 통계적 학습이 실행되는 방식에 미친 상당한 영향을 목격하는 것은 우리에게 영광이자 특권이었습니다.

Statistical learning refers to a vast set of tools for understanding data.

통계적 학습은 데이터를 이해하기 위한 광범위한 도구들을 의미합니다.

However, it is also clear from Figure 1.1 that there is a significant amount of variability associated with this average value, and so age alone is unlikely to provide an accurate prediction of a particular man’s wage.

그러나 그림 1.1에서 분명히 보이듯이, 이 평균 값에는 상당한 변동성이 관련되어 있으며, 따라서 나이만으로는 특정 남성의 임금을 정확히 예측하기 어려울 것입니다.

Wages increase by approximately $10,000, in a roughly linear (or straight-line) fashion, between 2003 and 2009, though this rise is very slight relative to the variability in the data.

2003년부터 2009년 사이에 임금은 대략 $10,000 증가하며, 이는 대략적으로 선형적(또는 직선적)인 방식으로 이루어집니다. 그러나 이 증가는 데이터의 변동성에 비해 매우 미미합니다.

Boxplots of the previous day’s percentage change in the S&P index for the days for which the market increased or decreased, obtained from the Smarket data.

Smarket 데이터에서 얻은, 시장이 상승하거나 하락한 날들에 대한 S&P 지수의 전날 퍼센트 변화에 대한 박스플롯입니다.

A model that could accurately predict the direction in which the market will move would be very useful!

시장이 움직일 방향을 정확하게 예측할 수 있는 모델은 매우 유용할 것입니다!

The left-hand panel of Figure 1.2 displays two boxplots of the previous day’s percentage changes in the stock index: one for the 648 days for which the market increased on the subsequent day, and one for the 602 days for which the market decreased.

그림 1.2의 왼쪽 패널은 주식 지수의 전날 퍼센트 변화에 대한 두 개의 박스플롯을 보여줍니다: 하나는 시장이 다음 날 상승한 648일에 대한 것이고, 다른 하나는 시장이 하락한 602일에 대한 것입니다.

Of course, this lack of pattern is to be expected: in the presence of strong correlations between successive days’ returns, one could adopt a simple trading strategy to generate profits from the market.

물론, 이러한 패턴의 부재는 예상되는 바입니다: 연속적인 날들의 수익률 사이에 강한 상관관계가 있을 경우, 시장에서 수익을 창출하기 위해 간단한 거래 전략을 채택할 수 있습니다.

Interestingly, there are hints of some weak trends in the data that suggest that, at least for this 5-year period, it is possible to correctly predict the direction of movement in the market approximately 60% of the time (Figure 1.3).

흥미롭게도, 데이터에는 약한 추세의 단서가 있어, 적어도 이 5년 기간 동안 시장의 움직임 방향을 약 60%의 정확도로 예측할 수 있다는 것을 시사합니다 (그림 1.3).

For example, in a marketing setting, we might have demographic information for a number of current or potential customers.

예를 들어, 마케팅 상황에서, 우리는 현재 또는 잠재 고객들에 대한 인구통계적 정보를 가지고 있을 수 있습니다.

We fit a quadratic discriminant analysis model to the subset of the Smarket data corresponding to the 2001–2004 time period, and predicted the probability of a stock market decrease using the 2005 data.

Quadratic discriminant analysis 모델을 2001년부터 2004년까지의 기간에 해당하는 Smarket 데이터의 부분집합에 적용하고, 2005년 데이터를 사용하여 주식 시장 하락의 확률을 예측했습니다.

On average, the predicted probability of decrease is higher for the days in which the market does decrease.

평균적으로, 시장이 하락하는 날에는 하락할 확률이 더 높게 예측됩니다.

We devote Chapter 12 to a discussion of statistical learning methods for problems in which no natural output variable is available.

우리는 자연스러운 출력 변수가 없는 문제에 대한 통계적 학습 방법에 대한 논의를 12장에 할애합니다.

There is clear evidence that cell lines with the same cancer type tend to be located near each other in this two-dimensional representation.

이러한 이차원 표현에서 같은 암 유형을 가진 세포주들이 서로 가까이 위치하는 경향이 있다는 명확한 증거가 있습니다.

In addition, even though the cancer information was not used to produce the left-hand panel, the clustering obtained does bear some resemblance to some of the actual cancer types observed in the right-hand panel.

또한, 왼쪽 패널을 생성하는 데 암 정보가 사용되지 않았음에도 불구하고, 얻어진 군집은 실제로 오른쪽 패널에서 관찰된 일부 암 유형과 어느 정도 유사성을 보입니다.

This provides some independent verification of the accuracy of our clustering analysis.

이것은 우리의 군집 분석의 정확성에 대한 독립적인 검증을 제공합니다.

At the beginning of the nineteenth century, the method of least squares was developed, implementing the earliest form of what is now known as linear regression.

19세기 초, least squares라는 방법이 개발되었으며, 이는 지금으로서는 linear regression으로 알려진 가장 초기의 형태를 구현했습니다.

In the 1940s, various authors put forth an alternative approach, logistic regression.

1940년대에 여러 저자들이 대안적인 접근 방식인 logistic regression을 제시했습니다.

However, they were almost exclusively linear methods because fitting non-linear relationships was computationally difficult at the time.

그러나 당시에는 non-linear 관계를 맞추는 것이 계산적으로 어려웠기 때문에 거의 대부분 linear 방법들이었습니다.

This has the potential to continue the transformation of the field from a set of techniques used and developed by statisticians and computer scientists to an essential toolkit for a much broader community.

이것은 통계학자들과 컴퓨터 과학자들이 사용하고 개발한 일련의 기술에서 훨씬 더 넓은 커뮤니티에 필수적인 도구 모음으로 분야를 계속 변화시킬 잠재력이 있습니다.

Its success derives from its comprehensive and detailed treatment of many important topics in statistical learning, as well as the fact that (relative to many upper-level statistics textbooks) it is accessible to a wide audience.

이 책의 성공은 통계 학습에서 중요한 많은 주제들을 종합적이고 상세하게 다루었다는 점과, 많은 상급 수준의 통계 교재들과 비교할 때 폭넓은 독자층에게 접근하기 쉽다는 사실에서 비롯됩니다.

The most obvious growth has involved the development of new and improved statistical learning approaches aimed at answering a range of scientific questions across a number of fields.

가장 명백한 발전은 다양한 분야에서 과학적 질문에 답하기 위해 새롭고 개선된 통계 학습 방법론의 개발에 관련되어 있습니다.

In the 1990s, increases in computational power generated a surge of interest in the field from non-statisticians who were eager to use cutting-edge statistical tools to analyze their data.

1990년대에는 컴퓨터 연산력의 증가가 비통계학자들 사이에서 관심을 불러일으켰고, 이들은 최신 통계 도구를 사용하여 데이터를 분석하는 데 열심이었습니다.

Unfortunately, the highly technical nature of these approaches meant that the user community remained primarily restricted to experts in statistics, computer science, and related fields with the training (and time) to understand and implement them.

유감스럽게도 이러한 접근법의 고도로 기술적인 성격 때문에 사용자 커뮤니티는 주로 통계학, 컴퓨터 과학, 관련 분야의 전문가들로 제한되었으며, 이들은 이를 이해하고 구현하기 위한 교육(및 시간)을 갖추고 있었습니다.

In recent years, new and improved software packages have significantly eased the implementation burden for many statistical learning methods.

최근 몇 년 동안 새롭고 개선된 소프트웨어 패키지들이 많은 통계 학습 방법들의 구현 부담을 크게 줄여주었습니다.

ISL is not intended to replace ESL, which is a far more comprehensive text both in terms of the number of approaches considered and the depth to which they are explored.

ISL은 ESL을 대체하기 위한 것이 아니며, ESL은 고려된 접근 방법의 수와 그들이 탐구된 깊이 면에서 훨씬 더 포괄적인 텍스트입니다.

In teaching these topics over the years, we have discovered that they are of interest to master’s and PhD students in fields as disparate as business administration, biology, and computer science, as well as to quantitatively-oriented upper-division undergraduates.

여러 해에 걸쳐 이러한 주제들을 가르치면서, 우리는 이들이 경영학, 생물학, 컴퓨터 과학과 같이 다양한 분야의 석사 및 박사 과정 학생들과, 정량적으로 지향하는 상급 학부생들에게도 관심을 끌고 있다는 것을 발견했습니다.

We believe that these students do not need a deep understanding of these aspects in order to become informed users of the various methodologies, and in order to contribute to their chosen fields through the use of statistical learning tools.

우리는 이러한 학생들이 다양한 방법론의 정보에 밝은 사용자가 되고, 통계 학습 도구를 사용하여 자신이 선택한 분야에 기여하기 위해 이러한 측면들에 대한 깊은 이해가 필요하지 않다고 믿습니다.

ISL is based on the following four premises.

ISL은 다음의 네 가지 전제에 기반을 두고 있습니다.

Many statistical learning methods are relevant and useful in a wide range of academic and non-academic disciplines, beyond just the statistical sciences.

많은 통계 학습 방법들은 통계 과학뿐만 아니라 광범위한 학문적 및 비학문적 분야에서도 관련성이 있고 유용합니다.

We believe that many contemporary statistical learning procedures should, and will, become as widely available and used as is currently the case for classical methods such as linear regression.

우리는 많은 현대 통계 학습 절차들이 선형 회귀와 같은 고전적 방법들처럼 현재와 같이 널리 이용 가능하고 사용될 것이라고 믿습니다.

As a result, rather than attempting to consider every possible approach (an impossible task), we have concentrated on presenting the methods that we believe are most widely applicable.

그 결과, 모든 가능한 접근 방법을 고려하는 것(불가능한 일)보다는, 우리가 가장 널리 적용 가능하다고 믿는 방법들을 제시하는 데 집중하였습니다.

This is a good example of an instance where having a conceptual viewpoint saves you a lot of work.

이는 개념적 관점이 있으면 많은 작업을 절약할 수 있는 좋은 예입니다.

In fact, AB may be defined when BA is not.

실제로 AB는 BA가 정의되지 않은 경우에 정의될 수 있습니다.

You have to be careful when multiplying matrices together, because things like commutativity and cancellation fail.

행렬을 곱할 때 주의해야 하는데, 교환법칙과 소거법칙과 같은 것들이 실패할 수 있습니다.

We begin by characterizing what we mean by an optimal value distribution.

최적의 가치 분포가 무엇을 의미하는지 특성화하는 것부터 시작합니다.

Without understanding all of the cogs inside the box, or the interaction between those cogs, it is impossible to select the best box.

상자 안의 모든 톱니바퀴들이나 그 톱니바퀴들 간의 상호작용을 이해하지 않고서는 최적의 상자를 선택하는 것이 불가능합니다.

We presume that the reader is interested in applying statistical learning methods to real-world problems.

우리는 독자가 실제 세계의 문제에 통계 학습 방법을 적용하는 데 관심이 있다고 가정합니다.

Many of the less computationally-oriented students who were initially intimidated by the labs got the hang of things over the course of the quarter or semester.

계산에 중점을 두지 않은 많은 학생들이 처음에는 lab에 겁을 먹었지만, 학기나 학기 동안 점차 일에 익숙해졌습니다.

However, the labs in ISL are self-contained, and can be skipped if the reader wishes to use a different software package or does not wish to apply the methods discussed to real-world problems.

그러나 ISL의 lab은 독립적으로 구성되어 있어, 독자가 다른 소프트웨어 패키지를 사용하길 원하거나 실제 세계의 문제에 논의된 방법들을 적용하고 싶지 않은 경우 건너뛸 수 있습니다.

This group includes scientists, engineers, data analysts, data scientists, and quants, but also less technical individuals with degrees in non-quantitative fields such as the social sciences or business.

이 그룹에는 과학자, 엔지니어, 데이터 분석가, 데이터 과학자, 퀀트뿐만 아니라 사회 과학이나 비즈니스와 같은 비정량적 분야의 학위를 가진 덜 기술적인 개인들도 포함됩니다.

In the rare cases in which these two uses for lower case normal font lead to ambiguity, we will clarify which use is intended.

이러한 두 가지 용도가 소문자 일반 폰트 사용에서 드물게 모호함을 초래하는 경우에는, 우리는 어떤 용도가 의도된 것인지 명확히 할 것입니다.

However, in a few instances it becomes too cumbersome to avoid it entirely.

그러나 몇몇 경우에는 완전히 피하는 것이 너무 번거로워집니다.

We then show how these methods can be used to fit non-linear additive models for which there is more than one input.

그런 다음 이러한 방법들을 하나 이상의 입력이 있는 비선형 가산 모델에 적용하는 방법을 보여줍니다.

These can be easily skipped by readers who do not wish to delve as deeply into the material, or who lack the mathematical background.

이들은 자료에 깊이 파고들고 싶지 않거나 수학적 배경이 부족한 독자들에 의해 쉽게 건너뛰어질 수 있습니다.

How on earth do you remember this?

이걸 어떻게 기억하나요?

This is a crucial component of the change-of-variables formula in multivariable calculus.

이것은 다변수 미적분에서 변수 변환 공식의 중요한 구성 요소입니다.

This is even true for curvy shapes, in the following sense.

이것은 다음과 같은 의미에서 곡선 모양에도 마찬가지로 적용됩니다.

Let S be the unit disk in $R^2$,

S를 $R^2$의 단위 원으로 두고,

JAX is laser-focused on program transformations and accelerator-backed NumPy, so we don’t include data loading or munging in the JAX library.

JAX는 프로그램 변환과 가속기 기반의 NumPy에 집중하고 있어, JAX 라이브러리에 데이터 로딩이나 가공을 포함하지 않습니다.

In other words, our goal is to develop an accurate model that can be used to predict sales on the basis of the three media budgets.

다시 말해, 우리의 목표는 세 가지 미디어 예산을 기반으로 판매를 예측할 수 있는 정확한 모델을 개발하는 것입니다.

More generally, suppose that we observe a quantitative response $Y$ and $p$ different predictors, $X_1, X_2,...,X_p$.

보다 일반적으로, 우리가 정량적인 반응 $Y$와 $p$개의 다른 예측 변수들, $X_1, X_2,...,X_p$를 관찰한다고 가정해 봅시다.

The plot displays sales, in thousands of units, as a function of `TV`, `radio`, and `newspaper` budgets, in thousands of dollars, for 200 different markets.

이 그래프는 200개의 다른 시장에 대해, 수천 달러 단위의 `TV`, `radio`, `newspaper` 예산을 function으로 하는 수천 단위의 판매량을 보여줍니다.

In this setting, $\hat{f}$ is often treated as a *black box*, in the sense that one is not typically concerned with the exact form of $\hat{f}$, provided that it yields accurate predictions for $Y$.

이 상황에서, $\hat{f}$는 종종 *black box* 로 취급되며, 그것이 $Y$에 대한 정확한 예측을 제공한다면 $\hat{f}$의 정확한 형태에 대해 일반적으로 걱정하지 않는다는 의미에서 그렇습니다.

It is often the case that only a small fraction of the available predictors are substantially associated with $Y$. Identifying the few important predictors among a large set of possible variables can be extremely useful, depending on the application.

주로 사용 가능한 예측 변수들 중 극히 일부만이 $Y$와 실질적으로 연관되어 있습니다. 많은 가능한 변수들 중 몇 가지 중요한 예측 변수를 식별하는 것은 응용에 따라 매우 유용할 수 있습니다.

For instance, to what extent is the product’s price associated with sales?

예를 들어, 제품의 가격이 판매와 어느 정도 연관되어 있나요?

For example, in a real estate setting, one may seek to relate values of homes to inputs such as crime rate, zoning, distance from a river, air quality, schools, income level of community, size of houses, and so forth.

예를 들어 부동산 상황에서, 주택 가치를 범죄율, 구역 지정, 강으로부터의 거리, 공기 질, 학교, 지역사회의 소득 수준, 주택 크기 등과 같은 입력 값과 연관시키려 할 수 있습니다.

This enables planning to be interrupted or redirected at any time with little wasted computation, which appears to be a key requirement for efficiently intermixing planning with acting and with learning of the model.

이렇게 하면 낭비되는 계산량이 거의 없이 언제든 계획을 수행하는 중간에 끼어들거나 진행 방향을 바꿀 수 있다. 이것은 계획을 행동 및 모델에 대한 학습과 효과적으로 혼합하기 위한 핵심 요구조건인 것처럼 보인다.

Because planning proceeds incrementally, it is trivial to intermix planning and acting.

계획이 점증적으로 진행되기 때문에, 계획과 행동을 혼합하는 것은 식은 죽 먹기다.

This tends to happen when the model is optimistic in the sense of predicting greater reward or better state transitions than are actually possible.

실제로 가능한 것보다 더 큰 보상 또는 더 좋은 상태 전이를 예측한다는 측면에서 모델이 긍정적일 경우 이러한 상황이 발생하는 경향이 있다.

This encourages the agent to keep testing all accessible state transitions and even to find long sequences of actions in order to carry out such tests.

이것은 학습자로 하여금 접근 가능한 모든 상태 전이에 대해 계속 테스트하고, 심지어는 그러한 테스트를 수행하기 위해 행동의 긴 나열을 찾도록 장려한다.

As planning progresses, the region of useful updates grows, but planning is still far less efficient than it would be if focused where it would do the most good.

계획이 진행됨에 따라 유용한 갱신의 영역은 커지지만, 계획 자체는 계획이 가장 잘 수행되는 곳에 초점을 맞추었을 때보다 여전히 훨씬 더 비효율적이다.

In the much larger problems that are our real objective, the number of states is so large that an unfocused search would be extremely inefficient.

이 책의 진정한 목표인 훨씬 더 규모가 큰 문제에서는 상태의 개수가 상당히 많아서 초점 없는 탐색이 극도로 비효율적일 것이다.

In this way one can work backward from arbitrary states that have changed in value, either performing useful updates or terminating the propagation. This general idea might be termed *backward focusing* of *planning computations*.

이러한 방식으로 가치가 변한 임의의 상태로부터 역방향으로 진행할 수 있다. 이 과정에서 유용한 갱신을 할 수도 있고 진행을 멈출 수도 있다. 이러한 일반적인 개념을 이름하여 계획 계산(planning computation)의 역행 초점(backward focusing)이라고 부를 수도 있다.

Sample updates can win because they break the overall backing-up computation into smaller pieces—those corresponding to individual transitions—which then enables it to be focused more narrowly on the pieces that will have the largest impact. This idea was taken to what may be its logical limit in the “small backups” introduced by van Seijen and Sutton (2013).

표본 갱신이 전체 보강 계산을 개별 전이 각각에 해당하는 더 작은 조각으로 나눔으로써 가장 큰 영향력이 있는 조각들에 더 좁게 초점을 맞출 수 있도록 하기 때문에 표본 갱신의 성능이 더 좋을 수 있다. 이 개념은 반 세이젠과 서튼(2013)이 소개한 '작은 보강'에서 그 개념의 논리적 한계일 수도 있는 것에 적용되었다.

For example, another would be to focus on states according to how easily they can be reached from the states that are visited frequently under the current policy, which might be called *forward focusing*.

예를 들어, 또 다른 전략은 현재 정책하에서 자주 마주치는 상태로부터 얼마나 쉽게 그 상태에 도달할 수 있는지에 따라 상태에 초점을 두는 것이다. 이러한 전략을 순행(forward focusing)이라고 부를 수도 있다.

You can notice the first difference if you check the type of `x`. It is a variable of type `Array`, which is the way JAX represents arrays.

x의 타입을 확인하면 첫 번째 차이점을 알 수 있습니다. `x`는 `Array` 타입의 변수로, 이는 JAX가 배열을 나타내는 방식입니다.

The returned array is therefore not necessarily ‘filled in’ as soon as the function returns. Thus, if we don’t require the result immediately, the computation won’t block Python execution.

따라서 반환된 배열은 함수가 반환되자마자 반드시 '채워진' 상태는 아닙니다. 그러므로 결과가 즉시 필요하지 않다면, 계산이 Python 실행을 차단하지 않을 것입니다.

In addition to wanting to log the value, we often want to report some intermediate results obtained in computing the loss function.

값을 기록하고 싶은 것 외에도, 우리는 종종 손실 함수를 계산하는 과정에서 얻은 중간 결과를 보고하고 싶어 합니다.

The most important difference, and in some sense the root of all the rest, is that JAX is designed to be functional, as in functional programming.

가장 중요한 차이점이자, 어떤 의미에서 모든 다른 차이의 근원은 JAX가 함수형 프로그래밍과 같이 함수형으로 설계되었다는 것입니다.

The reason behind this is that the kinds of program transformations that JAX enables are much more feasible in functional-style programs.

이것의 이유는 JAX가 가능하게 하는 프로그램 변환의 종류가 함수형 스타일의 프로그램에서 훨씬 더 실현 가능하기 때문입니다.

However, as we will explain in the next guide, JAX computations are often compiled before being run using another program transformation, `jax.jit`.

하지만, 다음 가이드에서 설명하겠지만, JAX 계산은 종종 실행되기 전에 다른 프로그램 변환인 `jax.jit`를 사용하여 컴파일됩니다.

If we don’t use the old array after modifying it ‘in place’ using indexed update operators, the compiler can recognise that it can in fact compile to an in-place modify, resulting in efficient code in the end.

인덱스 업데이트 연산자를 사용하여 '현장에서' 수정한 후 이전 배열을 사용하지 않으면, 컴파일러는 실제로 현장에서의 수정으로 컴파일할 수 있다는 것을 인식하여, 결국 효율적인 코드를 생성할 수 있습니다.

We will explain other places where the JAX idiosyncrasies become relevant as they come up.

JAX의 특이한 점이 등장하는 대로 관련성이 있는 다른 곳에 대해 설명해 드리겠습니다.

The main difference between this example and real training loops is the simplicity of our model: that allows us to use a single array to house all our parameters.

이 예시와 실제 훈련 루프 사이의 주된 차이점은 우리 모델의 단순성입니다: 이것은 모든 매개변수를 하나의 배열에 담을 수 있게 해줍니다.

The (algebraic) multiplicity of an eigenvalue λ is its multiplicity as a root of the characteristic polynomial.

고유값 λ의 (대수적) 중복도는 특성 다항식의 근으로서의 중복도입니다.

It's easy to factor quadratic polynomials.

이차 다항식을 인수분해하는 것은 쉽습니다.

First we talked about the geometry of eigenvalues and eigenvectors.

먼저 고유값과 고유벡터의 기하학에 대해 이야기했습니다.

Instead they seek an estimate of $f$ that gets as close to the data points as possible without being too rough or wiggly.

대신에, 너무 거칠거나 흔들리지 않으면서 가능한 한 데이터 포인트에 가까운 $f$의 추정치를 찾으려 합니다.

This approach does not impose any pre-specified model on $f$.

이 접근법은 $f$에 사전에 지정된 어떠한 모델도 강제하지 않습니다.

In order to fit a thin-plate spline, the data analyst must select a level of smoothness.

thin-plate spline을 적용하기 위해, 데이터 분석가는 부드러움의 수준을 선택해야 합니다.

Of the many methods that we examine in this book, some are less flexible, or more restrictive, in the sense that they can produce just a relatively small range of shapes to estimate f.

이 책에서 살펴보는 많은 방법들 중 일부는 f를 추정하기 위해 상대적으로 작은 범위의 형태만을 생성할 수 있다는 점에서 덜 유연하거나 더 제한적입니다.

It is also more interpretable than linear regression, because in the final model the response variable will only be related to a small subset of the predictors—namely, those with nonzero coefficient estimates.

또한 선형 회귀보다 해석하기 쉬운데, 이는 최종 모델에서 반응 변수가 예측 변수의 작은 부분집합, 즉 0이 아닌 계수 추정치를 가진 것들과만 관련되기 때문입니다.

Surprisingly, this is not always the case!

놀랍게도, 이것이 항상 그런 것은 아닙니다!

This phenomenon, which may seem counterintuitive at first glance, has to do with the potential for overfitting in highly flexible methods.

처음 보기에는 직관에 반하는 것처럼 보일 수 있는 이 현상은 매우 유연한 방법에서 과적합(overfitting)의 가능성과 관련이 있습니다.

In this example, this wouldn’t actually help speed anyway, for many reasons, but treat this as a toy model of wanting to JIT-compile the update of model parameters, where `jax.jit` makes an enormous difference.

이 예시에서는 여러 이유로 실제로 속도를 향상시키지는 못하지만, 이것을 모델 매개변수의 업데이트를 JIT 컴파일하고자 하는 장난감 모델로 취급하세요, 여기서 `jax.jit` 는 엄청난 차이를 만듭니다.

This won’t do.

이렇게 하면 안 됩니다.

Part of the problem with our counter was that the returned value didn’t depend on the arguments, meaning a constant was “baked into” the compiled output.

우리 카운터의 문제점 중 일부는 반환된 값이 인수에 의존하지 않아 컴파일된 출력에 상수가 "내장되었다"는 것이었습니다.

The most important difference, and in some sense the root of all the rest, is that JAX is designed to be functional, as in functional programming.

가장 중요한 차이점이자, 어떤 의미에서 모든 다른 차이의 근원은 JAX가 함수형 프로그래밍처럼 함수형으로 설계되었다는 것입니다.

This is because any such side-effects will only be executed once, when the python version of the function is run during compilation.

이는 그러한 side-effect가 컴파일 중에 파이썬 버전의 함수가 실행될 때 단 한 번만 실행되기 때문입니다.

Notice that the need for a class becomes less clear once we have rewritten it this way. We could just keep `stateless_method`, since the class is no longer doing any work. This is because, like the strategy we just applied, object-oriented programming (OOP) is a way to help programmers understand program state.

이렇게 다시 작성하면 클래스의 필요성이 덜 명확해집니다. 클래스가 더 이상 작업을 수행하지 않기 때문에 `stateless_method`만 유지할 수 있습니다. 이는 우리가 방금 적용한 전략과 마찬가지로, 객체 지향 프로그래밍(OOP)이 프로그래머가 프로그램 상태를 이해하는 데 도움을 주는 방법이기 때문입니다.

Here, we only deal with one kind of state: the model parameters. But generally, you’ll see many kinds of state being threaded in and out of JAX functions, like optimizer state, layer statistics for batchnorm, and others.

여기서는 모델 매개변수라는 한 종류의 상태만 다룹니다. 하지만 일반적으로 JAX functions, optimizer state, batchnorm을 위한 layer statistics 등과 같은 여러 종류의 상태가 JAX 함수에 들어가고 나오는 것을 볼 수 있습니다.

Are we supposed to initialize them all manually, essentially repeating what we already write in the forward pass definition?

우리는 기본적으로 순방향 패스 정의에서 이미 작성한 것을 반복하면서 모두 수동으로 초기화해야 하는 것인가요?

Greater difficulties arise when the environment changes to become better than it was before, and yet the formerly correct policy does not reveal the improvement. In these cases the modeling error may not be detected for a long time, if ever.

환경이 이전보다 개선되기 위해 바뀌었지만 이전에 올바른 정책이 개선된 점을 드러내지 않을 때 더 큰 어려움이 발생합니다. 이러한 경우 모델링 오류가 오랫동안 감지되지 않을 수 있습니다.

In a stochastic environment, variations in estimated transition probabilities also contribute to variations in the sizes of changes and in the urgency with which pairs need to be updated.

확률적 환경에서 추정된 전이 확률의 변화는 변화의 크기와 쌍을 업데이트해야 하는 긴급도의 변화에도 기여합니다.

The important feature of functional programming to grok when working with JAX is very simple: don’t write code with side-effects.

JAX를 사용할 때 이해해야 할 함수형 프로그래밍의 중요한 특징은 매우 간단합니다: side-effects가 있는 코드를 작성하지 마세요.

In this example, this wouldn’t actually help speed anyway, for many reasons, but treat this as a toy model of wanting to JIT-compile the update of model parameters, where `jax.jit` makes an enormous difference.

이 예시에서는 여러 이유로 실제로 속도를 향상시키지는 못하지만, 이것을 모델 매개변수의 업데이트를 JIT 컴파일하고자 하는 장난감 모델로 취급하세요, 여기서 `jax.jit`는 엄청난 차이를 만듭니다.

Implicit in that point of view is that expected updates, if possible, are preferable to sample updates.

이러한 관점에 내재된 생각은, 가능하기만 하다면 기댓값 갱신이 표본 갱신보다 더 선호할 만하다는 것이다.

The difference between these expected and sample updates is significant to the extent that the environment is stochastic, specifically, to the extent that, given a state and action, many possible next states may occur with various probabilities.

환경이 확률론적인 경우에 한해, 좀 더 분명히 말하면 상태와 행동이 주어졌을 때 가능성 있는 많은 다음 상태들이 다양한 확률로 발생할 수 있는 경우에 한해, 이러한 기댓값 갱신과 표본 갱신의 차이점은 중요한 의미를 갖는다.

Given a unit of computational effort, is it better devoted to a few expected updates or to b times as many sample updates?

한 단위의 계산량이 몇 개의 기댓값 갱신에 사용되는 것이 좋은가? 아니면 개수가 b배인 표본 갱신에 사용되는 것이 좋은가?

For these cases, many state–action pairs could have their values improved dramatically, to within a few percent of the effect of an expected update, in the same time that a single state–action pair could undergo an expected update.

이 경우에 많은 수의 상태-행동 쌍들은 기댓값 갱신이 가치를 향상시키는 효과의 몇 퍼센트 이내로 그들의 가치를 극적으로 향상시킬 수 있었을 것이다. 하지만 이때 소요되는 시간은 고작 하나의 상태-행동 쌍이 기댓값 갱신을 수행하는 시간과 같은 시간이다.

We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic.

$$y=r + \gamma Q_{\theta'}(s', \pi_{\phi}(s')). \tag{8}$$

이 문제가 액터-크리틱 설정에서도 지속되는 것을 보여주고, 액터와 크리틱 양쪽 모두에 미치는 영향을 최소화하기 위한 새로운 메커니즘을 제안합니다.

$$y=r + \gamma Q_{\theta'}(s', \pi_{\phi}(s')). \tag{8}$$

In reinforcement learning problems with discrete action spaces, the issue of value overestimation as a result of function approximation errors is well-studied.

이산 동작 공간을 갖는 강화학습 문제에서, 함수 근사 오류로 인한 가치 과대평가 문제는 잘 연구되어 있습니다.

Overestimation bias is a property of Q-learning in which the maximization of a noisy value estimate induces a consistent overestimation.

과대평가 편향은 Q-러닝의 특성으로, 잡음이 섞인 가치 추정치의 최대화가 일관된 과대평가를 유발한다는 것입니다.

This inaccuracy is further exaggerated by the nature of temporal difference learning (Sutton, 1988), in which an estimate of the value function is updated using the estimate of a subsequent state.

이러한 부정확함은 시간차 학습(Sutton, 1988)의 특성에 의해 더욱 과장됩니다. 여기서 가치 함수의 추정치는 후속 상태의 추정치를 사용하여 업데이트됩니다.

During training, Double DQN estimates the value of the current policy with a separate target value function, allowing actions to be evaluated without maximization bias.

트레이닝 중에, 더블 DQN은 별도의 목표 가치 함수를 사용하여 현재 정책의 가치를 추정하며, 이를 통해 행동을 최대화 편향 없이 평가할 수 있습니다.

This favors underestimations, which do not tend to be propagated during learning, as actions with low value estimates are avoided by the policy.

이는 학습 동안 전파되지 않는 경향이 있는 저평가를 선호합니다. 왜냐하면 낮은 가치 추정치를 가진 행동들은 정책에 의해 피해지기 때문입니다.

Given the connection of noise to overestimation bias, this paper contains a number of components that address variance reduction.

잡음과 과대평가 편향 사이의 연관성을 고려할 때, 이 논문은 분산 감소를 다루는 여러 구성 요소를 포함하고 있습니다.

Second, to address the coupling of value and policy, we propose delaying policy updates until the value estimate has converged. Finally, we introduce a novel regularization strategy, where a SARSA-style update bootstraps similar action estimates to further reduce variance.

둘째, 가치와 정책의 결합 문제를 해결하기 위해, 가치 추정치가 수렴할 때까지 정책 업데이트를 지연시키는 것을 제안합니다. 마지막으로, SARSA 스타일 업데이트가 유사한 행동 추정치를 부트스트랩하여 분산을 더욱 줄이는 새로운 정규화 전략을 소개합니다.

an actor-critic algorithm which considers the interplay between function approximation error in both policy and value updates.

정책과 가치 업데이트에서 함수 근사 오류 간의 상호작용을 고려하는 액터-크리틱 알고리즘입니다.

Our work focuses on two outcomes that occur as the result of estimation error, namely overestimation bias and a high variance build-up.

우리의 연구는 추정 오류의 결과로 발생하는 두 가지 결과, 즉 과대평가 편향과 높은 분산 축적에 초점을 맞춥니다.

Other approaches have focused directly on reducing the variance (Anschel et al., 2017), minimizing over-fitting to early high variance estimates (Fox et al., 2016), or through corrective terms (Lee et al., 2013).

다른 접근법들은 분산을 직접 줄이는 것(Anschel et al., 2017), 초기 높은 분산 추정치에 대한 과적합 최소화(Fox et al., 2016), 또는 corrective terms를 통한 방법(Lee et al., 2013)에 중점을 두었습니다.

Further, the variance of the value estimate has been considered directly for risk-aversion (Mannor & Tsitsiklis, 2011) and exploration (O’Donoghue et al., 2017), but without connection to overestimation bias.

더 나아가, 가치 추정의 분산은 위험 회피(Mannor & Tsitsiklis, 2011) 및 탐험(O’Donoghue et al., 2017)을 위해 직접 고려되었지만, 과대평가 편향과는 연관이 없었습니다.

However, rather than provide a direct solution to the accumulation of error, these methods circumvent the problem by considering a longer horizon.

그러나 이러한 방법들은 오류의 축적에 대한 직접적인 해결책을 제공하기보다는, 더 긴 시계열을 고려함으로써 문제를 회피합니다.

however, if the target is susceptible to error ε, then the maximum over the value along with its error will generally be greater than the true maximum.

그러나, 만약 목표가 오류 ε에 취약하다면, 그 오류를 포함한 값의 최대치는 일반적으로 실제 최대치보다 클 것입니다.

As a result, even initially zero-mean errors can cause value updates to result in a consistent overestimation bias, which is then propagated through the Bellman equation.

결과적으로, 초기에는 평균이 제로인 오류조차도 값 업데이트로 인해 일관된 과대평가 편향을 유발할 수 있으며, 이 편향은 그 후 벨만 방정식을 통해 전파됩니다.

Without normalized gradients, overestimation bias is still guaranteed to occur with slightly stricter conditions.

정규화된 그래디언트 없이도, 약간 더 엄격한 조건으로 과대평가 편향이 여전히 발생하는 것은 보장됩니다.

If in expectation the value estimate is at least as large as the true value with respect to $\phi_{\text{true}}, \mathbb{E}\left[ Q_{\theta}(s, \pi_{\text{true}}(s)) \right] \geq \mathbb{E} \left[ Q^{\pi}(s, \pi_{\text{true}}(s))\right]$, then Equations (5) and (6) imply that if $α < \min(ϵ_1, ϵ_2)$, then the value estimate will be overestimated:

$$\mathbb{E}\left[ Q_{\theta}(s, \pi_{\text{approx}}(s)) \right] \geq \mathbb{E} \left[ Q^{\pi}(s, \pi_{\text{approx}}(s))\right] \tag{7}$$

기대치에서 가치 추정치가 $\phi_{\text{true}}, \mathbb{E}\left[ Q_{\theta}(s, \pi_{\text{true}}(s)) \right] \geq \mathbb{E} \left[ Q^{\pi}(s, \pi_{\text{true}}(s))\right]$에 대한 실제 가치보다 적어도 그만큼 크다면, 방정식 (5)와 (6)은 $α < \min(ϵ_1, ϵ_2)$일 경우 가치 추정치가 과대평가될 것임을 시사합니다.

In Double Q-learning, the greedy update is disentangled from the value function by maintaining two separate value estimates, each of which is used to update the other.

Double Q-learning에서, Greedy 업데이트는 가치 함수로부터 분리되며, 이는 두 개의 별도 가치 추정치를 유지함으로써 이루어집니다. 각 추정치는 서로를 업데이트하는 데 사용됩니다.

In an actor-critic setting, an analogous update uses the current policy rather than the target policy in the learning target.

Actor-critic 설정에서, 유사한 업데이트는 학습 대상에서 target policy 대신 current policy를 사용합니다.

To address this problem, we propose to simply upper-bound the less biased value estimate $Q\_{\theta_{2}}$ by the biased estimate $Q\_{\theta_{1}}$.

이 문제를 해결하기 위해, 우리는 편향된 추정치 $Q\_{\theta_{1}}$에 의해 덜 편향된 가치 추정치 $Q\_{\theta_{2}}$를 단순히 상한으로 설정하는 것을 제안합니다.

This results in taking the minimum between the two estimates, to give the target update of our Clipped Double Q-learning algorithm:

$$y\_1 = r + \gamma \underset{i=1, 2}{\min}Q\_{\theta'\_{i}}(s', \pi\_{\phi\_{1}}(s')). \tag{10}$$

이는 두 추정치 사이의 최소값을 취함으로써 우리의 Clipped Double Q-learning 알고리즘의 타겟 업데이트를 결정하는 결과를 가져옵니다.

$$y\_1 = r + \gamma \underset{i=1, 2}{\min}Q\_{\theta'\_{i}}(s', \pi\_{\phi\_{1}}(s')). \tag{10}$$

With Clipped Double Q-learning, the value target cannot introduce any additional overestimation over using the standard Q-learning target.

Clipped Double Q-learning에서는 값 타겟이 표준 Q-learning 타겟을 사용할 때보다 추가적인 과대평가를 도입할 수 없습니다.

While this update rule may induce an underestimation bias, this is far preferable to overestimation bias, as unlike overestimated actions, the value of underestimated actions will not be explicitly propagated through the policy update.

이 업데이트 규칙은 과소평가 편향을 유발할 수 있지만, 이는 과대평가 편향보다 훨씬 바람직합니다. 과대평가된 행동과 달리, 과소평가된 행동의 가치는 정책 업데이트를 통해 명시적으로 전파되지 않기 때문입니다.

A secondary benefit is that by treating the function approximation error as a random variable we can see that the minimum operator should provide higher value to states with lower variance estimation error, as the expected minimum of a set of random variables decreases as the variance of the random variables increases.

부수적인 이점으로, 함수 근사 오류를 랜덤 변수로 취급함으로써, 최소 연산자가 분산 추정 오류가 낮은 상태에 더 높은 가치를 제공해야 함을 알 수 있습니다. 이는 랜덤 변수 집합의 예상 최소값이 랜덤 변수의 분산이 증가함에 따라 감소하기 때문입니다.

This is exacerbated in a function approximation setting where the Bellman equation is never exactly satisfied, and each update leaves some amount of residual TD-error $δ(s, a)$:

$$Q_{\theta}(s, a)=r + \gamma\mathbb{E}\left[Q_{\theta}(s', a')\right]-\delta(s, a). \tag{11}$$

이 문제는 함수 근사 설정에서 더욱 심화되는데, 여기서는 벨만 방정식이 정확히 만족되지 않으며, 각 업데이트가 일정량의 잔여 TD-error $δ(s, a)$를 남깁니다.

$$Q_{\theta}(s, a)=r + \gamma\mathbb{E}\left[Q_{\theta}(s', a')\right]-\delta(s, a). \tag{11}$$

Given a large discount factor γ, the variance can grow rapidly with each update if the error from each update is not tamed.

큰 할인 요인 γ가 주어진 경우, 각 업데이트에서 오류를 제어하지 않으면 분산이 각 업데이트와 함께 빠르게 증가할 수 있습니다.

Furthermore, each gradient update only reduces error with respect to a small mini-batch, which gives no guarantees about the size of errors in value estimates outside the mini-batch.

또한, 각 그래디언트 업데이트는 오직 작은 미니배치에 대한 오류만을 줄이므로, 미니배치 외부의 가치 추정치에서 오류의 크기에 대한 보장은 없습니다.

This insight allows us to consider the interplay between high variance estimates and policy performance, when designing reinforcement learning algorithms.

이 통찰은 우리가 강화 학습 알고리즘을 설계할 때, 높은 분산 추정치와 정책 성능 사이의 상호 작용을 고려할 수 있게 해줍니다.

While the accumulation of error can be detrimental in itself, when paired with a policy maximizing over the value estimate, it can result in wildly divergent values.

오류의 축적 자체가 해로울 수 있지만, 가치 추정치를 최대화하는 정책과 결합될 때, 매우 다양한 값으로 이어질 수 있습니다.

In (a) we compare the behavior with a fixed policy and in (b) we examine the value estimates with a policy that continues to learn, trained with the current value estimate.

(a)에서는 고정된 정책과의 행동을 비교하고, (b)에서는 계속 학습하는 정책과 함께 현재 가치 추정치로 훈련된 가치 추정치를 검토합니다.

While updating the value estimate without target networks ($τ = 1$) increases the volatility, all update rates result in similar convergent behaviors when considering a fixed policy.

타겟 네트워크 없이 가치 추정치를 업데이트하는 것($τ = 1$)이 변동성을 증가시키지만, 고정된 정책을 고려할 때 모든 업데이트 비율은 유사한 수렴 행동을 보입니다.

These results suggest that the divergence that occurs without target networks is the result of policy updates with a high variance value estimate.

이 결과들은 타겟 네트워크가 없을 때 발생하는 발산이 높은 분산 가치 추정치를 가진 정책 업데이트의 결과임을 시사합니다.

If target networks can be used to reduce the error over multiple updates, and policy updates on high-error states cause divergent behavior, then the policy network should be updated at a lower frequency than the value network, to first minimize error before introducing a policy update.

타겟 네트워크가 여러 업데이트에 걸쳐 오류를 줄이는 데 사용될 수 있고, 높은 오류 상태에서의 정책 업데이트가 발산 행동을 유발한다면, 정책 업데이트를 도입하기 전에 먼저 오류를 최소화하기 위해 정책 네트워크는 가치 네트워크보다 낮은 빈도로 업데이트되어야 합니다.

The modification is to only update the policy and target networks after a fixed number of updates $d$ to the critic.

수정 사항은 critic에 대한 고정된 횟수 $d$ 의 업데이트가 이루어진 후에만 정책 및 타겟 네트워크를 업데이트하는 것입니다.

By sufficiently delaying the policy updates, we limit the likelihood of repeating updates with respect to an unchanged critic.

정책 업데이트를 충분히 지연함으로써, 변하지 않은 critic에 대한 반복 업데이트의 가능성을 제한합니다.

The effectiveness of this strategy is captured by our empirical results presented in Section 6.1, which show an improvement in performance while using fewer policy updates.

이 전략의 효과성은 6.1절에 제시된 우리의 경험적 결과로 포착되며, 이는 적은 수의 정책 업데이트를 사용하면서도 성능이 향상됨을 보여줍니다.

Our approach enforces the notion that similar actions should have similar value.

우리의 접근 방식은 유사한 행동들은 유사한 가치를 가져야 한다는 개념을 강조합니다.

While the function approximation does this implicitly, the relationship between similar actions can be forced explicitly by modifying the training procedure.

함수 근사는 이를 암시적으로 수행하지만, 유사한 행동들 사이의 관계는 훈련 절차를 수정함으로써 명시적으로 강제될 수 있습니다.

This section will be concerned with producing an accurate estimate $\hat{A}_{t}$ of the discounted advantage function $A^{π,γ}(s_t, a_t)$, which will then be used to construct a policy gradient estimator of the following form:

$$
\hat{g} = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=0}^{\infty} \hat{A}^n_t \nabla_{\theta} \log \pi_{\theta}(a^n_t \mid s^n_t)
$$

이 섹션은 discounted advantage 함수 $A^{π,γ}(s_t, a_t)$의 정확한 추정치 $\hat{A}_{t}$를 생산하는 데에 중점을 두며, 이것은 다음 형태의 policy gradient 추정기를 구성하는 데 사용될 것입니다:

We propose that fitting the value of a small area around the target action

$$y = r + \mathbb{E}\_{\epsilon} \left[ Q\_{\theta'}(s', \pi_{\phi'}(s') + \epsilon)\right], \tag{13}$$

would have the benefit of smoothing the value estimate by bootstrapping off of similar state-action value estimates.

우리는 target action

$$y = r + \mathbb{E}\_{\epsilon} \left[ Q\_{\theta'}(s', \pi_{\phi'}(s') + \epsilon)\right], \tag{13}$$

주변의 작은 영역에 대한 가치를 적합시키는 것이 비슷한 상태-행동 가치 추정치에서 부트스트래핑을 통해 가치 추정을 smoothing하는 이점이 있을 것이라고 제안합니다.

In practice, we can approximate this expectation over actions by adding a small amount of random noise to the target policy and averaging over mini-batches.

실제로, 우리는 목표 정책에 작은 양의 랜덤 노이즈를 추가하고 미니배치에 대해 평균을 내어 이러한 행동에 대한 기대를 근사할 수 있습니다.

Intuitively, it is known that policies derived from SARSA value estimates tend to be safer, as they provide higher value to actions resistant to perturbations.

직관적으로, SARSA 가치 추정치에서 유도된 정책은 변동에 저항하는 행동에 더 높은 가치를 제공하기 때문에 더 안전하다고 알려져 있습니다.

Thus, this style of update can additionally lead to improvement in stochastic domains with failure cases.

따라서, 이러한 업데이트 방식은 실패 사례가 있는 확률적 영역에서의 개선으로 이어질 수 있습니다.

A similar idea was introduced concurrently by Nachum et al. (2018), smoothing over $Q_θ$, rather than $Q_{\theta'}$.

비슷한 아이디어가 Nachum et al. (2018)에 의해 동시에 소개되었는데, 이는 $Q_{\theta'}$가 아닌 $Q_{\theta}$에 대해 smoothing하는 것입니다.

Every $d$ iterations, the policy is updated with respect to $Q_{\theta_{1}}$ following the deterministic policy gradient algorithm.

매 $d$번의 반복마다, 정책은 결정론적 정책 그래디언트 알고리즘을 따라 $Q_{\theta_{1}}$에 대해 업데이트됩니다.

The sample update is in addition affected by sampling error. On the other hand, the sample update is cheaper computationally because it considers only one next state, not all possible next states.

샘플 업데이트는 샘플링 오류의 영향을 추가적으로 받습니다. 반면에, 샘플 업데이트는 가능한 모든 다음 상태가 아닌 오직 하나의 다음 상태만을 고려하기 때문에 계산적으로 더 저렴합니다.

To provide some intuition, we examine the learning behavior with and without target networks on both the critic and actor in Figure 3, where we graph the value, in a similar manner to Figure 1, in the Hopper-v1 environment.

직관을 제공하기 위해, 우리는 그림 3에서 Hopper-v1 환경에서 그림 1과 유사한 방식으로 값(value)을 그래프로 나타내며, critic과 actor에서 target 네트워크가 있는 경우와 없는 경우에 대한 학습 행동을 살펴봅니다.

