"An itemset is in the negative border if it is not frequent in the sample, but all its immediate subsets are frequent"  * immediate subset = “delete exactly one element”

아이템셋이 샘플에서 빈번하지 않은 경우, 그러나 모든 중간 부분집합들은 빈번하다면, 그 아이템셋은 음성의 경계에 속합니다.

"Try to choose the support threshold so the probability of failure is low, while the number of itemsets checked on the second pass fits in main memory." .

실패 확률이 낮도록 지지도 임계값을 선택하고, 두 번째 패스에서 확인된 아이템셋 수가 주 메모리에 맞을 수 있도록 하십시오.

"Two documents could (rarely) appear to have shingles in common, when in fact only the hash-values were shared."

두 문서가 공통의 shingles을 가진 것처럼 (드물게) 보일 수 있지만, 사실은 해시 값만이 공유되었을 수 있습니다.

"You only need to be consistent, and assure that two columns get the same value if and only if their first 1’s in the permuted order are in the same row."

일관성만 유지하고, 순열된 순서에서 첫 번째 1이 같은 행에 있는 경우에만 두 열이 같은 값을 갖도록 보장해야 합니다.

Using these functions, JAX can canonicalize any tree of registered container objects into tuples.

JAX는 등록된 컨테이너 객체의 어떤 트리도 튜플로 표준화할 수 있습니다.

In particular, to be able to “match up” leaves in these parameter pytrees with values in the argument pytrees, the parameter pytrees are often constrained to be tree prefixes of the argument pytrees.

특히, 이러한 parameter pytrees의 잎(leaf)과 argument pytrees의 값 사이에 "매칭"을 할 수 있도록, parameter pytrees는 종종 argument pytrees의 트리 접두사로 제한됩니다.

===

This keeps downstream JAX internals simpler: transformations like `grad()`, `jit()`, and `vmap()` can handle user functions that accept and return the myriad different Python containers, while all the other parts of the system can operate on functions that only take (multiple) array arguments and always return a flat list of arrays.

이렇게 하면 후속의 JAX 내부 구조가 더 간단해집니다: `grad()`, `jit()`, 그리고 `vmap()`과 같은 변환들은 수많은 다양한 파이썬 컨테이너를 받아들이고 반환하는 사용자 함수를 처리할 수 있으며, 시스템의 다른 모든 부분은 오직 (다중의) 배열 인자만을 받아들이고 항상 평평한 배열 리스트를 반환하는 함수에 대해 작동할 수 있습니다.

The treedef can then be used to construct a matching structured value after transforming the leaves.

treedef는 leaves를 변환한 후에 일치하는 구조화된 값을 구성하는 데 사용될 수 있다.

Pytrees are tree-like, rather than DAG-like or graph-like, in that we handle them assuming referential transparency and that they can’t contain reference cycles.

Pytrees는 referential transparency을 가정하고 reference cycles를 포함할 수 없다는 점에서 DAG 형태나 graph 형태가 아닌 tree 형태로 처리됩니다.

a pair of an iterable with the children to be flattened recursively, and some opaque auxiliary data to pass back to the unflattening recipe.

children을 재귀적으로 평탄화할 iterable과, unflattening recipe에 전달할 불투명한 auxiliary 데이터의 쌍입니다.

===

JAX sometimes needs to compare `treedef` for equality, or compute its hash for use in the JIT cache, and so care must be taken to ensure that the auxiliary data specified in the flattening recipe supports meaningful hashing and equality comparisons.

JAX는 때때로 treedef를 동등성을 위해 비교하거나 JIT 캐시에서 사용하기 위해 그 해시를 계산해야 하므로, 평탄화 recipe에서 지정된 auxiliary 데이터가 의미 있는 해싱 및 동등성 비교를 지원하도록 주의를 기울여야 합니다.

anticipate: 예상하다, 예측하다

In the first case, JAX’s internals use arrays of object() values to infer the structure of the tree; in the second case, the jacobian of a function mapping a tree to a tree is defined as a tree of trees.

첫 번째 경우에서, JAX의 내부는 tree의 구조를 추론하기 위해 object() 값들의 배열을 사용합니다; 두 번째 경우에서는 tree에서 tree로 매핑하는 함수의 jacobian이 tree의 tree로 정의됩니다.

For this reason, the `__init__` and `__new__` methods of custom PyTree classes should generally avoid doing any array conversion or other input validation, or else anticipate and handle these special cases. For example:

이러한 이유로, 사용자 정의 PyTree 클래스의 `__init__` 및 `__new__` 메서드는 일반적으로 배열 변환 또는 기타 입력 검증을 수행하는 것을 피해야 하며, 또는 이러한 특별한 경우를 예상하고 처리해야 합니다. 예를 들면:

One way to take larger steps in robust ways is using KL constraint bounded by δ, which imposes a hard constraint to control bad cases in the policy space, and it is easier to tune δ and C.

robust한 방식으로 larger steps를 취하는 한 가지 방법은 δ로 경계된 KL constraint를 사용하는 것이다. 이것은 정책 공간에서 나쁜 경우를 제어하기 위한 hard constraint를 부과하며, δ와 C를 조정하기가 더 쉽다.

Using q to denote the sampling distribution, the contribution of a single s_n to the loss function is

q를 샘플링 분포를 나타내는 데 사용하면, 단일 s_n의 손실 함수에 대한 기여는 다음과 같다.

===

This problem imposes a constraint that the KL divergence is bounded at every point in the state space. While it is motivated by the theory, this problem is impractical to solve due to the large number of constraints.

이 문제는 상태 공간의 모든 지점에서 KL divergence가 제한되는 constraint를 부과한다. 이론에 기반을 둔 것이지만, 제약의 수가 많기 때문에 이 문제를 해결하는 것은 실용적이지 않다.

For each of these states sn, we perform multiple actions (a1 and a2 here) and perform a rollout after each action, using common random numbers (CRN) to reduce the variance.

이러한 상태들 중 각각의 sn에 대해, 우리는 여러 행동(a1과 a2 여기서)을 수행하고 각 행동 후에 롤아웃을 수행하며, 분산을 줄이기 위해 공통 랜덤 숫자(CRN)를 사용한다.

===

We use the conjugate gradient algorithm followed by a line search, which is altogether only slightly more expensive than computing the gradient itself.

우리는 conjugate gradient 알고리즘을 사용한 후 Line search를 수행하는데, 이는 전체적으로 gradient를 계산하는 것보다 약간 더 비용이 든다.

With regard to (3), we construct the Fisher information matrix (FIM) by analytically computing the Hessian of the KL divergence, rather than using the covariance matrix of the gradients.

(3)과 관련하여, 우리는 gradient의 공분산 행렬(covariance matrix)을 사용하는 대신 KL divergence의 헤시안(Hessian)을 분석적으로 계산하여 Fisher information matrix(FIM)를 구성한다.

The analytic estimator integrates over the action at each state sn, and does not depend on the action an that was sampled.

분석적 추정치는 각 상태 sn에서의 행동에 대해 통합하고, 샘플링된 행동 an에 의존하지 않는다.

===

Kakade & Langford (2002) consider this error in their derivation, and the same arguments would hold in the setting of this paper, but we omit them for simplicity.

Kakade & Langford (2002)는 그들의 유도에서 이 오류를 고려하고, 이 논문의 설정에서도 동일한 논거가 적용되겠지만, 우리는 간단함을 위해 그것들을 생략한다.

This differs from our approach, which enforces the constraint at each update.

이것은 각 업데이트에서 제약을 적용하는 우리의 접근법과 다르다.

Several other methods employ an update similar to Equation (12).

여러 다른 방법들이 식 (12)와 유사한 업데이트를 사용합니다.

@@@

Levine and Abbeel (2014) also use a KL divergence constraint, but its purpose is to encourage the policy not to stray from regions where the estimated dynamics model is valid, while we do not attempt to estimate the system dynamics explicitly.

Levine와 Abbeel (2014)도 KL divergence 제약을 사용하지만, 그 목적은 추정된 동역학 모델이 유효한 영역에서 정책이 벗어나지 않도록 권장하는 것이며, 우리는 시스템 동역학을 명시적으로 추정하려고 시도하지 않습니다.

10-dimensional state space, linear reward for forward progress and a quadratic penalty on joint effort to produce the reward $r(x,u)=v_x-10^{-5} || u ||^2$

10차원의 상태 공간에서, 전진하는 보상에 대한 선형 보상과 joint effort에 대한 이차 패널티를 사용하여 보상. $r(x,u)=v_x-10^{-5} || u ||^2$ 을 생성합니다

The swimmer can propel itself forward by making an undulating motion.

수영자는 파도치는 움직임을 만들어 앞으로 나아갈 수 있습니다.

Natural gradient performed well on the two easier problems, but was unable to generate hopping and walking gaits that made forward progress.

Natural gradient는 두 개의 더 쉬운 문제에서 잘 수행되었지만, 전진하는 hopping과 walking gaits를 생성하는 데는 실패했습니다.

CEM and CMA are derivative-free algorithms, hence their sample complexity scales unfavorably with the number of parameters, and they performed poorly on the larger problems.

CEM과 CMA는 미분이 없는 알고리즘들이므로, 그들의 샘플 복잡도는 매개 변수의 수와 불리하게 비례하며, 그들은 더 큰 문제에서 성능이 떨어졌습니다.

Company A agreed to solicit customers for Company B, for a fee.

회사 A는 회사 B의 고객을 유치하기 위해 수수료를 받고 동의했습니다.

Neither recorded exactly which customers were involved.

두 회사 모두 어떤 고객이 관련되었는지 정확하게 기록하지 않았습니다.

deduct points for small misspellings (“Jeffrey” vs. “Jeffery”) or same phone with different area code.

작은 철자 오류("Jeffrey" vs. "Jeffery")나 같은 전화번호에 다른 지역 코드에 대해 점수를 차감합니다.

Score all pairs of records that the LSH scheme identified as candidates; report high scores as matches.

LSH 스키마에서 후보로 식별된 모든 레코드 쌍의 점수를 매기고, 높은 점수를 일치 항목으로 보고합니다.

Identical records had an average creation-date difference of 10 days

동일한 레코드는 평균 생성 날짜 차이가 10일이었습니다.

We only looked for records created within 90 days of each other, so bogus matches had a 45 day average difference in creation dates

우리는 서로 90일 이내에 생성된 레코드만 찾았기 때문에 잘못된 일치 항목은 생성 날짜의 평균 차이가 45일이었습니다.

By looking at the pool of matches with a fixed score, we could compute the average time difference, say x, and deduce that fraction (45-x)/35 of them were valid matches.

고정된 점수를 가진 일치 항목 그룹을 살펴보면, 평균 시간 차이인 x를 계산할 수 있고, 그 결과 (45-x)/35의 비율이 유효한 일치 항목임을 추론할 수 있습니다.

if records had a height field, we would expect true matches to be close, false matches  to have the average difference for random  people.

레코드에 키 필드가 있었다면, 참으로 일치하는 항목은 가까울 것이라고 예상되며, 거짓으로 일치하는 항목은 임의의 사람들에 대한 평균 차이를 가질 것입니다.

the same article, say from the Associated Press, appears on the Web site of  many newspapers, but looks quite different.

예를 들어, Associated Press에서 나온 동일한 기사가 많은 신문의 웹사이트에 게재되지만, 외관이 상당히 다를 수 있습니다.

LSH substitute: candidates are articles of similar length

LSH 대체: 후보는 유사한 길이의 기사들입니다.

Specifically, we’ll couple the policies, so that they define a joint distribution over pairs of actions.

구체적으로, 우리는 정책들을 연결하여 행동 쌍에 대한 공동 분포를 정의하겠습니다.

This notation is suggestive

이 표기법은 시사하는 바가 있습니다.

The important thing to understand today is the relationship between matrix multiplication and composition of transformations.

오늘 이해해야 할 중요한 것은 행렬 곱셈과 변환의 구성 사이의 관계입니다.

matrix multiplication is more subtle than addition and scalar multiplication.

행렬 곱셈은 덧셈이나 스칼라 곱셈보다 더 미묘합니다.

Our proof relies on the notion of coupling, where we jointly define the policies $π$ and $π'$ so that they choose the same action with high probability $= (1 − α)$.

우리의 증명은 coupling의 개념에 의존하는데, 여기서 우리는 정책들 $π$와 $π'$를 공동으로 정의하여 둘 다 높은 확률 $(1 - α)$로 동일한 행동을 선택하도록 합니다.

Surrogate loss $L_π(\tilde{π})$ accounts for the the advantage of $\tilde{\pi}$ the first time that it disagrees with $π$, but not subsequent disagreements.

Surrogate loss $L_π(\tilde{π})$는 $\tilde{\pi}$의 advantage를 $π$와 처음으로 불일치할 때 고려하지만, 이후의 불일치에 대해서는 고려하지 않습니다.

Rearranging, the results follows.

재배열하면, 결과가 따라옵니다.

It is a pain to verify using the row-column rule! Much easier: use associativity of linear transformations:

선형 변환(linear transformations)의 결합 법칙(associativity)을 사용하는 것이 행-열 규칙을 사용하는 것보다 훨씬 쉽습니다!

Composition of linear transformations corresponds to multiplication of matrices.

선형 변환의 구성은 행렬의 곱셈에 해당합니다.

The multiplicative inverse (or reciprocal) of nonzero number a is the number b such that ab = 1.

0이 아닌 수 a의 곱셈에 대한 역수(또는 역수)는 ab = 1을 만족하는 수 b입니다.

Any field not used in the LSH could have been used to validate, provided corresponding values  were closer for true matches than false.

LSH에 사용되지 않은 필드도 해당 값이 거짓보다 참 일치에 더 가깝다면 유효성 검사에 사용할 수 있었습니다.

The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data.

두 가지 주요 도전 과제는 일반적으로 필요한 많은 수의 샘플과, 들어오는 데이터의 비정상성에도 불구하고 안정적이고 지속적인 개선을 얻기 어렵다는 것입니다.

We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(λ).

첫 번째 도전 과제는 가치 함수를 사용해 policy gradient 추정의 분산을 크게 줄이면서 약간의 편향을 감수하는 것과, TD(λ)와 유사한 지수 가중치를 사용하는 advantage 함수의 추정기를 통해 해결합니다.

Value functions offer an elegant solution to the credit assignment problem—they allow us to estimate the goodness of an action before the delayed reward arrives.

가치 함수는 credit assignment 문제에 대한 우아한 해결책을 제공합니다 - 그것들은 지연된 보상이 도착하기 전에 어떤 행동의 좋음을 추정할 수 있게 해줍니다.

Reinforcement learning algorithms make use of value functions in a variety of different ways; this paper considers algorithms that optimize a parameterized policy and use value functions to help estimate how the policy should be improved.

강화 학습 알고리즘은 다양한 방식으로 가치 함수를 사용합니다; 이 논문은 매개변수화된 정책을 최적화하고 가치 함수를 사용하여 정책을 어떻게 개선해야 할지 추정하는 데 도움을 주는 알고리즘을 고려합니다.

Another class of policy gradient algorithms, called actor-critic methods, use a value function rather than the empirical returns, obtaining an estimator with lower variance at the cost of introducing bias (Konda & Tsitsiklis, 2003; Hafner & Riedmiller, 2011).

actor-critic 방법으로 불리는 또 다른 종류의 policy gradient 알고리즘은 경험적 return 대신 가치 함수를 사용하여, 편향을 도입하는 대신에 더 낮은 분산을 가진 추정치를 얻습니다 (Konda & Tsitsiklis, 2003; Hafner & Riedmiller, 2011).

While the formula has been proposed in prior work (Kimura & Kobayashi, 1998; Wawrzynski, 2009), our analysis is novel and enables GAE to be applied with a more general set of algorithms, including the batch trust-region algorithm we use for our experiments.

이전 연구에서 제안된 공식이 있지만 (Kimura & Kobayashi, 1998; Wawrzynski, 2009), 우리의 분석은 새롭고 GAE를 더 일반적인 알고리즘 세트에 적용할 수 있게 합니다, 이는 우리의 실험을 위해 사용하는 batch trust region 알고리즘을 포함합니다.

Note that we are not using a discount as part of the problem specification; it will appear below as an algorithm parameter that adjusts a bias-variance tradeoff.

문제 사양의 일부로 discount을 사용하지 않고 있음을 유의하세요; 이것은 아래에서 편향-분산 트레이드오프를 조정하는 알고리즘 매개변수로 나타납니다.

Before proceeding, we will introduce the notion of a γ-just estimator of the advantage function, which is an estimator that does not introduce bias when we use it in place of $A^{π,γ}$(which is not known and must be estimated) in Equation (6) to estimate $g^{γ}$.

계속 진행하기 전에 식 (6)에서 $A^{π,γ}$(알 수 없으므로 추정해야 함) 대신 사용하여 $g^{γ}$를 추정할 때 편향을 도입하지 않는 추정기인 어드밴티지 함수의 γ-just 추정기라는 개념을 소개하겠습니다.

One sufficient condition for $\hat{A}_t$ to be γ-just is that $\hat{A}_t$ decomposes as the difference between two functions $Q_t$ and $b_t$, where $Q_t$ can depend on any trajectory variables but gives an unbiased estimator of the γ-discounted Q-function, and $b_t$ is an arbitrary function of the states and actions sampled before $a_t$.

$\hat{A}_t$가 γ-just인 충분 조건 중 하나는 $\hat{A}_t$가 두 함수 $Q_t$와 $b_t$의 차이로 분해되며, $Q_t$는 임의의 궤적 변수에 의존할 수 있지만 γ-discounted Q-function의 무편향 추정치를 제공하고, $b_t$는 $a_t$ 이전에 샘플링된 상태와 행동들의 임의 함수라는 것입니다.

This section will be concerned with producing an accurate estimate $\hat{A}_{t}$ of the discounted advantage function $A^{π,γ}(s_t, a_t)$, which will then be used to construct a policy gradient estimator of the following form:

이 섹션은 discounted advantage 함수 $A^{π,γ}(s_t, a_t)$의 정확한 추정치 $\hat{A}_{t}$를 생산하는 데에 중점을 두며, 이것은 다음 형태의 policy gradient 추정기를 구성하는 데 사용될 것입니다:

In fact, if we have the correct value function $V=V^{π,γ}$, then it is a γ-just advantage estimator, and in fact, an unbiased estimator $A^{π,γ}$

실제로, 우리가 올바른 가치 함수 $V=V^{π,γ}$를 가지고 있다면, 이것은 γ-just advantage 추정기이며, 사실상 무편향 추정기 $A^{π,γ}$입니다.

These equations result from a telescoping sum, and we see that $\hat{A}_t(k)$ involves a k-step estimate of the returns, minus a baseline term $V(s_t)$.

이 방정식들은 망원합으로부터 나오며, 우리는 $\hat{A}_t(k)$가 k-단계 return 추정치에서 baseline term - $V(s_t)$를 뺀 것을 포함한다는 것을 알 수 있습니다.

So, in 10-dim to capture 0.1% of the data we need 50% of the range.

그러므로, 10차원에서 0.1%의 데이터를 포착하기 위해 우리는 범위의 50%가 필요합니다.

What fraction of “space” do we need to cover to get 0.1% of data (10 nearest neighbors)

0.1%의 데이터(가장 가까운 이웃 10개)를 얻기 위해 필요한 "공간"의 비율은 얼마인가요?

Similar CDs have similar sets of customers, and vice-versa.

비슷한 CD는 비슷한 고객 세트를 가지며, 그 반대도 마찬가지입니다.

Smallest maximum distance to other points.

다른 점들까지의 가장 작은 최대 거리입니다.

Define a notion of cohesion, and merge clusters whose union is most cohesive.

응집력의 개념을 정의하고, 응집력이 가장 높은 합집합을 이루는 클러스터들을 병합합니다.

Stop if the density of the cluster that results from the best merger is below some threshold.

최상의 병합으로 인해 생성된 클러스터의 밀도가 일정 임계값 아래로 떨어지면 중지합니다.

The density can be defined in many different ways. Roughly, it should be the number of cluster points per unit volume of the cluster.

밀도는 여러 가지 방식으로 정의될 수 있습니다. 대략적으로, 클러스터의 단위 부피당 클러스터 점의 수로 정의되어야 합니다.

That ratio can be estimated by the number of points divided by some power of the diameter or radius of the cluster.

그 비율은 클러스터의 직경이나 반경의 어떤 거듭제곱으로 나눈 점의 수로 추정할 수 있습니다.

The correct power could be the number of dimensions of the space.

올바른 거듭제곱은 공간의 차원 수일 수 있습니다.

Sometimes, 1 or 2 is chosen as the power, regardless of the number of dimensions.

때때로, 차원 수에 관계없이 1 또는 2가 거듭제곱으로 선택됩니다.

Density of the merged cluster = divide by the number of points in the cluster by diameter or avg. distance.

병합된 클러스터의 밀도 = 클러스터 내 점의 수를 클러스터의 지름이나 평균 거리로 나눕니다.

Other suggested criteria involved measuring the density of a cluster, based on the radius or diameter.

다른 제안된 기준에는 반지름이나 지름을 기반으로 클러스터의 밀도를 측정하는 것이 포함됩니다.

Both these notions make sense in the non-Euclidean environment. The diameter is still the maximum distance between any two points in the cluster.

이 두 가지 개념은 비유클리드 환경에서도 의미가 있습니다. 지름은 여전히 클러스터 내 임의의 두 점 사이의 최대 거리입니다.

The radius can be defined using the clustroid in place of the centroid.

반지름은 중심점 대신 클러스토이드를 사용하여 정의할 수 있습니다.

Moreover, it makes sense to use the same sort of evaluation for the radius as we used to select the clustroid in the first place.

또한, 클러스토이드를 처음 선택할 때 사용한 것과 같은 종류의 평가를 반지름에 사용하는 것이 합리적입니다.

For example, if we take the clustroid to be the point with the smallest sum of squares of distances to the other nodes, then define the radius to be that sum of squares (or its square root).

예를 들어, 클러스토이드를 다른 노드들에 대한 거리의 제곱합이 가장 작은 점으로 취한다면, 반지름을 그 제곱합(또는 그 제곱근)으로 정의합니다.

tree (Any) – a pytree to be mapped over, with each leaf providing the first positional argument to `f`.

tree (Any) - 각 leaf가 `f`에게 첫 번째 positional argument를 제공하는, 매핑될 pytree입니다.

rest (Any) – a tuple of pytrees, each of which has the same structure as `tree` or has `tree` as a prefix.

rest (Any) - 각각이 `tree`와 같은 구조를 가지거나 `tree`를 접두사로 가지는 pytree들의 튜플입니다.

is_leaf (Callable[[Any], bool] | None) – an optionally specified function that will be called at each flattening step. It should return a boolean, which indicates whether the flattening should traverse the current object, or if it should be stopped immediately, with the whole subtree being treated as a leaf.

is_leaf (Callable[[Any], bool] | None) – 선택적으로 지정된 함수로, 각 평탄화 단계에서 호출됩니다. 이 함수는 boolean을 반환해야 하며, 이는 평탄화가 현재 객체를 계속 탐색해야 하는지, 또는 즉시 중단해야 하는지를 나타냅니다. 즉시 중단하는 경우, 전체 하위 트리가 leaf로 취급됩니다.

A new pytree with the same structure as `tree` but with the value at each leaf given by `f(x, *xs)` where `x` is the value at the corresponding leaf in `tree` and `xs` is the tuple of values at corresponding nodes in `rest`.

`tree`와 동일한 구조를 가진 새로운 pytree이지만, 각 leaf의 값은 `f(x, *xs)`로 주어집니다. 여기서 `x`는 `tree`의 해당 leaf에 있는 값이고 `xs`는 `rest`의 해당 노드들에 있는 값들의 튜플입니다.

If multiple inputs are passed, the structure of the tree is taken from the first input; subsequent inputs need only have `tree` as a prefix:

여러 입력이 전달될 경우, tree의 구조는 첫 번째 입력에서 가져옵니다; 이후의 입력들은 `tree`를 접두사로만 가지면 됩니다:

If the positional arguments to `fun` are container (pytree) types, the corresponding element of `in_axes` can itself be a matching container, so that distinct array axes can be mapped for different container elements.

`fun`에 대한 positional arguments가 컨테이너(pytree) 타입인 경우, `in_axes`의 해당 요소 자체도 일치하는 컨테이너가 될 수 있어, 서로 다른 컨테이너 요소에 대해 다른 배열 축이 매핑될 수 있습니다.

Axis integers must be in the range `[-ndim, ndim)` for each output array, where `ndim` is the number of dimensions (axes) of the array returned by the `vmap()`-ed function, which is one more than the number of dimensions (axes) of the corresponding array returned by `fun`.

출력 배열에 대한 축 정수는 각각의 출력 배열에 대해 `[-ndim, ndim)` 범위 내에 있어야 하며, 여기서 `ndim`은 `vmap()`에 의해 변환된 함수가 반환하는 배열의 차원(축) 수로, 이는 `fun`에 의해 반환된 해당 배열의 차원(축) 수보다 하나 더 많습니다.

Batched/vectorized version of `fun` with arguments that correspond to those of `fun`, but with extra array axes at positions indicated by `in_axes`, and a return value that corresponds to that of `fun`, but with extra array axes at positions indicated by `out_axes`.

`in_axes`에 의해 지정된 위치에 추가적인 배열 축을 가진 `fun`의 인자에 해당하는 인자를 가지고 있으며, `out_axes`에 의해 지정된 위치에 추가적인 배열 축을 가진 `fun`의 반환 값에 해당하는 반환 값을 가진 `fun`의 배치된/벡터화된 버전입니다.

The vdot(*a*, *b*) function handles complex numbers differently than dot(*a*, *b*). If the first argument is complex the complex conjugate of the first argument is used for the calculation of the dot product.

vdot(*a*, *b*) 함수는 dot(*a*, *b*)와 다르게 복소수를 다룬다. 첫 번째 인자가 복소수인 경우, 점곱셈 계산을 위해 첫 번째 인자의 복소수 켤레가 사용된다.

Typical implementations create a new instance of the class by invoking the superclass’s `__new__()` method using `super().__new__(cls[, ...])` with appropriate arguments and then modifying the newly created instance as necessary before returning it.

일반적인 구현에서는 적절한 인자를 사용하여 상위 클래스의 `__new__()` 메서드를 `super().__new__(cls[, ...])`를 호출하여 클래스의 새로운 인스턴스를 생성하고, 필요에 따라 새로 생성된 인스턴스를 수정한 후에 반환합니다.

If `__new__()` is invoked during object construction and it returns an instance of cls, then the new instance’s `__init__()` method will be invoked like `__init__(self[, ...])`, where self is the new instance and the remaining arguments are the same as were passed to the object constructor.

만약 객체 생성 중 `__new__()`가 호출되어 cls의 인스턴스를 반환하면, 새 인스턴스의 `__init__()` 메서드가 `__init__(self[, ...])`와 같이 호출됩니다. 여기서 self는 새 인스턴스를 나타내고 나머지 인자들은 객체 생성자에 전달된 것과 동일합니다.

`__new__()` is intended mainly to allow subclasses of immutable types (like int, str, or tuple) to customize instance creation. It is also commonly overridden in custom metaclasses in order to customize class creation.

`__new__()`는 주로 불변 타입(예: int, str, tuple 등)의 서브클래스가 인스턴스 생성을 사용자 정의할 수 있도록 하기 위한 목적으로 사용됩니다. 또한, 클래스 생성을 사용자 정의하기 위해 사용자 정의 메타클래스에서도 흔히 오버라이드됩니다.

We also introduce the notion of a response function to help understand the bias introduced by $γ$ and $λ$.

우리는 또한 $γ$와 $λ$에 의해 도입된 편향을 이해하는 데 도움이 되는 응답 함수 개념을 소개합니다.

The response function lets us quantify the temporal credit assignment problem: long range dependencies between actions and rewards correspond to nonzero values of the response function for $l \gg 0$.

Response function은 우리가 시간적 credit assignment 문제를 정량화할 수 있게 해줍니다: 행동과 보상 간의 장기 의존성은 $l ≫ 0$ 일 때 Response function의 nonzero 값에 해당합니다.

Thus, the error introduced by this approximation will be small if $\mathcal{X}$ rapidly decays as $l$ increases, i.e., if the effect of an action on rewards is “forgotten” after $≈ 1/(1 − γ)$ timesteps.

따라서, 이 근사에 의해 도입된 오류는 $l$이 증가함에 따라 $\mathcal{X}$가 빠르게 감소한다면, 즉, 행동의 영향이 대략 $1/(1 - γ)$ 타임스텝 후에 "잊혀진다면" 작을 것입니다.

This constraint is equivalent to constraining the average KL divergence between the previous value function and the new value function to be smaller than ϵ, where the value function is taken to parameterize a conditional Gaussian distribution with mean $V_\phi(s)$ and variance $σ^2$ .

이 제약은 이전 가치 함수와 새 가치 함수 사이의 평균 KL divergence가 ε보다 작게 유지되는 것과 동등합니다. 여기서 가치 함수는 평균이 $V_\phi(s)$이고 분산이 $σ^2$인 조건부 가우시안 분포를 매개변수화하는 것으로 간주됩니다.

Note that $H$ is the “Gauss-Newton” approximation of the Hessian of the objective, and it is (up to a $σ^2$ factor) the Fisher information matrix when interpreting the value function as a conditional probability distribution.

$H$는 목적의 헤시안 근사치에 대한 "가우스-뉴턴" 근사치이며, 값 함수를 조건부 확률 분포로 해석할 때 ($σ^2$ 계수까지) 피셔 정보 행렬이라는 점에 유의하세요.

While we experimented with this choice, we did not notice a difference in performance from the λ = 1 estimator in Equation (28).

우리는 이 선택을 실험해 보았지만, 식 (28)의 λ = 1 추정기와 성능에서 차이를 느끼지 못했습니다.

To see this, consider the extreme case where we overfit the value function, and the Bellman residual $r_t + γV (s_{t+1}) − V (s_t)$ becomes zero at all timesteps—the policy gradient estimate would be zero.

이를 이해하려면, Value function을 과적합하여 Bellman residual $r_t + γV (s_{t+1}) − V (s_t)$가 모든 타임스텝에서 0이 되는 극단적인 경우를 고려해보세요—Policy gradient 추정치는 0이 될 것입니다.

dynamically standing up, for the biped, which starts off laying on its back.

등을 대고 누워 시작하는, 이족보행을 위해, 동적으로 일어서는 동작입니다.

The humanoid model has 33 state dimensions and 10 actuated degrees of freedom, while the quadruped model has 29 state dimensions and 8 actuated degrees of freedom.

휴머노이드 모델은 33개의 상태 차원과 10개의 구동 자유도를 가지고 있으며, 사족보행 로봇 모델은 29개의 상태 차원과 8개의 구동 자유도를 가집니다.

Right: performance after 20 iterations of policy optimization, as γ and λ are varied. White means higher reward. The best results are obtained at intermediate values of both.

오른쪽: γ와 λ가 변할 때 Policy optimization의 20번 반복 후의 성능입니다. 흰색은 더 높은 보상을 의미합니다. 두 값의 중간 값에서 최상의 결과를 얻습니다.

if a sub-environment is not terminated nor truncated, PPO estimates the value of the next state in this sub-environment as the value target.

sub-environment가 terminated되거나 truncated되지 않은 경우, PPO는 이 sub-environment에서 다음 상태의 가치를 value target으로 추정합니다.

A class method can be called either on the class (such as `C.f()`) or on an instance (such as `C().f()`). The instance is ignored except for its class. If a class method is called for a derived class, the derived class object is passed as the implied first argument.

클래스 메서드는 클래스에 대해 (예: `C.f()`) 또는 인스턴스에 대해 (예: `C().f()`) 호출될 수 있습니다. 인스턴스는 그 클래스를 제외하고는 무시됩니다. 파생된 클래스에 대한 클래스 메서드가 호출되면, 파생된 클래스 객체가 암시적인 첫 번째 인자로 전달됩니다.

Various cohesion measures of the union of two clusters.

두 클러스터의 합집합에 대한 다양한 응집력 측정 방법입니다.

Each trial took about 2 hours to run on a 16-core machine, where the simulation rollouts were parallelized, as were the function, gradient, and matrix-vector-product evaluations used when optimizing the policy and value function.

각 시험은 16코어 기계에서 약 2시간이 걸렸으며, 여기서 시뮬레이션 롤아웃은 병렬화되었고, 정책과 가치 함수를 최적화할 때 사용되는 함수, 그래디언트, 행렬-벡터 곱셈 평가도 병렬화되었습니다.

Hence, it is plausible that this algorithm could be run on a real robot, or multiple real robots learning in parallel, if there were a way to reset the state of the robot and ensure that it doesn’t damage itself.

따라서, 로봇의 상태를 재설정하고 로봇이 스스로를 손상시키지 않도록 보장할 수 있는 방법이 있다면, 이 알고리즘이 실제 로봇이나 병렬로 학습하는 여러 실제 로봇에서 실행될 수 있을 것으로 보입니다.

For 3D standing, the value function always helped, but the results are roughly the same for λ = 0.96 and λ = 1.

3D 서기의 경우, Value function은 항상 도움이 되었지만, λ = 0.96과 λ = 1의 결과는 대략 비슷합니다.

Policy gradient methods provide a way to reduce reinforcement learning to stochastic gradient descent, by providing unbiased gradient estimates.

Policy gradient 방법은 편향되지 않은 그래디언트 추정치를 제공함으로써 강화 학습을 확률적 그래디언트 하강으로 줄이는 방법을 제공합니다.

One question that merits future investigation is the relationship between value function estimation error and policy gradient estimation error.

미래의 조사가 필요한 한 가지 질문은 Value function 추정 오류와 Policy gradient 추정 오류 사이의 관계입니다.

